{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "jBDw0dWTwrdx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "debug = True\n",
        "\n",
        "# TODO stride=2 is used in the paper but this is causing shape issues right now :(\n",
        "def conv_3x3(in_channels, out_channels, pad=1, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=pad)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        A ResidualBlock implements the basic residual block discussed in the paper, for the network used on CIFAR 10.\n",
        "        It consists of a pair of 3x3 convolutional layers, all with the same output feature map size (either 32, 16, or 8)\n",
        "        We apply conv-BN-relu for the first layer, then conv-BN, then add the input (residual connection) and do a final RELU.\n",
        "        We zero-pad the input for dimension mismatches, so that no new parameters are introduced in the residual connections.\n",
        "        \"\"\"\n",
        "        self.in_channels, self.out_channels = in_channels, out_channels\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv_3x3(in_channels, in_channels)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=in_channels)\n",
        "        self.conv2 = conv_3x3(in_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        # The paper said that identity shortcuts are used in all cases, \n",
        "        # so in cases of shape mismatch I pad to align dimensions, which introduces no new parameters.\n",
        "        if x.shape != identity.shape:\n",
        "            shape_diff = abs((sum(identity.shape) - sum(x.shape)))\n",
        "            identity = F.pad(identity, pad=(0,0,0,0,shape_diff//2,shape_diff//2))\n",
        "        x = F.relu(identity + x)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Residual block with in_channels {self.in_channels} and out channels {self.out_channels}'\n",
        "\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, n=1, dbg=False):\n",
        "        super(Resnet, self).__init__()\n",
        "        debug = dbg\n",
        "        self.residual_blocks = []\n",
        "        # create number of residual blocks needed\n",
        "        cur_feature_map_size = 16\n",
        "        changed = False\n",
        "        for i in range(3*n):\n",
        "            if i != 0 and i % n == 0:\n",
        "                cur_feature_map_size = cur_feature_map_size*2\n",
        "                changed = True\n",
        "            block = ResidualBlock(cur_feature_map_size if not changed else cur_feature_map_size//2, cur_feature_map_size)\n",
        "            changed = False\n",
        "            self.residual_blocks.append(block)\n",
        "\n",
        "        self.linear = nn.Linear(16384, 10)\n",
        "        self.conv1 = conv_3x3(3, 16)\n",
        "        self.pool = nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        for block in self.residual_blocks:\n",
        "            x = block(x)\n",
        "        # x = self.first_block(x)\n",
        "        # flatten the multidimensional input to a single matix for input into the FC layer\n",
        "        x = self.pool(x) # only difference is this pool\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S3A8I7K9wrd3",
        "colab_type": "code",
        "outputId": "2fcb31f3-37b9-402f-b7c4-0b620d81fb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 44923
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def get_dataset():\n",
        "    transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=1)\n",
        "    return trainloader, testloader \n",
        "\n",
        "def get_test_accuracy(net, testloader):\n",
        "  accs = []\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "      x, y = data\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "      preds = resnet(x)\n",
        "      _, predicted = torch.max(preds, 1)\n",
        "      accuracy = accuracy_score(predicted.cpu(), y.cpu())\n",
        "      accs.append(accuracy)\n",
        "      break\n",
        "  net.train()\n",
        "  return np.mean(accs)\n",
        "\n",
        "def update_learning_rate(current_lr, optimizer):\n",
        "  new_lr = current_lr/10\n",
        "  for g in optimizer.param_groups:\n",
        "    g['lr'] = new_lr\n",
        "  return new_lr\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    num_epochs = 80\n",
        "    # get cifar 10 data\n",
        "    trainloader, testloader = get_dataset()\n",
        "    benchmark, debug = False, True\n",
        "    resnet = Resnet(n=2,dbg=debug)\n",
        "    resnet.train()\n",
        "    resnet = resnet.cuda()\n",
        "    for block in resnet.residual_blocks:\n",
        "      block.cuda()\n",
        "    current_lr = 1e-4\n",
        "#     optimizer = optim.SGD(resnet.parameters(), lr=current_lr, weight_decay=0.0001, momentum=0.9)\n",
        "    optimizer = optim.Adam(resnet.parameters(), lr=1e-4, weight_decay=0.0001)\n",
        "    train_accs, test_accs = [], []\n",
        "    gradient_norms = []\n",
        "    def train_model():\n",
        "      current_lr=1e-4\n",
        "      stopping_threshold, current_count = 3, 0\n",
        "      n_iters = 0\n",
        "      for e in range(num_epochs):\n",
        "        # modify learning rate at \n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "              x, y = data\n",
        "              x, y = x.cuda(), y.cuda()\n",
        "              # zero the grad\n",
        "              optimizer.zero_grad()\n",
        "              preds = resnet(x)\n",
        "              loss = F.cross_entropy(preds, y)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              if i % 10 == 0:\n",
        "                  _, predicted = torch.max(preds, 1)\n",
        "                  accuracy = accuracy_score(predicted.cpu(), y.cpu())\n",
        "                  train_accs.append(accuracy)\n",
        "                  print('n_iters: {} loss: {}, accuracy: {}'.format(n_iters, loss, accuracy))\n",
        "              if i % 50 == 0:\n",
        "                # get test accuracy\n",
        "                test_acc = get_test_accuracy(resnet, testloader)\n",
        "                test_accs.append(test_acc)\n",
        "                # monitor gradient norms\n",
        "                total_norm = sum([p.grad.data.norm(2).item() ** 2 for p in resnet.parameters()])\n",
        "                total_norm**=(1./2)\n",
        "                gradient_norms.append(total_norm)\n",
        "                print('n_iters: {} test accuracy: {} gradient_norm: {}'.format(n_iters, test_acc, total_norm))\n",
        "              n_iters+=1\n",
        "              if n_iters == 10000 or n_iters == 16000 or n_iters == 24000:\n",
        "                current_lr = update_learning_rate(current_lr, optimizer)\n",
        "                print('decayed learning rate to {}'.format(current_lr))\n",
        "      print('iterated {} times'.format(n_iters))\n",
        "      return resnet, n_iters, train_accs, test_accs\n",
        "    \n",
        "    trained_resnet, n_iters, train_accs, test_accs, gradient_norms = train_model()\n",
        "    \n",
        "                \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "n_iters: 0 loss: 2.3997092247009277, accuracy: 0.1484375\n",
            "n_iters: 0 test accuracy: 0.112 gradient_norm: 17.40572552328217\n",
            "n_iters: 10 loss: 2.320924758911133, accuracy: 0.1328125\n",
            "n_iters: 20 loss: 2.242439031600952, accuracy: 0.171875\n",
            "n_iters: 30 loss: 2.271766185760498, accuracy: 0.1875\n",
            "n_iters: 40 loss: 2.20607590675354, accuracy: 0.1328125\n",
            "n_iters: 50 loss: 2.10225248336792, accuracy: 0.25\n",
            "n_iters: 50 test accuracy: 0.189 gradient_norm: 11.573039621982213\n",
            "n_iters: 60 loss: 2.1989784240722656, accuracy: 0.25\n",
            "n_iters: 70 loss: 2.169342279434204, accuracy: 0.25\n",
            "n_iters: 80 loss: 2.184079170227051, accuracy: 0.1796875\n",
            "n_iters: 90 loss: 2.1422743797302246, accuracy: 0.2734375\n",
            "n_iters: 100 loss: 2.134808301925659, accuracy: 0.265625\n",
            "n_iters: 100 test accuracy: 0.217 gradient_norm: 13.074716923751454\n",
            "n_iters: 110 loss: 2.2287118434906006, accuracy: 0.203125\n",
            "n_iters: 120 loss: 2.07859206199646, accuracy: 0.2265625\n",
            "n_iters: 130 loss: 2.0223469734191895, accuracy: 0.2890625\n",
            "n_iters: 140 loss: 2.1980180740356445, accuracy: 0.234375\n",
            "n_iters: 150 loss: 2.0217604637145996, accuracy: 0.2890625\n",
            "n_iters: 150 test accuracy: 0.246 gradient_norm: 11.858762711196132\n",
            "n_iters: 160 loss: 2.135424852371216, accuracy: 0.234375\n",
            "n_iters: 170 loss: 2.121224880218506, accuracy: 0.25\n",
            "n_iters: 180 loss: 2.130006790161133, accuracy: 0.1796875\n",
            "n_iters: 190 loss: 2.17136812210083, accuracy: 0.2265625\n",
            "n_iters: 200 loss: 2.2333083152770996, accuracy: 0.2578125\n",
            "n_iters: 200 test accuracy: 0.243 gradient_norm: 16.260468585159057\n",
            "n_iters: 210 loss: 1.9795465469360352, accuracy: 0.265625\n",
            "n_iters: 220 loss: 2.2520599365234375, accuracy: 0.21875\n",
            "n_iters: 230 loss: 1.9675719738006592, accuracy: 0.2421875\n",
            "n_iters: 240 loss: 2.180551528930664, accuracy: 0.2421875\n",
            "n_iters: 250 loss: 2.175609827041626, accuracy: 0.234375\n",
            "n_iters: 250 test accuracy: 0.256 gradient_norm: 16.72025218712375\n",
            "n_iters: 260 loss: 2.1613523960113525, accuracy: 0.234375\n",
            "n_iters: 270 loss: 2.0726845264434814, accuracy: 0.234375\n",
            "n_iters: 280 loss: 2.0559685230255127, accuracy: 0.296875\n",
            "n_iters: 290 loss: 2.0704267024993896, accuracy: 0.2890625\n",
            "n_iters: 300 loss: 2.1422886848449707, accuracy: 0.2109375\n",
            "n_iters: 300 test accuracy: 0.272 gradient_norm: 16.031906664490577\n",
            "n_iters: 310 loss: 2.14778995513916, accuracy: 0.296875\n",
            "n_iters: 320 loss: 2.161602258682251, accuracy: 0.234375\n",
            "n_iters: 330 loss: 2.249142646789551, accuracy: 0.1796875\n",
            "n_iters: 340 loss: 2.069444417953491, accuracy: 0.2265625\n",
            "n_iters: 350 loss: 2.0332839488983154, accuracy: 0.25\n",
            "n_iters: 350 test accuracy: 0.274 gradient_norm: 12.986455456224025\n",
            "n_iters: 360 loss: 2.0159811973571777, accuracy: 0.3203125\n",
            "n_iters: 370 loss: 2.0480010509490967, accuracy: 0.28125\n",
            "n_iters: 380 loss: 2.022193193435669, accuracy: 0.3046875\n",
            "n_iters: 390 loss: 2.2199316024780273, accuracy: 0.1625\n",
            "n_iters: 391 loss: 2.126918315887451, accuracy: 0.2265625\n",
            "n_iters: 391 test accuracy: 0.267 gradient_norm: 15.957925461761986\n",
            "n_iters: 401 loss: 2.1217212677001953, accuracy: 0.21875\n",
            "n_iters: 411 loss: 2.0169179439544678, accuracy: 0.3828125\n",
            "n_iters: 421 loss: 2.027526378631592, accuracy: 0.34375\n",
            "n_iters: 431 loss: 1.9948517084121704, accuracy: 0.34375\n",
            "n_iters: 441 loss: 1.9888912439346313, accuracy: 0.296875\n",
            "n_iters: 441 test accuracy: 0.285 gradient_norm: 12.546135627174356\n",
            "n_iters: 451 loss: 2.058293342590332, accuracy: 0.2109375\n",
            "n_iters: 461 loss: 1.9520665407180786, accuracy: 0.2734375\n",
            "n_iters: 471 loss: 1.9508202075958252, accuracy: 0.359375\n",
            "n_iters: 481 loss: 1.8997719287872314, accuracy: 0.3203125\n",
            "n_iters: 491 loss: 2.1740918159484863, accuracy: 0.34375\n",
            "n_iters: 491 test accuracy: 0.276 gradient_norm: 13.681281089946387\n",
            "n_iters: 501 loss: 2.059640884399414, accuracy: 0.3125\n",
            "n_iters: 511 loss: 1.9973061084747314, accuracy: 0.3203125\n",
            "n_iters: 521 loss: 1.9182031154632568, accuracy: 0.3359375\n",
            "n_iters: 531 loss: 1.9206531047821045, accuracy: 0.3515625\n",
            "n_iters: 541 loss: 2.1185903549194336, accuracy: 0.3046875\n",
            "n_iters: 541 test accuracy: 0.304 gradient_norm: 11.898718710012904\n",
            "n_iters: 551 loss: 2.0794565677642822, accuracy: 0.234375\n",
            "n_iters: 561 loss: 2.0502593517303467, accuracy: 0.3203125\n",
            "n_iters: 571 loss: 2.045835256576538, accuracy: 0.328125\n",
            "n_iters: 581 loss: 2.11767840385437, accuracy: 0.265625\n",
            "n_iters: 591 loss: 2.088933229446411, accuracy: 0.3046875\n",
            "n_iters: 591 test accuracy: 0.302 gradient_norm: 14.586183985917017\n",
            "n_iters: 601 loss: 2.021807909011841, accuracy: 0.2265625\n",
            "n_iters: 611 loss: 2.101595640182495, accuracy: 0.2265625\n",
            "n_iters: 621 loss: 1.9388436079025269, accuracy: 0.3125\n",
            "n_iters: 631 loss: 1.9495965242385864, accuracy: 0.3828125\n",
            "n_iters: 641 loss: 1.9101580381393433, accuracy: 0.3984375\n",
            "n_iters: 641 test accuracy: 0.324 gradient_norm: 12.17160955078434\n",
            "n_iters: 651 loss: 1.9143517017364502, accuracy: 0.234375\n",
            "n_iters: 661 loss: 1.904879093170166, accuracy: 0.3203125\n",
            "n_iters: 671 loss: 1.9472616910934448, accuracy: 0.34375\n",
            "n_iters: 681 loss: 1.8565925359725952, accuracy: 0.328125\n",
            "n_iters: 691 loss: 1.853030800819397, accuracy: 0.375\n",
            "n_iters: 691 test accuracy: 0.289 gradient_norm: 14.414727167799056\n",
            "n_iters: 701 loss: 2.002622127532959, accuracy: 0.3203125\n",
            "n_iters: 711 loss: 1.868126392364502, accuracy: 0.34375\n",
            "n_iters: 721 loss: 1.9170020818710327, accuracy: 0.3203125\n",
            "n_iters: 731 loss: 2.020449638366699, accuracy: 0.296875\n",
            "n_iters: 741 loss: 2.0524866580963135, accuracy: 0.3125\n",
            "n_iters: 741 test accuracy: 0.319 gradient_norm: 13.137868692534282\n",
            "n_iters: 751 loss: 1.8992201089859009, accuracy: 0.34375\n",
            "n_iters: 761 loss: 1.9353376626968384, accuracy: 0.3125\n",
            "n_iters: 771 loss: 1.9076244831085205, accuracy: 0.3671875\n",
            "n_iters: 781 loss: 1.9186112880706787, accuracy: 0.3375\n",
            "n_iters: 782 loss: 1.8356751203536987, accuracy: 0.375\n",
            "n_iters: 782 test accuracy: 0.311 gradient_norm: 13.38586978483334\n",
            "n_iters: 792 loss: 1.913483738899231, accuracy: 0.328125\n",
            "n_iters: 802 loss: 1.9377838373184204, accuracy: 0.328125\n",
            "n_iters: 812 loss: 1.9205929040908813, accuracy: 0.3203125\n",
            "n_iters: 822 loss: 1.8736552000045776, accuracy: 0.421875\n",
            "n_iters: 832 loss: 1.9688010215759277, accuracy: 0.2578125\n",
            "n_iters: 832 test accuracy: 0.318 gradient_norm: 13.089916302195714\n",
            "n_iters: 842 loss: 1.9968535900115967, accuracy: 0.3046875\n",
            "n_iters: 852 loss: 1.97776460647583, accuracy: 0.2890625\n",
            "n_iters: 862 loss: 1.8298245668411255, accuracy: 0.328125\n",
            "n_iters: 872 loss: 1.9714446067810059, accuracy: 0.328125\n",
            "n_iters: 882 loss: 1.948559284210205, accuracy: 0.3125\n",
            "n_iters: 882 test accuracy: 0.312 gradient_norm: 11.61844252019844\n",
            "n_iters: 892 loss: 1.9274909496307373, accuracy: 0.3515625\n",
            "n_iters: 902 loss: 1.9814398288726807, accuracy: 0.296875\n",
            "n_iters: 912 loss: 1.9263675212860107, accuracy: 0.3203125\n",
            "n_iters: 922 loss: 1.865308403968811, accuracy: 0.375\n",
            "n_iters: 932 loss: 1.9224998950958252, accuracy: 0.328125\n",
            "n_iters: 932 test accuracy: 0.337 gradient_norm: 13.50603608947448\n",
            "n_iters: 942 loss: 1.8610060214996338, accuracy: 0.40625\n",
            "n_iters: 952 loss: 2.0859172344207764, accuracy: 0.28125\n",
            "n_iters: 962 loss: 1.8298652172088623, accuracy: 0.375\n",
            "n_iters: 972 loss: 1.7946419715881348, accuracy: 0.3359375\n",
            "n_iters: 982 loss: 2.038378953933716, accuracy: 0.3125\n",
            "n_iters: 982 test accuracy: 0.328 gradient_norm: 12.332733642831968\n",
            "n_iters: 992 loss: 1.7433302402496338, accuracy: 0.4140625\n",
            "n_iters: 1002 loss: 1.8665975332260132, accuracy: 0.3359375\n",
            "n_iters: 1012 loss: 1.966455340385437, accuracy: 0.265625\n",
            "n_iters: 1022 loss: 1.8852531909942627, accuracy: 0.34375\n",
            "n_iters: 1032 loss: 1.9395841360092163, accuracy: 0.328125\n",
            "n_iters: 1032 test accuracy: 0.349 gradient_norm: 11.269305351819405\n",
            "n_iters: 1042 loss: 1.9348334074020386, accuracy: 0.359375\n",
            "n_iters: 1052 loss: 1.8677878379821777, accuracy: 0.390625\n",
            "n_iters: 1062 loss: 1.930619716644287, accuracy: 0.28125\n",
            "n_iters: 1072 loss: 1.8983193635940552, accuracy: 0.359375\n",
            "n_iters: 1082 loss: 1.87355375289917, accuracy: 0.3515625\n",
            "n_iters: 1082 test accuracy: 0.337 gradient_norm: 11.016483504330056\n",
            "n_iters: 1092 loss: 1.852036714553833, accuracy: 0.375\n",
            "n_iters: 1102 loss: 2.146953582763672, accuracy: 0.296875\n",
            "n_iters: 1112 loss: 1.8264728784561157, accuracy: 0.3515625\n",
            "n_iters: 1122 loss: 1.9422301054000854, accuracy: 0.2890625\n",
            "n_iters: 1132 loss: 1.9310964345932007, accuracy: 0.3515625\n",
            "n_iters: 1132 test accuracy: 0.327 gradient_norm: 12.507771349688934\n",
            "n_iters: 1142 loss: 1.9317353963851929, accuracy: 0.3359375\n",
            "n_iters: 1152 loss: 1.7879899740219116, accuracy: 0.40625\n",
            "n_iters: 1162 loss: 2.005354404449463, accuracy: 0.265625\n",
            "n_iters: 1172 loss: 2.2061619758605957, accuracy: 0.2625\n",
            "n_iters: 1173 loss: 1.841341495513916, accuracy: 0.3359375\n",
            "n_iters: 1173 test accuracy: 0.348 gradient_norm: 12.033943973762389\n",
            "n_iters: 1183 loss: 1.947619080543518, accuracy: 0.296875\n",
            "n_iters: 1193 loss: 1.8475596904754639, accuracy: 0.3671875\n",
            "n_iters: 1203 loss: 1.8931348323822021, accuracy: 0.3359375\n",
            "n_iters: 1213 loss: 1.9353549480438232, accuracy: 0.2734375\n",
            "n_iters: 1223 loss: 1.9711610078811646, accuracy: 0.34375\n",
            "n_iters: 1223 test accuracy: 0.332 gradient_norm: 15.826241317139633\n",
            "n_iters: 1233 loss: 1.9656165838241577, accuracy: 0.328125\n",
            "n_iters: 1243 loss: 1.9163250923156738, accuracy: 0.28125\n",
            "n_iters: 1253 loss: 1.823179841041565, accuracy: 0.3984375\n",
            "n_iters: 1263 loss: 1.9657480716705322, accuracy: 0.3046875\n",
            "n_iters: 1273 loss: 1.8599761724472046, accuracy: 0.3515625\n",
            "n_iters: 1273 test accuracy: 0.341 gradient_norm: 11.536888777101922\n",
            "n_iters: 1283 loss: 1.9711562395095825, accuracy: 0.34375\n",
            "n_iters: 1293 loss: 1.8142797946929932, accuracy: 0.328125\n",
            "n_iters: 1303 loss: 1.7785155773162842, accuracy: 0.3984375\n",
            "n_iters: 1313 loss: 1.8748197555541992, accuracy: 0.3515625\n",
            "n_iters: 1323 loss: 1.8886606693267822, accuracy: 0.34375\n",
            "n_iters: 1323 test accuracy: 0.353 gradient_norm: 12.631500943342514\n",
            "n_iters: 1333 loss: 1.9940392971038818, accuracy: 0.3203125\n",
            "n_iters: 1343 loss: 1.926722764968872, accuracy: 0.3125\n",
            "n_iters: 1353 loss: 2.0092642307281494, accuracy: 0.3125\n",
            "n_iters: 1363 loss: 1.900817632675171, accuracy: 0.40625\n",
            "n_iters: 1373 loss: 1.8673086166381836, accuracy: 0.3125\n",
            "n_iters: 1373 test accuracy: 0.322 gradient_norm: 14.333096295865694\n",
            "n_iters: 1383 loss: 1.830784559249878, accuracy: 0.359375\n",
            "n_iters: 1393 loss: 1.8795711994171143, accuracy: 0.375\n",
            "n_iters: 1403 loss: 1.9558435678482056, accuracy: 0.3203125\n",
            "n_iters: 1413 loss: 1.7061407566070557, accuracy: 0.375\n",
            "n_iters: 1423 loss: 1.9327342510223389, accuracy: 0.3515625\n",
            "n_iters: 1423 test accuracy: 0.332 gradient_norm: 13.70157370339029\n",
            "n_iters: 1433 loss: 1.9900104999542236, accuracy: 0.2890625\n",
            "n_iters: 1443 loss: 1.8291183710098267, accuracy: 0.3359375\n",
            "n_iters: 1453 loss: 1.8915128707885742, accuracy: 0.3515625\n",
            "n_iters: 1463 loss: 1.8133258819580078, accuracy: 0.390625\n",
            "n_iters: 1473 loss: 1.9551494121551514, accuracy: 0.2578125\n",
            "n_iters: 1473 test accuracy: 0.324 gradient_norm: 11.501988258013146\n",
            "n_iters: 1483 loss: 1.9913458824157715, accuracy: 0.3203125\n",
            "n_iters: 1493 loss: 1.8050527572631836, accuracy: 0.3828125\n",
            "n_iters: 1503 loss: 1.9512959718704224, accuracy: 0.3203125\n",
            "n_iters: 1513 loss: 2.083977460861206, accuracy: 0.3203125\n",
            "n_iters: 1523 loss: 1.7353545427322388, accuracy: 0.375\n",
            "n_iters: 1523 test accuracy: 0.351 gradient_norm: 10.835827886280512\n",
            "n_iters: 1533 loss: 1.9690765142440796, accuracy: 0.3359375\n",
            "n_iters: 1543 loss: 2.023963689804077, accuracy: 0.265625\n",
            "n_iters: 1553 loss: 1.8405556678771973, accuracy: 0.3828125\n",
            "n_iters: 1563 loss: 1.9804121255874634, accuracy: 0.2875\n",
            "n_iters: 1564 loss: 1.8601866960525513, accuracy: 0.3359375\n",
            "n_iters: 1564 test accuracy: 0.3 gradient_norm: 14.50993536998093\n",
            "n_iters: 1574 loss: 1.7716096639633179, accuracy: 0.359375\n",
            "n_iters: 1584 loss: 1.9598875045776367, accuracy: 0.3046875\n",
            "n_iters: 1594 loss: 1.7963895797729492, accuracy: 0.359375\n",
            "n_iters: 1604 loss: 1.7429745197296143, accuracy: 0.390625\n",
            "n_iters: 1614 loss: 1.8181589841842651, accuracy: 0.40625\n",
            "n_iters: 1614 test accuracy: 0.356 gradient_norm: 12.810946848893362\n",
            "n_iters: 1624 loss: 1.7718126773834229, accuracy: 0.3828125\n",
            "n_iters: 1634 loss: 1.7718701362609863, accuracy: 0.390625\n",
            "n_iters: 1644 loss: 1.9136757850646973, accuracy: 0.3515625\n",
            "n_iters: 1654 loss: 1.8282941579818726, accuracy: 0.328125\n",
            "n_iters: 1664 loss: 1.8423398733139038, accuracy: 0.3671875\n",
            "n_iters: 1664 test accuracy: 0.346 gradient_norm: 13.159177697963006\n",
            "n_iters: 1674 loss: 1.7624248266220093, accuracy: 0.375\n",
            "n_iters: 1684 loss: 1.966834545135498, accuracy: 0.3125\n",
            "n_iters: 1694 loss: 1.8363724946975708, accuracy: 0.375\n",
            "n_iters: 1704 loss: 1.901677131652832, accuracy: 0.3515625\n",
            "n_iters: 1714 loss: 1.8311867713928223, accuracy: 0.3515625\n",
            "n_iters: 1714 test accuracy: 0.329 gradient_norm: 11.870805525858751\n",
            "n_iters: 1724 loss: 1.7454278469085693, accuracy: 0.3359375\n",
            "n_iters: 1734 loss: 1.8104382753372192, accuracy: 0.3671875\n",
            "n_iters: 1744 loss: 1.7669929265975952, accuracy: 0.359375\n",
            "n_iters: 1754 loss: 1.7418806552886963, accuracy: 0.40625\n",
            "n_iters: 1764 loss: 1.9382165670394897, accuracy: 0.328125\n",
            "n_iters: 1764 test accuracy: 0.342 gradient_norm: 14.25869048476878\n",
            "n_iters: 1774 loss: 1.933956503868103, accuracy: 0.3125\n",
            "n_iters: 1784 loss: 1.879431962966919, accuracy: 0.3515625\n",
            "n_iters: 1794 loss: 1.8138346672058105, accuracy: 0.3671875\n",
            "n_iters: 1804 loss: 1.8144910335540771, accuracy: 0.375\n",
            "n_iters: 1814 loss: 1.811010479927063, accuracy: 0.375\n",
            "n_iters: 1814 test accuracy: 0.372 gradient_norm: 13.391064322123748\n",
            "n_iters: 1824 loss: 1.701748013496399, accuracy: 0.390625\n",
            "n_iters: 1834 loss: 2.035961627960205, accuracy: 0.3203125\n",
            "n_iters: 1844 loss: 1.900485634803772, accuracy: 0.265625\n",
            "n_iters: 1854 loss: 1.8597474098205566, accuracy: 0.3515625\n",
            "n_iters: 1864 loss: 1.8307698965072632, accuracy: 0.359375\n",
            "n_iters: 1864 test accuracy: 0.381 gradient_norm: 12.509538222288747\n",
            "n_iters: 1874 loss: 1.7870285511016846, accuracy: 0.3984375\n",
            "n_iters: 1884 loss: 1.8185815811157227, accuracy: 0.3984375\n",
            "n_iters: 1894 loss: 1.8736988306045532, accuracy: 0.3671875\n",
            "n_iters: 1904 loss: 1.7855122089385986, accuracy: 0.3828125\n",
            "n_iters: 1914 loss: 1.8490355014801025, accuracy: 0.375\n",
            "n_iters: 1914 test accuracy: 0.385 gradient_norm: 12.273820534555254\n",
            "n_iters: 1924 loss: 1.9188107252120972, accuracy: 0.3125\n",
            "n_iters: 1934 loss: 1.8800370693206787, accuracy: 0.34375\n",
            "n_iters: 1944 loss: 1.7721582651138306, accuracy: 0.3984375\n",
            "n_iters: 1954 loss: 1.839531660079956, accuracy: 0.3125\n",
            "n_iters: 1955 loss: 1.81159245967865, accuracy: 0.328125\n",
            "n_iters: 1955 test accuracy: 0.337 gradient_norm: 11.605580657039816\n",
            "n_iters: 1965 loss: 1.8600592613220215, accuracy: 0.359375\n",
            "n_iters: 1975 loss: 1.8814036846160889, accuracy: 0.328125\n",
            "n_iters: 1985 loss: 1.7570149898529053, accuracy: 0.421875\n",
            "n_iters: 1995 loss: 1.687407374382019, accuracy: 0.4296875\n",
            "n_iters: 2005 loss: 1.7454155683517456, accuracy: 0.359375\n",
            "n_iters: 2005 test accuracy: 0.358 gradient_norm: 9.82770338854358\n",
            "n_iters: 2015 loss: 1.9989229440689087, accuracy: 0.3125\n",
            "n_iters: 2025 loss: 1.8062025308609009, accuracy: 0.34375\n",
            "n_iters: 2035 loss: 1.7992194890975952, accuracy: 0.328125\n",
            "n_iters: 2045 loss: 1.6985349655151367, accuracy: 0.3828125\n",
            "n_iters: 2055 loss: 1.8372653722763062, accuracy: 0.3515625\n",
            "n_iters: 2055 test accuracy: 0.36 gradient_norm: 11.187578918177431\n",
            "n_iters: 2065 loss: 1.7297041416168213, accuracy: 0.3828125\n",
            "n_iters: 2075 loss: 1.9136563539505005, accuracy: 0.328125\n",
            "n_iters: 2085 loss: 1.864007830619812, accuracy: 0.34375\n",
            "n_iters: 2095 loss: 1.770213007926941, accuracy: 0.4140625\n",
            "n_iters: 2105 loss: 1.8632276058197021, accuracy: 0.3046875\n",
            "n_iters: 2105 test accuracy: 0.374 gradient_norm: 11.231039196920854\n",
            "n_iters: 2115 loss: 1.8853886127471924, accuracy: 0.3359375\n",
            "n_iters: 2125 loss: 1.8802170753479004, accuracy: 0.328125\n",
            "n_iters: 2135 loss: 1.8709169626235962, accuracy: 0.3984375\n",
            "n_iters: 2145 loss: 1.5572364330291748, accuracy: 0.4375\n",
            "n_iters: 2155 loss: 1.9020802974700928, accuracy: 0.3046875\n",
            "n_iters: 2155 test accuracy: 0.35 gradient_norm: 12.643629639763608\n",
            "n_iters: 2165 loss: 1.9396616220474243, accuracy: 0.3046875\n",
            "n_iters: 2175 loss: 1.914355754852295, accuracy: 0.328125\n",
            "n_iters: 2185 loss: 1.8247010707855225, accuracy: 0.3515625\n",
            "n_iters: 2195 loss: 1.9003194570541382, accuracy: 0.3515625\n",
            "n_iters: 2205 loss: 1.6959052085876465, accuracy: 0.40625\n",
            "n_iters: 2205 test accuracy: 0.329 gradient_norm: 11.793701234637203\n",
            "n_iters: 2215 loss: 1.7883892059326172, accuracy: 0.421875\n",
            "n_iters: 2225 loss: 1.7399598360061646, accuracy: 0.421875\n",
            "n_iters: 2235 loss: 1.7390003204345703, accuracy: 0.421875\n",
            "n_iters: 2245 loss: 1.755185604095459, accuracy: 0.375\n",
            "n_iters: 2255 loss: 1.8123217821121216, accuracy: 0.40625\n",
            "n_iters: 2255 test accuracy: 0.354 gradient_norm: 9.71770191398505\n",
            "n_iters: 2265 loss: 2.0063891410827637, accuracy: 0.28125\n",
            "n_iters: 2275 loss: 1.7834081649780273, accuracy: 0.3984375\n",
            "n_iters: 2285 loss: 1.8179121017456055, accuracy: 0.3828125\n",
            "n_iters: 2295 loss: 1.901215672492981, accuracy: 0.3203125\n",
            "n_iters: 2305 loss: 1.7707840204238892, accuracy: 0.390625\n",
            "n_iters: 2305 test accuracy: 0.356 gradient_norm: 10.456091020020876\n",
            "n_iters: 2315 loss: 1.7446651458740234, accuracy: 0.375\n",
            "n_iters: 2325 loss: 1.8393256664276123, accuracy: 0.3359375\n",
            "n_iters: 2335 loss: 1.9950850009918213, accuracy: 0.3203125\n",
            "n_iters: 2345 loss: 1.8602426052093506, accuracy: 0.275\n",
            "n_iters: 2346 loss: 1.9446156024932861, accuracy: 0.3359375\n",
            "n_iters: 2346 test accuracy: 0.37 gradient_norm: 17.39508674924625\n",
            "n_iters: 2356 loss: 1.8362746238708496, accuracy: 0.3515625\n",
            "n_iters: 2366 loss: 1.7708382606506348, accuracy: 0.3515625\n",
            "n_iters: 2376 loss: 1.7750976085662842, accuracy: 0.3984375\n",
            "n_iters: 2386 loss: 1.806938648223877, accuracy: 0.421875\n",
            "n_iters: 2396 loss: 1.757394790649414, accuracy: 0.390625\n",
            "n_iters: 2396 test accuracy: 0.369 gradient_norm: 13.689268129641189\n",
            "n_iters: 2406 loss: 1.596022129058838, accuracy: 0.3984375\n",
            "n_iters: 2416 loss: 1.871253490447998, accuracy: 0.34375\n",
            "n_iters: 2426 loss: 1.7591793537139893, accuracy: 0.3515625\n",
            "n_iters: 2436 loss: 1.9116064310073853, accuracy: 0.3515625\n",
            "n_iters: 2446 loss: 1.819320797920227, accuracy: 0.3359375\n",
            "n_iters: 2446 test accuracy: 0.393 gradient_norm: 11.874805068969144\n",
            "n_iters: 2456 loss: 1.7895861864089966, accuracy: 0.40625\n",
            "n_iters: 2466 loss: 1.7772008180618286, accuracy: 0.3671875\n",
            "n_iters: 2476 loss: 1.7895840406417847, accuracy: 0.3828125\n",
            "n_iters: 2486 loss: 1.785806655883789, accuracy: 0.3671875\n",
            "n_iters: 2496 loss: 1.9893989562988281, accuracy: 0.3125\n",
            "n_iters: 2496 test accuracy: 0.37 gradient_norm: 11.544368867689963\n",
            "n_iters: 2506 loss: 1.9020735025405884, accuracy: 0.375\n",
            "n_iters: 2516 loss: 2.0196611881256104, accuracy: 0.234375\n",
            "n_iters: 2526 loss: 1.8934592008590698, accuracy: 0.3046875\n",
            "n_iters: 2536 loss: 1.7242014408111572, accuracy: 0.3671875\n",
            "n_iters: 2546 loss: 1.8614219427108765, accuracy: 0.359375\n",
            "n_iters: 2546 test accuracy: 0.344 gradient_norm: 12.379114372062835\n",
            "n_iters: 2556 loss: 1.6619855165481567, accuracy: 0.421875\n",
            "n_iters: 2566 loss: 1.7473496198654175, accuracy: 0.359375\n",
            "n_iters: 2576 loss: 1.8701564073562622, accuracy: 0.3203125\n",
            "n_iters: 2586 loss: 1.7249561548233032, accuracy: 0.3828125\n",
            "n_iters: 2596 loss: 1.8805484771728516, accuracy: 0.3515625\n",
            "n_iters: 2596 test accuracy: 0.37 gradient_norm: 14.018191202844594\n",
            "n_iters: 2606 loss: 1.7372710704803467, accuracy: 0.421875\n",
            "n_iters: 2616 loss: 1.803961157798767, accuracy: 0.4375\n",
            "n_iters: 2626 loss: 1.897141456604004, accuracy: 0.375\n",
            "n_iters: 2636 loss: 1.8333239555358887, accuracy: 0.3515625\n",
            "n_iters: 2646 loss: 1.9762822389602661, accuracy: 0.390625\n",
            "n_iters: 2646 test accuracy: 0.359 gradient_norm: 12.353936496279005\n",
            "n_iters: 2656 loss: 1.935965895652771, accuracy: 0.328125\n",
            "n_iters: 2666 loss: 1.8493216037750244, accuracy: 0.375\n",
            "n_iters: 2676 loss: 1.7495052814483643, accuracy: 0.3984375\n",
            "n_iters: 2686 loss: 1.750312328338623, accuracy: 0.40625\n",
            "n_iters: 2696 loss: 1.8173956871032715, accuracy: 0.390625\n",
            "n_iters: 2696 test accuracy: 0.381 gradient_norm: 10.483610710303326\n",
            "n_iters: 2706 loss: 1.965470314025879, accuracy: 0.34375\n",
            "n_iters: 2716 loss: 1.8263671398162842, accuracy: 0.3359375\n",
            "n_iters: 2726 loss: 1.7515586614608765, accuracy: 0.3984375\n",
            "n_iters: 2736 loss: 1.89404296875, accuracy: 0.375\n",
            "n_iters: 2737 loss: 1.913251280784607, accuracy: 0.3671875\n",
            "n_iters: 2737 test accuracy: 0.352 gradient_norm: 11.858975968573624\n",
            "n_iters: 2747 loss: 1.742165207862854, accuracy: 0.3515625\n",
            "n_iters: 2757 loss: 1.8337829113006592, accuracy: 0.375\n",
            "n_iters: 2767 loss: 1.8120067119598389, accuracy: 0.3984375\n",
            "n_iters: 2777 loss: 1.9742096662521362, accuracy: 0.3359375\n",
            "n_iters: 2787 loss: 1.7249083518981934, accuracy: 0.359375\n",
            "n_iters: 2787 test accuracy: 0.365 gradient_norm: 12.425680515234266\n",
            "n_iters: 2797 loss: 1.810684084892273, accuracy: 0.3828125\n",
            "n_iters: 2807 loss: 1.790581464767456, accuracy: 0.3828125\n",
            "n_iters: 2817 loss: 1.845647931098938, accuracy: 0.3671875\n",
            "n_iters: 2827 loss: 1.784531831741333, accuracy: 0.359375\n",
            "n_iters: 2837 loss: 1.7228573560714722, accuracy: 0.40625\n",
            "n_iters: 2837 test accuracy: 0.352 gradient_norm: 11.948639976782934\n",
            "n_iters: 2847 loss: 1.9043222665786743, accuracy: 0.328125\n",
            "n_iters: 2857 loss: 1.6888597011566162, accuracy: 0.46875\n",
            "n_iters: 2867 loss: 1.6360936164855957, accuracy: 0.3828125\n",
            "n_iters: 2877 loss: 1.8058115243911743, accuracy: 0.34375\n",
            "n_iters: 2887 loss: 1.7767401933670044, accuracy: 0.390625\n",
            "n_iters: 2887 test accuracy: 0.379 gradient_norm: 12.774278079801482\n",
            "n_iters: 2897 loss: 1.6070663928985596, accuracy: 0.5\n",
            "n_iters: 2907 loss: 1.772482991218567, accuracy: 0.3515625\n",
            "n_iters: 2917 loss: 1.6697214841842651, accuracy: 0.4140625\n",
            "n_iters: 2927 loss: 1.7547458410263062, accuracy: 0.3671875\n",
            "n_iters: 2937 loss: 1.7785637378692627, accuracy: 0.3671875\n",
            "n_iters: 2937 test accuracy: 0.361 gradient_norm: 12.711213366976471\n",
            "n_iters: 2947 loss: 1.6478463411331177, accuracy: 0.3984375\n",
            "n_iters: 2957 loss: 1.9237948656082153, accuracy: 0.3359375\n",
            "n_iters: 2967 loss: 1.8181021213531494, accuracy: 0.4375\n",
            "n_iters: 2977 loss: 1.8364858627319336, accuracy: 0.3515625\n",
            "n_iters: 2987 loss: 1.8278546333312988, accuracy: 0.390625\n",
            "n_iters: 2987 test accuracy: 0.362 gradient_norm: 10.40012500591658\n",
            "n_iters: 2997 loss: 1.8580955266952515, accuracy: 0.3515625\n",
            "n_iters: 3007 loss: 1.6882692575454712, accuracy: 0.3984375\n",
            "n_iters: 3017 loss: 1.6685835123062134, accuracy: 0.40625\n",
            "n_iters: 3027 loss: 1.8759971857070923, accuracy: 0.40625\n",
            "n_iters: 3037 loss: 1.6977238655090332, accuracy: 0.4375\n",
            "n_iters: 3037 test accuracy: 0.381 gradient_norm: 14.645583153252447\n",
            "n_iters: 3047 loss: 1.867355465888977, accuracy: 0.3203125\n",
            "n_iters: 3057 loss: 1.6766246557235718, accuracy: 0.3828125\n",
            "n_iters: 3067 loss: 1.7863186597824097, accuracy: 0.3515625\n",
            "n_iters: 3077 loss: 1.8771636486053467, accuracy: 0.3359375\n",
            "n_iters: 3087 loss: 1.8356051445007324, accuracy: 0.3828125\n",
            "n_iters: 3087 test accuracy: 0.403 gradient_norm: 11.052777784668683\n",
            "n_iters: 3097 loss: 1.717694878578186, accuracy: 0.3984375\n",
            "n_iters: 3107 loss: 1.7546658515930176, accuracy: 0.421875\n",
            "n_iters: 3117 loss: 1.8629944324493408, accuracy: 0.3515625\n",
            "n_iters: 3127 loss: 2.0214898586273193, accuracy: 0.275\n",
            "n_iters: 3128 loss: 1.8222929239273071, accuracy: 0.359375\n",
            "n_iters: 3128 test accuracy: 0.383 gradient_norm: 13.23801645835937\n",
            "n_iters: 3138 loss: 1.754456639289856, accuracy: 0.3046875\n",
            "n_iters: 3148 loss: 1.7209999561309814, accuracy: 0.4765625\n",
            "n_iters: 3158 loss: 1.7864832878112793, accuracy: 0.3828125\n",
            "n_iters: 3168 loss: 1.6793304681777954, accuracy: 0.3671875\n",
            "n_iters: 3178 loss: 1.6807328462600708, accuracy: 0.421875\n",
            "n_iters: 3178 test accuracy: 0.386 gradient_norm: 10.951815177843628\n",
            "n_iters: 3188 loss: 1.8850854635238647, accuracy: 0.3671875\n",
            "n_iters: 3198 loss: 1.8188635110855103, accuracy: 0.359375\n",
            "n_iters: 3208 loss: 1.5987982749938965, accuracy: 0.421875\n",
            "n_iters: 3218 loss: 1.7770061492919922, accuracy: 0.3671875\n",
            "n_iters: 3228 loss: 1.800229787826538, accuracy: 0.3203125\n",
            "n_iters: 3228 test accuracy: 0.375 gradient_norm: 12.917479336256555\n",
            "n_iters: 3238 loss: 1.7407490015029907, accuracy: 0.3828125\n",
            "n_iters: 3248 loss: 1.6971995830535889, accuracy: 0.453125\n",
            "n_iters: 3258 loss: 1.857922077178955, accuracy: 0.3515625\n",
            "n_iters: 3268 loss: 1.9169650077819824, accuracy: 0.3828125\n",
            "n_iters: 3278 loss: 1.7542701959609985, accuracy: 0.3671875\n",
            "n_iters: 3278 test accuracy: 0.374 gradient_norm: 11.04341369552338\n",
            "n_iters: 3288 loss: 1.7867978811264038, accuracy: 0.3828125\n",
            "n_iters: 3298 loss: 1.8774807453155518, accuracy: 0.3125\n",
            "n_iters: 3308 loss: 1.630281686782837, accuracy: 0.4296875\n",
            "n_iters: 3318 loss: 1.760785460472107, accuracy: 0.4140625\n",
            "n_iters: 3328 loss: 1.7640244960784912, accuracy: 0.3515625\n",
            "n_iters: 3328 test accuracy: 0.378 gradient_norm: 14.45369137453955\n",
            "n_iters: 3338 loss: 1.5646750926971436, accuracy: 0.5\n",
            "n_iters: 3348 loss: 1.8469493389129639, accuracy: 0.3203125\n",
            "n_iters: 3358 loss: 1.7718274593353271, accuracy: 0.40625\n",
            "n_iters: 3368 loss: 1.8424837589263916, accuracy: 0.3671875\n",
            "n_iters: 3378 loss: 1.8338686227798462, accuracy: 0.296875\n",
            "n_iters: 3378 test accuracy: 0.402 gradient_norm: 13.370247435077827\n",
            "n_iters: 3388 loss: 1.6711872816085815, accuracy: 0.421875\n",
            "n_iters: 3398 loss: 1.860745906829834, accuracy: 0.3671875\n",
            "n_iters: 3408 loss: 1.8796401023864746, accuracy: 0.421875\n",
            "n_iters: 3418 loss: 1.8442367315292358, accuracy: 0.3125\n",
            "n_iters: 3428 loss: 1.6695754528045654, accuracy: 0.421875\n",
            "n_iters: 3428 test accuracy: 0.372 gradient_norm: 12.007577045315015\n",
            "n_iters: 3438 loss: 1.834596037864685, accuracy: 0.3828125\n",
            "n_iters: 3448 loss: 1.7517688274383545, accuracy: 0.3671875\n",
            "n_iters: 3458 loss: 1.7294025421142578, accuracy: 0.3984375\n",
            "n_iters: 3468 loss: 1.9202009439468384, accuracy: 0.3125\n",
            "n_iters: 3478 loss: 1.8167650699615479, accuracy: 0.40625\n",
            "n_iters: 3478 test accuracy: 0.37 gradient_norm: 16.359683465091663\n",
            "n_iters: 3488 loss: 1.5758360624313354, accuracy: 0.4765625\n",
            "n_iters: 3498 loss: 1.7937116622924805, accuracy: 0.40625\n",
            "n_iters: 3508 loss: 1.6698771715164185, accuracy: 0.4296875\n",
            "n_iters: 3518 loss: 1.9067671298980713, accuracy: 0.325\n",
            "n_iters: 3519 loss: 1.7240806818008423, accuracy: 0.3828125\n",
            "n_iters: 3519 test accuracy: 0.361 gradient_norm: 9.03314460176388\n",
            "n_iters: 3529 loss: 1.7265548706054688, accuracy: 0.421875\n",
            "n_iters: 3539 loss: 1.714558720588684, accuracy: 0.4296875\n",
            "n_iters: 3549 loss: 1.7034671306610107, accuracy: 0.3671875\n",
            "n_iters: 3559 loss: 1.8145458698272705, accuracy: 0.40625\n",
            "n_iters: 3569 loss: 1.8229832649230957, accuracy: 0.328125\n",
            "n_iters: 3569 test accuracy: 0.361 gradient_norm: 10.501581292125866\n",
            "n_iters: 3579 loss: 1.731817364692688, accuracy: 0.3515625\n",
            "n_iters: 3589 loss: 1.8188167810440063, accuracy: 0.3671875\n",
            "n_iters: 3599 loss: 1.7028740644454956, accuracy: 0.3828125\n",
            "n_iters: 3609 loss: 1.7434558868408203, accuracy: 0.40625\n",
            "n_iters: 3619 loss: 1.8725841045379639, accuracy: 0.3828125\n",
            "n_iters: 3619 test accuracy: 0.38 gradient_norm: 13.251758697333694\n",
            "n_iters: 3629 loss: 1.5983929634094238, accuracy: 0.4453125\n",
            "n_iters: 3639 loss: 1.774440884590149, accuracy: 0.390625\n",
            "n_iters: 3649 loss: 1.6207659244537354, accuracy: 0.390625\n",
            "n_iters: 3659 loss: 1.9991198778152466, accuracy: 0.3125\n",
            "n_iters: 3669 loss: 1.7884174585342407, accuracy: 0.4296875\n",
            "n_iters: 3669 test accuracy: 0.372 gradient_norm: 13.12263578806567\n",
            "n_iters: 3679 loss: 2.062424659729004, accuracy: 0.296875\n",
            "n_iters: 3689 loss: 1.7851016521453857, accuracy: 0.4140625\n",
            "n_iters: 3699 loss: 1.89799165725708, accuracy: 0.34375\n",
            "n_iters: 3709 loss: 1.7972891330718994, accuracy: 0.34375\n",
            "n_iters: 3719 loss: 1.7274507284164429, accuracy: 0.3828125\n",
            "n_iters: 3719 test accuracy: 0.382 gradient_norm: 12.7963240584837\n",
            "n_iters: 3729 loss: 1.6099790334701538, accuracy: 0.46875\n",
            "n_iters: 3739 loss: 1.669996738433838, accuracy: 0.4375\n",
            "n_iters: 3749 loss: 1.7832655906677246, accuracy: 0.3515625\n",
            "n_iters: 3759 loss: 1.9231655597686768, accuracy: 0.3203125\n",
            "n_iters: 3769 loss: 1.8120806217193604, accuracy: 0.3828125\n",
            "n_iters: 3769 test accuracy: 0.356 gradient_norm: 10.669139533878994\n",
            "n_iters: 3779 loss: 1.6845715045928955, accuracy: 0.4140625\n",
            "n_iters: 3789 loss: 1.7700610160827637, accuracy: 0.3515625\n",
            "n_iters: 3799 loss: 1.7017719745635986, accuracy: 0.3671875\n",
            "n_iters: 3809 loss: 1.735802412033081, accuracy: 0.421875\n",
            "n_iters: 3819 loss: 1.8484928607940674, accuracy: 0.2890625\n",
            "n_iters: 3819 test accuracy: 0.374 gradient_norm: 14.753174150026261\n",
            "n_iters: 3829 loss: 1.7704875469207764, accuracy: 0.3984375\n",
            "n_iters: 3839 loss: 1.638015627861023, accuracy: 0.40625\n",
            "n_iters: 3849 loss: 1.7807549238204956, accuracy: 0.3125\n",
            "n_iters: 3859 loss: 1.7452301979064941, accuracy: 0.421875\n",
            "n_iters: 3869 loss: 1.8345789909362793, accuracy: 0.3671875\n",
            "n_iters: 3869 test accuracy: 0.373 gradient_norm: 13.685721137450427\n",
            "n_iters: 3879 loss: 1.6792134046554565, accuracy: 0.46875\n",
            "n_iters: 3889 loss: 1.7449820041656494, accuracy: 0.40625\n",
            "n_iters: 3899 loss: 1.7507586479187012, accuracy: 0.4140625\n",
            "n_iters: 3909 loss: 1.879565954208374, accuracy: 0.3125\n",
            "n_iters: 3910 loss: 1.9160021543502808, accuracy: 0.328125\n",
            "n_iters: 3910 test accuracy: 0.397 gradient_norm: 12.62942376614456\n",
            "n_iters: 3920 loss: 1.7277237176895142, accuracy: 0.4140625\n",
            "n_iters: 3930 loss: 1.9808889627456665, accuracy: 0.296875\n",
            "n_iters: 3940 loss: 1.755663275718689, accuracy: 0.3515625\n",
            "n_iters: 3950 loss: 2.045128345489502, accuracy: 0.296875\n",
            "n_iters: 3960 loss: 1.7785673141479492, accuracy: 0.4140625\n",
            "n_iters: 3960 test accuracy: 0.372 gradient_norm: 10.298667781646364\n",
            "n_iters: 3970 loss: 1.7882598638534546, accuracy: 0.3359375\n",
            "n_iters: 3980 loss: 1.6677287817001343, accuracy: 0.390625\n",
            "n_iters: 3990 loss: 1.847776174545288, accuracy: 0.390625\n",
            "n_iters: 4000 loss: 1.7603929042816162, accuracy: 0.3984375\n",
            "n_iters: 4010 loss: 1.703663945198059, accuracy: 0.3671875\n",
            "n_iters: 4010 test accuracy: 0.375 gradient_norm: 12.80692305900535\n",
            "n_iters: 4020 loss: 1.792893886566162, accuracy: 0.4140625\n",
            "n_iters: 4030 loss: 1.6280391216278076, accuracy: 0.453125\n",
            "n_iters: 4040 loss: 1.7134233713150024, accuracy: 0.3828125\n",
            "n_iters: 4050 loss: 1.751220464706421, accuracy: 0.4453125\n",
            "n_iters: 4060 loss: 1.8725937604904175, accuracy: 0.34375\n",
            "n_iters: 4060 test accuracy: 0.396 gradient_norm: 12.629690103679698\n",
            "n_iters: 4070 loss: 1.826582670211792, accuracy: 0.328125\n",
            "n_iters: 4080 loss: 1.7521003484725952, accuracy: 0.390625\n",
            "n_iters: 4090 loss: 1.7692759037017822, accuracy: 0.3671875\n",
            "n_iters: 4100 loss: 1.6688529253005981, accuracy: 0.3984375\n",
            "n_iters: 4110 loss: 1.6902103424072266, accuracy: 0.421875\n",
            "n_iters: 4110 test accuracy: 0.366 gradient_norm: 9.15100901511155\n",
            "n_iters: 4120 loss: 1.774468183517456, accuracy: 0.421875\n",
            "n_iters: 4130 loss: 1.7768614292144775, accuracy: 0.3359375\n",
            "n_iters: 4140 loss: 1.8938361406326294, accuracy: 0.3359375\n",
            "n_iters: 4150 loss: 1.8000190258026123, accuracy: 0.3359375\n",
            "n_iters: 4160 loss: 1.6733629703521729, accuracy: 0.4296875\n",
            "n_iters: 4160 test accuracy: 0.362 gradient_norm: 13.955816978769853\n",
            "n_iters: 4170 loss: 1.5321602821350098, accuracy: 0.453125\n",
            "n_iters: 4180 loss: 1.7506532669067383, accuracy: 0.4375\n",
            "n_iters: 4190 loss: 1.8068442344665527, accuracy: 0.4140625\n",
            "n_iters: 4200 loss: 1.7496877908706665, accuracy: 0.4140625\n",
            "n_iters: 4210 loss: 1.6510893106460571, accuracy: 0.4453125\n",
            "n_iters: 4210 test accuracy: 0.394 gradient_norm: 11.748869149490261\n",
            "n_iters: 4220 loss: 1.817854642868042, accuracy: 0.3671875\n",
            "n_iters: 4230 loss: 1.646568775177002, accuracy: 0.40625\n",
            "n_iters: 4240 loss: 1.8244799375534058, accuracy: 0.3828125\n",
            "n_iters: 4250 loss: 1.6296848058700562, accuracy: 0.4140625\n",
            "n_iters: 4260 loss: 1.7037458419799805, accuracy: 0.390625\n",
            "n_iters: 4260 test accuracy: 0.388 gradient_norm: 11.931072217077435\n",
            "n_iters: 4270 loss: 1.7853580713272095, accuracy: 0.359375\n",
            "n_iters: 4280 loss: 1.7230767011642456, accuracy: 0.3984375\n",
            "n_iters: 4290 loss: 1.8396137952804565, accuracy: 0.3671875\n",
            "n_iters: 4300 loss: 1.9766706228256226, accuracy: 0.3\n",
            "n_iters: 4301 loss: 1.827079176902771, accuracy: 0.34375\n",
            "n_iters: 4301 test accuracy: 0.38 gradient_norm: 11.684408793725936\n",
            "n_iters: 4311 loss: 1.7467354536056519, accuracy: 0.390625\n",
            "n_iters: 4321 loss: 1.5920532941818237, accuracy: 0.3671875\n",
            "n_iters: 4331 loss: 1.7333511114120483, accuracy: 0.390625\n",
            "n_iters: 4341 loss: 1.5522924661636353, accuracy: 0.4765625\n",
            "n_iters: 4351 loss: 1.8772518634796143, accuracy: 0.328125\n",
            "n_iters: 4351 test accuracy: 0.387 gradient_norm: 11.378545885217646\n",
            "n_iters: 4361 loss: 1.608899712562561, accuracy: 0.3984375\n",
            "n_iters: 4371 loss: 1.8756239414215088, accuracy: 0.3125\n",
            "n_iters: 4381 loss: 1.7887030839920044, accuracy: 0.3828125\n",
            "n_iters: 4391 loss: 1.725684404373169, accuracy: 0.359375\n",
            "n_iters: 4401 loss: 1.8415887355804443, accuracy: 0.28125\n",
            "n_iters: 4401 test accuracy: 0.368 gradient_norm: 12.980479786021453\n",
            "n_iters: 4411 loss: 1.8368942737579346, accuracy: 0.3984375\n",
            "n_iters: 4421 loss: 1.6370950937271118, accuracy: 0.4609375\n",
            "n_iters: 4431 loss: 1.8983771800994873, accuracy: 0.3125\n",
            "n_iters: 4441 loss: 1.7295444011688232, accuracy: 0.375\n",
            "n_iters: 4451 loss: 1.7246437072753906, accuracy: 0.375\n",
            "n_iters: 4451 test accuracy: 0.395 gradient_norm: 11.143831906153263\n",
            "n_iters: 4461 loss: 1.739429235458374, accuracy: 0.4375\n",
            "n_iters: 4471 loss: 1.691291093826294, accuracy: 0.3671875\n",
            "n_iters: 4481 loss: 1.674704909324646, accuracy: 0.4296875\n",
            "n_iters: 4491 loss: 1.6930793523788452, accuracy: 0.4140625\n",
            "n_iters: 4501 loss: 1.8047749996185303, accuracy: 0.3984375\n",
            "n_iters: 4501 test accuracy: 0.391 gradient_norm: 11.419683191704083\n",
            "n_iters: 4511 loss: 1.8012336492538452, accuracy: 0.390625\n",
            "n_iters: 4521 loss: 1.5602388381958008, accuracy: 0.421875\n",
            "n_iters: 4531 loss: 1.8089780807495117, accuracy: 0.3984375\n",
            "n_iters: 4541 loss: 1.613183617591858, accuracy: 0.4296875\n",
            "n_iters: 4551 loss: 1.808963418006897, accuracy: 0.375\n",
            "n_iters: 4551 test accuracy: 0.367 gradient_norm: 10.474708985126203\n",
            "n_iters: 4561 loss: 1.8162662982940674, accuracy: 0.3828125\n",
            "n_iters: 4571 loss: 1.776841402053833, accuracy: 0.3828125\n",
            "n_iters: 4581 loss: 1.734386920928955, accuracy: 0.421875\n",
            "n_iters: 4591 loss: 1.7614480257034302, accuracy: 0.421875\n",
            "n_iters: 4601 loss: 1.818520188331604, accuracy: 0.3515625\n",
            "n_iters: 4601 test accuracy: 0.382 gradient_norm: 12.708684708035308\n",
            "n_iters: 4611 loss: 1.6476801633834839, accuracy: 0.3828125\n",
            "n_iters: 4621 loss: 1.5372735261917114, accuracy: 0.4921875\n",
            "n_iters: 4631 loss: 1.8059720993041992, accuracy: 0.34375\n",
            "n_iters: 4641 loss: 1.780834436416626, accuracy: 0.3671875\n",
            "n_iters: 4651 loss: 1.784836769104004, accuracy: 0.40625\n",
            "n_iters: 4651 test accuracy: 0.379 gradient_norm: 11.15260063991679\n",
            "n_iters: 4661 loss: 1.803188443183899, accuracy: 0.3359375\n",
            "n_iters: 4671 loss: 1.6569795608520508, accuracy: 0.3515625\n",
            "n_iters: 4681 loss: 1.7319222688674927, accuracy: 0.484375\n",
            "n_iters: 4691 loss: 2.004296064376831, accuracy: 0.3125\n",
            "n_iters: 4692 loss: 1.846091389656067, accuracy: 0.3359375\n",
            "n_iters: 4692 test accuracy: 0.397 gradient_norm: 11.167071440970973\n",
            "n_iters: 4702 loss: 1.9260061979293823, accuracy: 0.3515625\n",
            "n_iters: 4712 loss: 1.686400055885315, accuracy: 0.4375\n",
            "n_iters: 4722 loss: 1.6591705083847046, accuracy: 0.375\n",
            "n_iters: 4732 loss: 1.8343944549560547, accuracy: 0.328125\n",
            "n_iters: 4742 loss: 1.7748943567276, accuracy: 0.3984375\n",
            "n_iters: 4742 test accuracy: 0.377 gradient_norm: 12.51216917675473\n",
            "n_iters: 4752 loss: 1.8059453964233398, accuracy: 0.3828125\n",
            "n_iters: 4762 loss: 1.7841713428497314, accuracy: 0.3984375\n",
            "n_iters: 4772 loss: 1.5831363201141357, accuracy: 0.484375\n",
            "n_iters: 4782 loss: 1.6539901494979858, accuracy: 0.453125\n",
            "n_iters: 4792 loss: 1.734450340270996, accuracy: 0.359375\n",
            "n_iters: 4792 test accuracy: 0.366 gradient_norm: 11.964420962707015\n",
            "n_iters: 4802 loss: 1.7329915761947632, accuracy: 0.375\n",
            "n_iters: 4812 loss: 1.8682687282562256, accuracy: 0.3125\n",
            "n_iters: 4822 loss: 1.7173490524291992, accuracy: 0.40625\n",
            "n_iters: 4832 loss: 1.7329386472702026, accuracy: 0.3828125\n",
            "n_iters: 4842 loss: 1.8229066133499146, accuracy: 0.3984375\n",
            "n_iters: 4842 test accuracy: 0.385 gradient_norm: 13.250510893720032\n",
            "n_iters: 4852 loss: 1.7712832689285278, accuracy: 0.328125\n",
            "n_iters: 4862 loss: 1.9080530405044556, accuracy: 0.3828125\n",
            "n_iters: 4872 loss: 1.6625791788101196, accuracy: 0.421875\n",
            "n_iters: 4882 loss: 1.6506812572479248, accuracy: 0.359375\n",
            "n_iters: 4892 loss: 1.8933746814727783, accuracy: 0.3359375\n",
            "n_iters: 4892 test accuracy: 0.372 gradient_norm: 15.657875042655492\n",
            "n_iters: 4902 loss: 1.9385716915130615, accuracy: 0.2890625\n",
            "n_iters: 4912 loss: 1.8568450212478638, accuracy: 0.3515625\n",
            "n_iters: 4922 loss: 1.7907757759094238, accuracy: 0.390625\n",
            "n_iters: 4932 loss: 1.755385160446167, accuracy: 0.3984375\n",
            "n_iters: 4942 loss: 1.664912223815918, accuracy: 0.4296875\n",
            "n_iters: 4942 test accuracy: 0.378 gradient_norm: 13.043066826631366\n",
            "n_iters: 4952 loss: 1.745111107826233, accuracy: 0.4296875\n",
            "n_iters: 4962 loss: 1.7233861684799194, accuracy: 0.3828125\n",
            "n_iters: 4972 loss: 1.8491801023483276, accuracy: 0.390625\n",
            "n_iters: 4982 loss: 1.8087078332901, accuracy: 0.3671875\n",
            "n_iters: 4992 loss: 1.6295526027679443, accuracy: 0.3984375\n",
            "n_iters: 4992 test accuracy: 0.382 gradient_norm: 11.114795907081907\n",
            "n_iters: 5002 loss: 1.6115338802337646, accuracy: 0.421875\n",
            "n_iters: 5012 loss: 1.7189207077026367, accuracy: 0.4453125\n",
            "n_iters: 5022 loss: 1.7275232076644897, accuracy: 0.3828125\n",
            "n_iters: 5032 loss: 1.7139637470245361, accuracy: 0.40625\n",
            "n_iters: 5042 loss: 1.647376537322998, accuracy: 0.4296875\n",
            "n_iters: 5042 test accuracy: 0.381 gradient_norm: 10.963754691660231\n",
            "n_iters: 5052 loss: 1.6894900798797607, accuracy: 0.390625\n",
            "n_iters: 5062 loss: 1.663569688796997, accuracy: 0.4609375\n",
            "n_iters: 5072 loss: 1.5650907754898071, accuracy: 0.4140625\n",
            "n_iters: 5082 loss: 2.002016067504883, accuracy: 0.3\n",
            "n_iters: 5083 loss: 1.7834457159042358, accuracy: 0.40625\n",
            "n_iters: 5083 test accuracy: 0.381 gradient_norm: 10.961417655672573\n",
            "n_iters: 5093 loss: 1.814707636833191, accuracy: 0.375\n",
            "n_iters: 5103 loss: 1.8802571296691895, accuracy: 0.3203125\n",
            "n_iters: 5113 loss: 1.7850531339645386, accuracy: 0.4375\n",
            "n_iters: 5123 loss: 1.7399505376815796, accuracy: 0.3125\n",
            "n_iters: 5133 loss: 1.5527828931808472, accuracy: 0.453125\n",
            "n_iters: 5133 test accuracy: 0.389 gradient_norm: 10.976235843863941\n",
            "n_iters: 5143 loss: 1.8224149942398071, accuracy: 0.34375\n",
            "n_iters: 5153 loss: 1.7710130214691162, accuracy: 0.390625\n",
            "n_iters: 5163 loss: 1.8446261882781982, accuracy: 0.2734375\n",
            "n_iters: 5173 loss: 1.8943549394607544, accuracy: 0.3359375\n",
            "n_iters: 5183 loss: 1.6399214267730713, accuracy: 0.40625\n",
            "n_iters: 5183 test accuracy: 0.377 gradient_norm: 14.38912720159968\n",
            "n_iters: 5193 loss: 1.7906986474990845, accuracy: 0.2890625\n",
            "n_iters: 5203 loss: 1.6085894107818604, accuracy: 0.421875\n",
            "n_iters: 5213 loss: 1.5854859352111816, accuracy: 0.421875\n",
            "n_iters: 5223 loss: 1.652454137802124, accuracy: 0.40625\n",
            "n_iters: 5233 loss: 1.7887904644012451, accuracy: 0.40625\n",
            "n_iters: 5233 test accuracy: 0.36 gradient_norm: 10.817560709736368\n",
            "n_iters: 5243 loss: 1.6909451484680176, accuracy: 0.40625\n",
            "n_iters: 5253 loss: 1.8147892951965332, accuracy: 0.328125\n",
            "n_iters: 5263 loss: 1.7207753658294678, accuracy: 0.375\n",
            "n_iters: 5273 loss: 1.809719443321228, accuracy: 0.390625\n",
            "n_iters: 5283 loss: 1.7162035703659058, accuracy: 0.4375\n",
            "n_iters: 5283 test accuracy: 0.369 gradient_norm: 13.443418040174807\n",
            "n_iters: 5293 loss: 1.7676243782043457, accuracy: 0.3828125\n",
            "n_iters: 5303 loss: 1.6940408945083618, accuracy: 0.3671875\n",
            "n_iters: 5313 loss: 1.6694097518920898, accuracy: 0.3984375\n",
            "n_iters: 5323 loss: 1.6521793603897095, accuracy: 0.46875\n",
            "n_iters: 5333 loss: 1.7510689496994019, accuracy: 0.3984375\n",
            "n_iters: 5333 test accuracy: 0.379 gradient_norm: 12.890580603531454\n",
            "n_iters: 5343 loss: 1.6498774290084839, accuracy: 0.40625\n",
            "n_iters: 5353 loss: 1.9777820110321045, accuracy: 0.234375\n",
            "n_iters: 5363 loss: 1.6250853538513184, accuracy: 0.4765625\n",
            "n_iters: 5373 loss: 1.8116580247879028, accuracy: 0.3359375\n",
            "n_iters: 5383 loss: 1.7283046245574951, accuracy: 0.40625\n",
            "n_iters: 5383 test accuracy: 0.418 gradient_norm: 10.531223716629608\n",
            "n_iters: 5393 loss: 1.7472482919692993, accuracy: 0.3671875\n",
            "n_iters: 5403 loss: 1.6725605726242065, accuracy: 0.4453125\n",
            "n_iters: 5413 loss: 1.6998199224472046, accuracy: 0.3515625\n",
            "n_iters: 5423 loss: 1.6030220985412598, accuracy: 0.3984375\n",
            "n_iters: 5433 loss: 1.8167266845703125, accuracy: 0.40625\n",
            "n_iters: 5433 test accuracy: 0.385 gradient_norm: 12.785853903151544\n",
            "n_iters: 5443 loss: 1.7419649362564087, accuracy: 0.3515625\n",
            "n_iters: 5453 loss: 1.662361741065979, accuracy: 0.4375\n",
            "n_iters: 5463 loss: 1.6918072700500488, accuracy: 0.390625\n",
            "n_iters: 5473 loss: 1.727113962173462, accuracy: 0.4\n",
            "n_iters: 5474 loss: 1.7103320360183716, accuracy: 0.3828125\n",
            "n_iters: 5474 test accuracy: 0.395 gradient_norm: 12.646359880211131\n",
            "n_iters: 5484 loss: 1.763088345527649, accuracy: 0.3515625\n",
            "n_iters: 5494 loss: 1.7153469324111938, accuracy: 0.4453125\n",
            "n_iters: 5504 loss: 1.6943137645721436, accuracy: 0.4375\n",
            "n_iters: 5514 loss: 1.6659029722213745, accuracy: 0.4375\n",
            "n_iters: 5524 loss: 1.6413824558258057, accuracy: 0.4609375\n",
            "n_iters: 5524 test accuracy: 0.397 gradient_norm: 11.38005867200272\n",
            "n_iters: 5534 loss: 1.7816442251205444, accuracy: 0.296875\n",
            "n_iters: 5544 loss: 1.5614923238754272, accuracy: 0.4375\n",
            "n_iters: 5554 loss: 1.7287030220031738, accuracy: 0.3984375\n",
            "n_iters: 5564 loss: 1.7979905605316162, accuracy: 0.3671875\n",
            "n_iters: 5574 loss: 1.5883880853652954, accuracy: 0.5078125\n",
            "n_iters: 5574 test accuracy: 0.387 gradient_norm: 10.738540386895627\n",
            "n_iters: 5584 loss: 1.6423524618148804, accuracy: 0.4375\n",
            "n_iters: 5594 loss: 1.6727272272109985, accuracy: 0.4296875\n",
            "n_iters: 5604 loss: 1.6089249849319458, accuracy: 0.3984375\n",
            "n_iters: 5614 loss: 1.6977092027664185, accuracy: 0.3984375\n",
            "n_iters: 5624 loss: 1.4975038766860962, accuracy: 0.46875\n",
            "n_iters: 5624 test accuracy: 0.399 gradient_norm: 8.669511066080991\n",
            "n_iters: 5634 loss: 1.667170763015747, accuracy: 0.3984375\n",
            "n_iters: 5644 loss: 1.5754791498184204, accuracy: 0.4296875\n",
            "n_iters: 5654 loss: 1.6552734375, accuracy: 0.3671875\n",
            "n_iters: 5664 loss: 1.6686915159225464, accuracy: 0.421875\n",
            "n_iters: 5674 loss: 1.7807828187942505, accuracy: 0.4375\n",
            "n_iters: 5674 test accuracy: 0.403 gradient_norm: 10.410240473394596\n",
            "n_iters: 5684 loss: 1.686267375946045, accuracy: 0.421875\n",
            "n_iters: 5694 loss: 1.7304104566574097, accuracy: 0.3828125\n",
            "n_iters: 5704 loss: 1.7199426889419556, accuracy: 0.3515625\n",
            "n_iters: 5714 loss: 1.5885614156723022, accuracy: 0.4296875\n",
            "n_iters: 5724 loss: 1.6412893533706665, accuracy: 0.328125\n",
            "n_iters: 5724 test accuracy: 0.402 gradient_norm: 10.971059918578474\n",
            "n_iters: 5734 loss: 1.6903990507125854, accuracy: 0.484375\n",
            "n_iters: 5744 loss: 1.679608702659607, accuracy: 0.3984375\n",
            "n_iters: 5754 loss: 1.746370553970337, accuracy: 0.359375\n",
            "n_iters: 5764 loss: 1.9864333868026733, accuracy: 0.328125\n",
            "n_iters: 5774 loss: 1.661810040473938, accuracy: 0.390625\n",
            "n_iters: 5774 test accuracy: 0.351 gradient_norm: 12.209707223860203\n",
            "n_iters: 5784 loss: 1.6925016641616821, accuracy: 0.359375\n",
            "n_iters: 5794 loss: 1.7150806188583374, accuracy: 0.3984375\n",
            "n_iters: 5804 loss: 1.6538939476013184, accuracy: 0.40625\n",
            "n_iters: 5814 loss: 1.621983289718628, accuracy: 0.3828125\n",
            "n_iters: 5824 loss: 1.6286354064941406, accuracy: 0.3984375\n",
            "n_iters: 5824 test accuracy: 0.382 gradient_norm: 9.595825131743041\n",
            "n_iters: 5834 loss: 1.7381691932678223, accuracy: 0.421875\n",
            "n_iters: 5844 loss: 1.719120979309082, accuracy: 0.390625\n",
            "n_iters: 5854 loss: 1.79165518283844, accuracy: 0.421875\n",
            "n_iters: 5864 loss: 2.059190273284912, accuracy: 0.2875\n",
            "n_iters: 5865 loss: 1.9360162019729614, accuracy: 0.3203125\n",
            "n_iters: 5865 test accuracy: 0.381 gradient_norm: 11.78630656923423\n",
            "n_iters: 5875 loss: 1.6508303880691528, accuracy: 0.484375\n",
            "n_iters: 5885 loss: 1.6460779905319214, accuracy: 0.375\n",
            "n_iters: 5895 loss: 1.7134908437728882, accuracy: 0.40625\n",
            "n_iters: 5905 loss: 1.662413477897644, accuracy: 0.390625\n",
            "n_iters: 5915 loss: 1.6536247730255127, accuracy: 0.4296875\n",
            "n_iters: 5915 test accuracy: 0.367 gradient_norm: 13.321710486767417\n",
            "n_iters: 5925 loss: 1.820571780204773, accuracy: 0.3515625\n",
            "n_iters: 5935 loss: 1.7242721319198608, accuracy: 0.4453125\n",
            "n_iters: 5945 loss: 1.6696462631225586, accuracy: 0.3984375\n",
            "n_iters: 5955 loss: 1.7227599620819092, accuracy: 0.4140625\n",
            "n_iters: 5965 loss: 1.6717157363891602, accuracy: 0.4296875\n",
            "n_iters: 5965 test accuracy: 0.407 gradient_norm: 11.602171101969391\n",
            "n_iters: 5975 loss: 1.7813910245895386, accuracy: 0.4140625\n",
            "n_iters: 5985 loss: 1.7125773429870605, accuracy: 0.3828125\n",
            "n_iters: 5995 loss: 1.6088030338287354, accuracy: 0.453125\n",
            "n_iters: 6005 loss: 1.5350736379623413, accuracy: 0.4453125\n",
            "n_iters: 6015 loss: 1.8434386253356934, accuracy: 0.3984375\n",
            "n_iters: 6015 test accuracy: 0.39 gradient_norm: 10.995858002301256\n",
            "n_iters: 6025 loss: 1.5719906091690063, accuracy: 0.4609375\n",
            "n_iters: 6035 loss: 1.6892945766448975, accuracy: 0.375\n",
            "n_iters: 6045 loss: 1.603725790977478, accuracy: 0.453125\n",
            "n_iters: 6055 loss: 1.6606196165084839, accuracy: 0.390625\n",
            "n_iters: 6065 loss: 1.7710824012756348, accuracy: 0.359375\n",
            "n_iters: 6065 test accuracy: 0.402 gradient_norm: 11.671560322307501\n",
            "n_iters: 6075 loss: 1.6769182682037354, accuracy: 0.3984375\n",
            "n_iters: 6085 loss: 1.9660481214523315, accuracy: 0.359375\n",
            "n_iters: 6095 loss: 1.7997665405273438, accuracy: 0.3359375\n",
            "n_iters: 6105 loss: 1.738196611404419, accuracy: 0.3515625\n",
            "n_iters: 6115 loss: 1.6546286344528198, accuracy: 0.3984375\n",
            "n_iters: 6115 test accuracy: 0.395 gradient_norm: 11.566978506891065\n",
            "n_iters: 6125 loss: 1.7541755437850952, accuracy: 0.3359375\n",
            "n_iters: 6135 loss: 1.6582047939300537, accuracy: 0.421875\n",
            "n_iters: 6145 loss: 1.80917227268219, accuracy: 0.3515625\n",
            "n_iters: 6155 loss: 1.9076229333877563, accuracy: 0.3671875\n",
            "n_iters: 6165 loss: 1.850868821144104, accuracy: 0.3828125\n",
            "n_iters: 6165 test accuracy: 0.396 gradient_norm: 13.058012708157538\n",
            "n_iters: 6175 loss: 1.8533953428268433, accuracy: 0.3671875\n",
            "n_iters: 6185 loss: 1.8448776006698608, accuracy: 0.359375\n",
            "n_iters: 6195 loss: 1.7004520893096924, accuracy: 0.40625\n",
            "n_iters: 6205 loss: 1.6883277893066406, accuracy: 0.4140625\n",
            "n_iters: 6215 loss: 1.7699064016342163, accuracy: 0.4140625\n",
            "n_iters: 6215 test accuracy: 0.401 gradient_norm: 12.568225114339072\n",
            "n_iters: 6225 loss: 1.6865184307098389, accuracy: 0.3671875\n",
            "n_iters: 6235 loss: 1.7425531148910522, accuracy: 0.328125\n",
            "n_iters: 6245 loss: 1.7009761333465576, accuracy: 0.40625\n",
            "n_iters: 6255 loss: 2.0466222763061523, accuracy: 0.3375\n",
            "n_iters: 6256 loss: 1.6997709274291992, accuracy: 0.4140625\n",
            "n_iters: 6256 test accuracy: 0.405 gradient_norm: 11.685824789177467\n",
            "n_iters: 6266 loss: 1.861807107925415, accuracy: 0.3671875\n",
            "n_iters: 6276 loss: 1.614564299583435, accuracy: 0.46875\n",
            "n_iters: 6286 loss: 1.5642982721328735, accuracy: 0.4609375\n",
            "n_iters: 6296 loss: 1.6545854806900024, accuracy: 0.4140625\n",
            "n_iters: 6306 loss: 1.999353289604187, accuracy: 0.3125\n",
            "n_iters: 6306 test accuracy: 0.376 gradient_norm: 15.895935196248471\n",
            "n_iters: 6316 loss: 1.737575650215149, accuracy: 0.3671875\n",
            "n_iters: 6326 loss: 1.5950309038162231, accuracy: 0.4375\n",
            "n_iters: 6336 loss: 1.6575894355773926, accuracy: 0.4453125\n",
            "n_iters: 6346 loss: 1.7672806978225708, accuracy: 0.375\n",
            "n_iters: 6356 loss: 1.8112292289733887, accuracy: 0.4140625\n",
            "n_iters: 6356 test accuracy: 0.384 gradient_norm: 13.718653294408881\n",
            "n_iters: 6366 loss: 1.6307693719863892, accuracy: 0.4609375\n",
            "n_iters: 6376 loss: 1.6480529308319092, accuracy: 0.3828125\n",
            "n_iters: 6386 loss: 1.7646009922027588, accuracy: 0.359375\n",
            "n_iters: 6396 loss: 1.7470781803131104, accuracy: 0.40625\n",
            "n_iters: 6406 loss: 1.6820532083511353, accuracy: 0.3984375\n",
            "n_iters: 6406 test accuracy: 0.386 gradient_norm: 10.851497397290954\n",
            "n_iters: 6416 loss: 1.642642855644226, accuracy: 0.453125\n",
            "n_iters: 6426 loss: 1.6130502223968506, accuracy: 0.421875\n",
            "n_iters: 6436 loss: 1.7146413326263428, accuracy: 0.3671875\n",
            "n_iters: 6446 loss: 1.6228892803192139, accuracy: 0.3671875\n",
            "n_iters: 6456 loss: 1.737723708152771, accuracy: 0.4140625\n",
            "n_iters: 6456 test accuracy: 0.393 gradient_norm: 12.318702596963377\n",
            "n_iters: 6466 loss: 1.8148136138916016, accuracy: 0.4140625\n",
            "n_iters: 6476 loss: 1.7587724924087524, accuracy: 0.3515625\n",
            "n_iters: 6486 loss: 1.6682995557785034, accuracy: 0.390625\n",
            "n_iters: 6496 loss: 1.750591516494751, accuracy: 0.3828125\n",
            "n_iters: 6506 loss: 1.711804986000061, accuracy: 0.4375\n",
            "n_iters: 6506 test accuracy: 0.375 gradient_norm: 12.864937587044052\n",
            "n_iters: 6516 loss: 1.7083840370178223, accuracy: 0.3828125\n",
            "n_iters: 6526 loss: 1.7362703084945679, accuracy: 0.4296875\n",
            "n_iters: 6536 loss: 1.8009315729141235, accuracy: 0.34375\n",
            "n_iters: 6546 loss: 1.8647156953811646, accuracy: 0.34375\n",
            "n_iters: 6556 loss: 1.6453900337219238, accuracy: 0.4375\n",
            "n_iters: 6556 test accuracy: 0.398 gradient_norm: 10.675237813112354\n",
            "n_iters: 6566 loss: 1.7198009490966797, accuracy: 0.375\n",
            "n_iters: 6576 loss: 1.6054068803787231, accuracy: 0.421875\n",
            "n_iters: 6586 loss: 1.7015365362167358, accuracy: 0.375\n",
            "n_iters: 6596 loss: 1.7453432083129883, accuracy: 0.3828125\n",
            "n_iters: 6606 loss: 1.5328459739685059, accuracy: 0.4375\n",
            "n_iters: 6606 test accuracy: 0.396 gradient_norm: 12.47103331799402\n",
            "n_iters: 6616 loss: 1.7035471200942993, accuracy: 0.34375\n",
            "n_iters: 6626 loss: 1.6176358461380005, accuracy: 0.4140625\n",
            "n_iters: 6636 loss: 1.7558380365371704, accuracy: 0.3359375\n",
            "n_iters: 6646 loss: 1.6237550973892212, accuracy: 0.3875\n",
            "n_iters: 6647 loss: 1.7467209100723267, accuracy: 0.3828125\n",
            "n_iters: 6647 test accuracy: 0.401 gradient_norm: 12.356807238503663\n",
            "n_iters: 6657 loss: 1.681821346282959, accuracy: 0.390625\n",
            "n_iters: 6667 loss: 1.6785781383514404, accuracy: 0.4609375\n",
            "n_iters: 6677 loss: 1.8427984714508057, accuracy: 0.3515625\n",
            "n_iters: 6687 loss: 1.7648251056671143, accuracy: 0.3515625\n",
            "n_iters: 6697 loss: 1.509332537651062, accuracy: 0.4921875\n",
            "n_iters: 6697 test accuracy: 0.392 gradient_norm: 10.40080102970827\n",
            "n_iters: 6707 loss: 1.6721513271331787, accuracy: 0.421875\n",
            "n_iters: 6717 loss: 1.7655994892120361, accuracy: 0.375\n",
            "n_iters: 6727 loss: 1.7906794548034668, accuracy: 0.4140625\n",
            "n_iters: 6737 loss: 1.5594936609268188, accuracy: 0.46875\n",
            "n_iters: 6747 loss: 1.8156262636184692, accuracy: 0.328125\n",
            "n_iters: 6747 test accuracy: 0.377 gradient_norm: 15.222093777414281\n",
            "n_iters: 6757 loss: 1.7453116178512573, accuracy: 0.4453125\n",
            "n_iters: 6767 loss: 1.6558948755264282, accuracy: 0.484375\n",
            "n_iters: 6777 loss: 1.8008002042770386, accuracy: 0.3828125\n",
            "n_iters: 6787 loss: 1.8572795391082764, accuracy: 0.40625\n",
            "n_iters: 6797 loss: 1.79071843624115, accuracy: 0.390625\n",
            "n_iters: 6797 test accuracy: 0.403 gradient_norm: 13.239135361506882\n",
            "n_iters: 6807 loss: 1.8039450645446777, accuracy: 0.390625\n",
            "n_iters: 6817 loss: 1.5385396480560303, accuracy: 0.453125\n",
            "n_iters: 6827 loss: 1.6778744459152222, accuracy: 0.4140625\n",
            "n_iters: 6837 loss: 1.8540380001068115, accuracy: 0.3984375\n",
            "n_iters: 6847 loss: 1.619693636894226, accuracy: 0.4375\n",
            "n_iters: 6847 test accuracy: 0.385 gradient_norm: 11.05347690878877\n",
            "n_iters: 6857 loss: 1.7512224912643433, accuracy: 0.3359375\n",
            "n_iters: 6867 loss: 1.526297688484192, accuracy: 0.4609375\n",
            "n_iters: 6877 loss: 1.8350856304168701, accuracy: 0.3671875\n",
            "n_iters: 6887 loss: 1.6113073825836182, accuracy: 0.40625\n",
            "n_iters: 6897 loss: 1.79967200756073, accuracy: 0.3671875\n",
            "n_iters: 6897 test accuracy: 0.385 gradient_norm: 12.33072841865249\n",
            "n_iters: 6907 loss: 1.6257966756820679, accuracy: 0.40625\n",
            "n_iters: 6917 loss: 1.5918594598770142, accuracy: 0.4921875\n",
            "n_iters: 6927 loss: 1.8567370176315308, accuracy: 0.359375\n",
            "n_iters: 6937 loss: 1.7423012256622314, accuracy: 0.3671875\n",
            "n_iters: 6947 loss: 1.75105881690979, accuracy: 0.4140625\n",
            "n_iters: 6947 test accuracy: 0.383 gradient_norm: 11.161930892925504\n",
            "n_iters: 6957 loss: 1.6377248764038086, accuracy: 0.40625\n",
            "n_iters: 6967 loss: 1.6679736375808716, accuracy: 0.4609375\n",
            "n_iters: 6977 loss: 1.71429443359375, accuracy: 0.3671875\n",
            "n_iters: 6987 loss: 1.645439863204956, accuracy: 0.453125\n",
            "n_iters: 6997 loss: 1.7519676685333252, accuracy: 0.40625\n",
            "n_iters: 6997 test accuracy: 0.39 gradient_norm: 12.697594385224171\n",
            "n_iters: 7007 loss: 1.7795783281326294, accuracy: 0.3828125\n",
            "n_iters: 7017 loss: 1.7887787818908691, accuracy: 0.34375\n",
            "n_iters: 7027 loss: 1.8075335025787354, accuracy: 0.421875\n",
            "n_iters: 7037 loss: 1.4569616317749023, accuracy: 0.5\n",
            "n_iters: 7038 loss: 1.717189908027649, accuracy: 0.4296875\n",
            "n_iters: 7038 test accuracy: 0.39 gradient_norm: 13.101580437504957\n",
            "n_iters: 7048 loss: 1.781290888786316, accuracy: 0.375\n",
            "n_iters: 7058 loss: 1.6322450637817383, accuracy: 0.4453125\n",
            "n_iters: 7068 loss: 1.60969078540802, accuracy: 0.421875\n",
            "n_iters: 7078 loss: 1.5723063945770264, accuracy: 0.484375\n",
            "n_iters: 7088 loss: 1.8835227489471436, accuracy: 0.34375\n",
            "n_iters: 7088 test accuracy: 0.374 gradient_norm: 13.276564849648436\n",
            "n_iters: 7098 loss: 1.5394139289855957, accuracy: 0.4453125\n",
            "n_iters: 7108 loss: 1.582939624786377, accuracy: 0.5\n",
            "n_iters: 7118 loss: 1.6346962451934814, accuracy: 0.3828125\n",
            "n_iters: 7128 loss: 1.6448063850402832, accuracy: 0.375\n",
            "n_iters: 7138 loss: 1.701488733291626, accuracy: 0.4296875\n",
            "n_iters: 7138 test accuracy: 0.396 gradient_norm: 11.678060011432258\n",
            "n_iters: 7148 loss: 1.8724250793457031, accuracy: 0.3515625\n",
            "n_iters: 7158 loss: 1.5101099014282227, accuracy: 0.4609375\n",
            "n_iters: 7168 loss: 1.7577283382415771, accuracy: 0.3671875\n",
            "n_iters: 7178 loss: 1.7586268186569214, accuracy: 0.3515625\n",
            "n_iters: 7188 loss: 1.7035396099090576, accuracy: 0.421875\n",
            "n_iters: 7188 test accuracy: 0.396 gradient_norm: 11.762968940396885\n",
            "n_iters: 7198 loss: 1.620457410812378, accuracy: 0.4375\n",
            "n_iters: 7208 loss: 1.6500452756881714, accuracy: 0.4140625\n",
            "n_iters: 7218 loss: 1.6371910572052002, accuracy: 0.4375\n",
            "n_iters: 7228 loss: 1.637215256690979, accuracy: 0.3828125\n",
            "n_iters: 7238 loss: 1.8216750621795654, accuracy: 0.375\n",
            "n_iters: 7238 test accuracy: 0.387 gradient_norm: 13.714815225087984\n",
            "n_iters: 7248 loss: 1.7002962827682495, accuracy: 0.3671875\n",
            "n_iters: 7258 loss: 1.750299334526062, accuracy: 0.4140625\n",
            "n_iters: 7268 loss: 1.5918787717819214, accuracy: 0.4453125\n",
            "n_iters: 7278 loss: 1.641219973564148, accuracy: 0.4375\n",
            "n_iters: 7288 loss: 1.7494972944259644, accuracy: 0.3671875\n",
            "n_iters: 7288 test accuracy: 0.395 gradient_norm: 14.468536330839223\n",
            "n_iters: 7298 loss: 1.6127445697784424, accuracy: 0.4609375\n",
            "n_iters: 7308 loss: 1.6623317003250122, accuracy: 0.40625\n",
            "n_iters: 7318 loss: 1.7984730005264282, accuracy: 0.3515625\n",
            "n_iters: 7328 loss: 1.754408597946167, accuracy: 0.3828125\n",
            "n_iters: 7338 loss: 1.6188499927520752, accuracy: 0.40625\n",
            "n_iters: 7338 test accuracy: 0.391 gradient_norm: 12.098239685551334\n",
            "n_iters: 7348 loss: 1.7252857685089111, accuracy: 0.40625\n",
            "n_iters: 7358 loss: 1.7362693548202515, accuracy: 0.328125\n",
            "n_iters: 7368 loss: 1.7082176208496094, accuracy: 0.4140625\n",
            "n_iters: 7378 loss: 1.8503668308258057, accuracy: 0.3515625\n",
            "n_iters: 7388 loss: 1.7109155654907227, accuracy: 0.359375\n",
            "n_iters: 7388 test accuracy: 0.377 gradient_norm: 12.164576144145276\n",
            "n_iters: 7398 loss: 1.7036762237548828, accuracy: 0.3984375\n",
            "n_iters: 7408 loss: 1.5947420597076416, accuracy: 0.4296875\n",
            "n_iters: 7418 loss: 1.6995283365249634, accuracy: 0.3984375\n",
            "n_iters: 7428 loss: 1.7403829097747803, accuracy: 0.3625\n",
            "n_iters: 7429 loss: 1.5901716947555542, accuracy: 0.40625\n",
            "n_iters: 7429 test accuracy: 0.387 gradient_norm: 9.312840533671833\n",
            "n_iters: 7439 loss: 1.5957348346710205, accuracy: 0.4765625\n",
            "n_iters: 7449 loss: 1.6662970781326294, accuracy: 0.40625\n",
            "n_iters: 7459 loss: 1.640683650970459, accuracy: 0.453125\n",
            "n_iters: 7469 loss: 1.7158507108688354, accuracy: 0.3828125\n",
            "n_iters: 7479 loss: 1.7210952043533325, accuracy: 0.40625\n",
            "n_iters: 7479 test accuracy: 0.409 gradient_norm: 12.07955756549083\n",
            "n_iters: 7489 loss: 1.536272406578064, accuracy: 0.421875\n",
            "n_iters: 7499 loss: 1.6579357385635376, accuracy: 0.4375\n",
            "n_iters: 7509 loss: 1.6215256452560425, accuracy: 0.453125\n",
            "n_iters: 7519 loss: 1.6717225313186646, accuracy: 0.453125\n",
            "n_iters: 7529 loss: 1.6253242492675781, accuracy: 0.453125\n",
            "n_iters: 7529 test accuracy: 0.384 gradient_norm: 11.456996614159271\n",
            "n_iters: 7539 loss: 1.780362606048584, accuracy: 0.421875\n",
            "n_iters: 7549 loss: 1.6198829412460327, accuracy: 0.5\n",
            "n_iters: 7559 loss: 1.6943868398666382, accuracy: 0.4140625\n",
            "n_iters: 7569 loss: 1.5706406831741333, accuracy: 0.390625\n",
            "n_iters: 7579 loss: 1.6504319906234741, accuracy: 0.40625\n",
            "n_iters: 7579 test accuracy: 0.41 gradient_norm: 11.356041788746618\n",
            "n_iters: 7589 loss: 1.4564231634140015, accuracy: 0.484375\n",
            "n_iters: 7599 loss: 1.6277278661727905, accuracy: 0.4453125\n",
            "n_iters: 7609 loss: 1.6359907388687134, accuracy: 0.4375\n",
            "n_iters: 7619 loss: 1.6100926399230957, accuracy: 0.4296875\n",
            "n_iters: 7629 loss: 1.7386798858642578, accuracy: 0.296875\n",
            "n_iters: 7629 test accuracy: 0.41 gradient_norm: 11.753531911889866\n",
            "n_iters: 7639 loss: 1.6889249086380005, accuracy: 0.453125\n",
            "n_iters: 7649 loss: 1.9823685884475708, accuracy: 0.359375\n",
            "n_iters: 7659 loss: 1.6693089008331299, accuracy: 0.3359375\n",
            "n_iters: 7669 loss: 1.5870164632797241, accuracy: 0.46875\n",
            "n_iters: 7679 loss: 1.570341944694519, accuracy: 0.4375\n",
            "n_iters: 7679 test accuracy: 0.404 gradient_norm: 11.109861291200156\n",
            "n_iters: 7689 loss: 1.554175615310669, accuracy: 0.4765625\n",
            "n_iters: 7699 loss: 1.6836190223693848, accuracy: 0.390625\n",
            "n_iters: 7709 loss: 1.9770605564117432, accuracy: 0.3046875\n",
            "n_iters: 7719 loss: 1.7589919567108154, accuracy: 0.375\n",
            "n_iters: 7729 loss: 1.9235434532165527, accuracy: 0.3203125\n",
            "n_iters: 7729 test accuracy: 0.392 gradient_norm: 12.601839390512824\n",
            "n_iters: 7739 loss: 1.7550982236862183, accuracy: 0.359375\n",
            "n_iters: 7749 loss: 1.784454584121704, accuracy: 0.3046875\n",
            "n_iters: 7759 loss: 1.7278112173080444, accuracy: 0.390625\n",
            "n_iters: 7769 loss: 1.6413873434066772, accuracy: 0.4140625\n",
            "n_iters: 7779 loss: 1.6760174036026, accuracy: 0.4296875\n",
            "n_iters: 7779 test accuracy: 0.382 gradient_norm: 12.158707388433498\n",
            "n_iters: 7789 loss: 1.766495943069458, accuracy: 0.4140625\n",
            "n_iters: 7799 loss: 1.730818748474121, accuracy: 0.421875\n",
            "n_iters: 7809 loss: 1.8154250383377075, accuracy: 0.375\n",
            "n_iters: 7819 loss: 1.6747058629989624, accuracy: 0.375\n",
            "n_iters: 7820 loss: 1.465393304824829, accuracy: 0.4453125\n",
            "n_iters: 7820 test accuracy: 0.401 gradient_norm: 11.510648918997438\n",
            "n_iters: 7830 loss: 1.708185076713562, accuracy: 0.3984375\n",
            "n_iters: 7840 loss: 1.766219973564148, accuracy: 0.34375\n",
            "n_iters: 7850 loss: 1.843974232673645, accuracy: 0.3828125\n",
            "n_iters: 7860 loss: 1.6315460205078125, accuracy: 0.4453125\n",
            "n_iters: 7870 loss: 1.6401183605194092, accuracy: 0.390625\n",
            "n_iters: 7870 test accuracy: 0.387 gradient_norm: 11.614668020240286\n",
            "n_iters: 7880 loss: 1.9718952178955078, accuracy: 0.3203125\n",
            "n_iters: 7890 loss: 1.7531379461288452, accuracy: 0.375\n",
            "n_iters: 7900 loss: 1.6759940385818481, accuracy: 0.3984375\n",
            "n_iters: 7910 loss: 1.5697916746139526, accuracy: 0.421875\n",
            "n_iters: 7920 loss: 1.7371643781661987, accuracy: 0.3984375\n",
            "n_iters: 7920 test accuracy: 0.394 gradient_norm: 14.138538171207472\n",
            "n_iters: 7930 loss: 1.6998850107192993, accuracy: 0.328125\n",
            "n_iters: 7940 loss: 1.803017258644104, accuracy: 0.4375\n",
            "n_iters: 7950 loss: 1.5997105836868286, accuracy: 0.421875\n",
            "n_iters: 7960 loss: 1.682902216911316, accuracy: 0.4453125\n",
            "n_iters: 7970 loss: 1.6780612468719482, accuracy: 0.3828125\n",
            "n_iters: 7970 test accuracy: 0.39 gradient_norm: 12.260652733839793\n",
            "n_iters: 7980 loss: 1.6225427389144897, accuracy: 0.4375\n",
            "n_iters: 7990 loss: 1.788061261177063, accuracy: 0.3515625\n",
            "n_iters: 8000 loss: 1.5346009731292725, accuracy: 0.4375\n",
            "n_iters: 8010 loss: 1.4827210903167725, accuracy: 0.4765625\n",
            "n_iters: 8020 loss: 1.5546358823776245, accuracy: 0.4140625\n",
            "n_iters: 8020 test accuracy: 0.372 gradient_norm: 11.677669535725839\n",
            "n_iters: 8030 loss: 1.640851378440857, accuracy: 0.5\n",
            "n_iters: 8040 loss: 1.6121735572814941, accuracy: 0.4453125\n",
            "n_iters: 8050 loss: 1.785885214805603, accuracy: 0.3515625\n",
            "n_iters: 8060 loss: 1.8079113960266113, accuracy: 0.3984375\n",
            "n_iters: 8070 loss: 1.6097056865692139, accuracy: 0.4921875\n",
            "n_iters: 8070 test accuracy: 0.407 gradient_norm: 10.663394400620263\n",
            "n_iters: 8080 loss: 1.8478127717971802, accuracy: 0.3203125\n",
            "n_iters: 8090 loss: 1.6724340915679932, accuracy: 0.4375\n",
            "n_iters: 8100 loss: 1.634495496749878, accuracy: 0.390625\n",
            "n_iters: 8110 loss: 1.6105598211288452, accuracy: 0.40625\n",
            "n_iters: 8120 loss: 1.5640122890472412, accuracy: 0.4609375\n",
            "n_iters: 8120 test accuracy: 0.4 gradient_norm: 11.667929740867944\n",
            "n_iters: 8130 loss: 1.8600897789001465, accuracy: 0.34375\n",
            "n_iters: 8140 loss: 1.674825668334961, accuracy: 0.3984375\n",
            "n_iters: 8150 loss: 1.5113548040390015, accuracy: 0.484375\n",
            "n_iters: 8160 loss: 1.7112433910369873, accuracy: 0.3984375\n",
            "n_iters: 8170 loss: 1.6783640384674072, accuracy: 0.421875\n",
            "n_iters: 8170 test accuracy: 0.418 gradient_norm: 10.500630663030321\n",
            "n_iters: 8180 loss: 1.5971969366073608, accuracy: 0.4140625\n",
            "n_iters: 8190 loss: 1.5753599405288696, accuracy: 0.453125\n",
            "n_iters: 8200 loss: 1.7365477085113525, accuracy: 0.421875\n",
            "n_iters: 8210 loss: 1.7543179988861084, accuracy: 0.4\n",
            "n_iters: 8211 loss: 1.6115119457244873, accuracy: 0.4375\n",
            "n_iters: 8211 test accuracy: 0.41 gradient_norm: 10.27160535568595\n",
            "n_iters: 8221 loss: 1.7375248670578003, accuracy: 0.4375\n",
            "n_iters: 8231 loss: 1.639687418937683, accuracy: 0.4375\n",
            "n_iters: 8241 loss: 1.5650503635406494, accuracy: 0.5234375\n",
            "n_iters: 8251 loss: 1.6495037078857422, accuracy: 0.46875\n",
            "n_iters: 8261 loss: 1.6683450937271118, accuracy: 0.3671875\n",
            "n_iters: 8261 test accuracy: 0.385 gradient_norm: 10.871941519609223\n",
            "n_iters: 8271 loss: 1.6292798519134521, accuracy: 0.46875\n",
            "n_iters: 8281 loss: 1.6721649169921875, accuracy: 0.4375\n",
            "n_iters: 8291 loss: 1.6496334075927734, accuracy: 0.4296875\n",
            "n_iters: 8301 loss: 1.650302767753601, accuracy: 0.4140625\n",
            "n_iters: 8311 loss: 1.7521191835403442, accuracy: 0.4375\n",
            "n_iters: 8311 test accuracy: 0.401 gradient_norm: 11.484511855856043\n",
            "n_iters: 8321 loss: 1.7347173690795898, accuracy: 0.375\n",
            "n_iters: 8331 loss: 1.6772711277008057, accuracy: 0.3828125\n",
            "n_iters: 8341 loss: 1.5835826396942139, accuracy: 0.46875\n",
            "n_iters: 8351 loss: 1.5623043775558472, accuracy: 0.421875\n",
            "n_iters: 8361 loss: 1.7322592735290527, accuracy: 0.421875\n",
            "n_iters: 8361 test accuracy: 0.384 gradient_norm: 10.946376996230635\n",
            "n_iters: 8371 loss: 1.4756510257720947, accuracy: 0.546875\n",
            "n_iters: 8381 loss: 1.5575522184371948, accuracy: 0.46875\n",
            "n_iters: 8391 loss: 1.5777809619903564, accuracy: 0.4609375\n",
            "n_iters: 8401 loss: 1.6847004890441895, accuracy: 0.34375\n",
            "n_iters: 8411 loss: 1.7133772373199463, accuracy: 0.4140625\n",
            "n_iters: 8411 test accuracy: 0.376 gradient_norm: 11.296793248028788\n",
            "n_iters: 8421 loss: 1.7397634983062744, accuracy: 0.4296875\n",
            "n_iters: 8431 loss: 1.888031244277954, accuracy: 0.3046875\n",
            "n_iters: 8441 loss: 1.7485350370407104, accuracy: 0.3828125\n",
            "n_iters: 8451 loss: 1.645999550819397, accuracy: 0.40625\n",
            "n_iters: 8461 loss: 1.5837737321853638, accuracy: 0.4375\n",
            "n_iters: 8461 test accuracy: 0.383 gradient_norm: 11.459919457201888\n",
            "n_iters: 8471 loss: 1.6183768510818481, accuracy: 0.390625\n",
            "n_iters: 8481 loss: 1.558686375617981, accuracy: 0.4609375\n",
            "n_iters: 8491 loss: 1.761569619178772, accuracy: 0.3828125\n",
            "n_iters: 8501 loss: 1.80008065700531, accuracy: 0.421875\n",
            "n_iters: 8511 loss: 1.7073204517364502, accuracy: 0.4296875\n",
            "n_iters: 8511 test accuracy: 0.391 gradient_norm: 11.277763563985701\n",
            "n_iters: 8521 loss: 1.9491571187973022, accuracy: 0.359375\n",
            "n_iters: 8531 loss: 1.6969963312149048, accuracy: 0.4140625\n",
            "n_iters: 8541 loss: 1.6453359127044678, accuracy: 0.4921875\n",
            "n_iters: 8551 loss: 1.7002509832382202, accuracy: 0.375\n",
            "n_iters: 8561 loss: 1.8398979902267456, accuracy: 0.359375\n",
            "n_iters: 8561 test accuracy: 0.398 gradient_norm: 14.369014381884814\n",
            "n_iters: 8571 loss: 1.6873875856399536, accuracy: 0.4140625\n",
            "n_iters: 8581 loss: 1.5882644653320312, accuracy: 0.4453125\n",
            "n_iters: 8591 loss: 1.6303322315216064, accuracy: 0.40625\n",
            "n_iters: 8601 loss: 1.660569190979004, accuracy: 0.4375\n",
            "n_iters: 8602 loss: 1.5825798511505127, accuracy: 0.4609375\n",
            "n_iters: 8602 test accuracy: 0.406 gradient_norm: 12.758542305466364\n",
            "n_iters: 8612 loss: 1.5035948753356934, accuracy: 0.4609375\n",
            "n_iters: 8622 loss: 1.730573058128357, accuracy: 0.4453125\n",
            "n_iters: 8632 loss: 1.6940370798110962, accuracy: 0.359375\n",
            "n_iters: 8642 loss: 1.6054705381393433, accuracy: 0.4140625\n",
            "n_iters: 8652 loss: 1.819532036781311, accuracy: 0.3828125\n",
            "n_iters: 8652 test accuracy: 0.395 gradient_norm: 14.43991980029745\n",
            "n_iters: 8662 loss: 1.5646265745162964, accuracy: 0.4453125\n",
            "n_iters: 8672 loss: 1.7391515970230103, accuracy: 0.3984375\n",
            "n_iters: 8682 loss: 1.6398597955703735, accuracy: 0.4296875\n",
            "n_iters: 8692 loss: 1.57368803024292, accuracy: 0.4375\n",
            "n_iters: 8702 loss: 1.5793269872665405, accuracy: 0.453125\n",
            "n_iters: 8702 test accuracy: 0.405 gradient_norm: 10.998192672845306\n",
            "n_iters: 8712 loss: 1.807884693145752, accuracy: 0.3515625\n",
            "n_iters: 8722 loss: 1.7610784769058228, accuracy: 0.421875\n",
            "n_iters: 8732 loss: 1.845415472984314, accuracy: 0.40625\n",
            "n_iters: 8742 loss: 1.7351386547088623, accuracy: 0.390625\n",
            "n_iters: 8752 loss: 1.8558207750320435, accuracy: 0.359375\n",
            "n_iters: 8752 test accuracy: 0.378 gradient_norm: 11.04060345979946\n",
            "n_iters: 8762 loss: 1.7673496007919312, accuracy: 0.4140625\n",
            "n_iters: 8772 loss: 1.5033214092254639, accuracy: 0.4375\n",
            "n_iters: 8782 loss: 1.7176501750946045, accuracy: 0.3984375\n",
            "n_iters: 8792 loss: 1.5720908641815186, accuracy: 0.40625\n",
            "n_iters: 8802 loss: 1.6301246881484985, accuracy: 0.3828125\n",
            "n_iters: 8802 test accuracy: 0.388 gradient_norm: 10.460836240865559\n",
            "n_iters: 8812 loss: 1.5463955402374268, accuracy: 0.4765625\n",
            "n_iters: 8822 loss: 1.5612173080444336, accuracy: 0.4375\n",
            "n_iters: 8832 loss: 1.5824278593063354, accuracy: 0.4296875\n",
            "n_iters: 8842 loss: 1.6233234405517578, accuracy: 0.40625\n",
            "n_iters: 8852 loss: 1.8408427238464355, accuracy: 0.359375\n",
            "n_iters: 8852 test accuracy: 0.378 gradient_norm: 13.607661766576985\n",
            "n_iters: 8862 loss: 1.7221060991287231, accuracy: 0.421875\n",
            "n_iters: 8872 loss: 1.5783522129058838, accuracy: 0.421875\n",
            "n_iters: 8882 loss: 1.5655845403671265, accuracy: 0.4765625\n",
            "n_iters: 8892 loss: 1.5971087217330933, accuracy: 0.40625\n",
            "n_iters: 8902 loss: 1.8467525243759155, accuracy: 0.3671875\n",
            "n_iters: 8902 test accuracy: 0.389 gradient_norm: 10.82994387934579\n",
            "n_iters: 8912 loss: 1.6847949028015137, accuracy: 0.4140625\n",
            "n_iters: 8922 loss: 1.8184527158737183, accuracy: 0.4140625\n",
            "n_iters: 8932 loss: 1.6324268579483032, accuracy: 0.4296875\n",
            "n_iters: 8942 loss: 1.7029778957366943, accuracy: 0.421875\n",
            "n_iters: 8952 loss: 1.7654054164886475, accuracy: 0.3203125\n",
            "n_iters: 8952 test accuracy: 0.378 gradient_norm: 11.488275937035683\n",
            "n_iters: 8962 loss: 1.8103039264678955, accuracy: 0.328125\n",
            "n_iters: 8972 loss: 1.6556878089904785, accuracy: 0.4296875\n",
            "n_iters: 8982 loss: 1.631912112236023, accuracy: 0.375\n",
            "n_iters: 8992 loss: 1.5169346332550049, accuracy: 0.5125\n",
            "n_iters: 8993 loss: 1.546714186668396, accuracy: 0.453125\n",
            "n_iters: 8993 test accuracy: 0.388 gradient_norm: 12.446161919615802\n",
            "n_iters: 9003 loss: 1.597206950187683, accuracy: 0.4453125\n",
            "n_iters: 9013 loss: 1.6613703966140747, accuracy: 0.40625\n",
            "n_iters: 9023 loss: 1.6922738552093506, accuracy: 0.3359375\n",
            "n_iters: 9033 loss: 1.7798997163772583, accuracy: 0.3203125\n",
            "n_iters: 9043 loss: 1.5472369194030762, accuracy: 0.421875\n",
            "n_iters: 9043 test accuracy: 0.388 gradient_norm: 10.75171359221259\n",
            "n_iters: 9053 loss: 1.5325579643249512, accuracy: 0.46875\n",
            "n_iters: 9063 loss: 1.7034391164779663, accuracy: 0.3828125\n",
            "n_iters: 9073 loss: 1.4701029062271118, accuracy: 0.4765625\n",
            "n_iters: 9083 loss: 1.7731809616088867, accuracy: 0.40625\n",
            "n_iters: 9093 loss: 1.7607005834579468, accuracy: 0.4140625\n",
            "n_iters: 9093 test accuracy: 0.405 gradient_norm: 11.020967810039068\n",
            "n_iters: 9103 loss: 1.6812282800674438, accuracy: 0.390625\n",
            "n_iters: 9113 loss: 1.48060142993927, accuracy: 0.4296875\n",
            "n_iters: 9123 loss: 1.4963316917419434, accuracy: 0.515625\n",
            "n_iters: 9133 loss: 1.5924221277236938, accuracy: 0.421875\n",
            "n_iters: 9143 loss: 1.8809258937835693, accuracy: 0.3125\n",
            "n_iters: 9143 test accuracy: 0.371 gradient_norm: 11.603414888816916\n",
            "n_iters: 9153 loss: 1.7216845750808716, accuracy: 0.4453125\n",
            "n_iters: 9163 loss: 1.5724995136260986, accuracy: 0.4140625\n",
            "n_iters: 9173 loss: 1.7005658149719238, accuracy: 0.390625\n",
            "n_iters: 9183 loss: 1.5415098667144775, accuracy: 0.421875\n",
            "n_iters: 9193 loss: 1.5381062030792236, accuracy: 0.4921875\n",
            "n_iters: 9193 test accuracy: 0.394 gradient_norm: 11.072591760279382\n",
            "n_iters: 9203 loss: 1.6415985822677612, accuracy: 0.4765625\n",
            "n_iters: 9213 loss: 1.5170358419418335, accuracy: 0.4609375\n",
            "n_iters: 9223 loss: 1.7375123500823975, accuracy: 0.4453125\n",
            "n_iters: 9233 loss: 1.652100682258606, accuracy: 0.484375\n",
            "n_iters: 9243 loss: 1.5342057943344116, accuracy: 0.421875\n",
            "n_iters: 9243 test accuracy: 0.412 gradient_norm: 9.962854475033966\n",
            "n_iters: 9253 loss: 1.7530971765518188, accuracy: 0.359375\n",
            "n_iters: 9263 loss: 1.6679139137268066, accuracy: 0.3828125\n",
            "n_iters: 9273 loss: 1.6170419454574585, accuracy: 0.4296875\n",
            "n_iters: 9283 loss: 1.7261765003204346, accuracy: 0.3984375\n",
            "n_iters: 9293 loss: 1.6990940570831299, accuracy: 0.421875\n",
            "n_iters: 9293 test accuracy: 0.393 gradient_norm: 11.763421487335714\n",
            "n_iters: 9303 loss: 1.7354182004928589, accuracy: 0.3671875\n",
            "n_iters: 9313 loss: 1.7141103744506836, accuracy: 0.4140625\n",
            "n_iters: 9323 loss: 1.6634559631347656, accuracy: 0.4140625\n",
            "n_iters: 9333 loss: 1.4586069583892822, accuracy: 0.4765625\n",
            "n_iters: 9343 loss: 1.5740612745285034, accuracy: 0.4765625\n",
            "n_iters: 9343 test accuracy: 0.402 gradient_norm: 13.12136516520501\n",
            "n_iters: 9353 loss: 1.515846610069275, accuracy: 0.421875\n",
            "n_iters: 9363 loss: 1.6588529348373413, accuracy: 0.3828125\n",
            "n_iters: 9373 loss: 1.4724068641662598, accuracy: 0.4921875\n",
            "n_iters: 9383 loss: 1.783429741859436, accuracy: 0.3875\n",
            "n_iters: 9384 loss: 1.500733733177185, accuracy: 0.4453125\n",
            "n_iters: 9384 test accuracy: 0.39 gradient_norm: 11.11992114721897\n",
            "n_iters: 9394 loss: 1.5592974424362183, accuracy: 0.4296875\n",
            "n_iters: 9404 loss: 1.6667094230651855, accuracy: 0.453125\n",
            "n_iters: 9414 loss: 1.809032678604126, accuracy: 0.390625\n",
            "n_iters: 9424 loss: 1.5642298460006714, accuracy: 0.4375\n",
            "n_iters: 9434 loss: 1.5541231632232666, accuracy: 0.4296875\n",
            "n_iters: 9434 test accuracy: 0.387 gradient_norm: 9.230419059497592\n",
            "n_iters: 9444 loss: 1.5630794763565063, accuracy: 0.46875\n",
            "n_iters: 9454 loss: 1.668154239654541, accuracy: 0.3984375\n",
            "n_iters: 9464 loss: 1.7147952318191528, accuracy: 0.3671875\n",
            "n_iters: 9474 loss: 1.6439279317855835, accuracy: 0.40625\n",
            "n_iters: 9484 loss: 1.5764224529266357, accuracy: 0.40625\n",
            "n_iters: 9484 test accuracy: 0.399 gradient_norm: 11.244587689956642\n",
            "n_iters: 9494 loss: 1.588904857635498, accuracy: 0.421875\n",
            "n_iters: 9504 loss: 1.8636236190795898, accuracy: 0.328125\n",
            "n_iters: 9514 loss: 1.6568729877471924, accuracy: 0.390625\n",
            "n_iters: 9524 loss: 1.8826910257339478, accuracy: 0.3359375\n",
            "n_iters: 9534 loss: 1.5955525636672974, accuracy: 0.4296875\n",
            "n_iters: 9534 test accuracy: 0.393 gradient_norm: 12.35586047550829\n",
            "n_iters: 9544 loss: 1.8651643991470337, accuracy: 0.375\n",
            "n_iters: 9554 loss: 1.729228138923645, accuracy: 0.359375\n",
            "n_iters: 9564 loss: 1.690412998199463, accuracy: 0.4453125\n",
            "n_iters: 9574 loss: 1.8515950441360474, accuracy: 0.4296875\n",
            "n_iters: 9584 loss: 1.6794087886810303, accuracy: 0.4375\n",
            "n_iters: 9584 test accuracy: 0.396 gradient_norm: 10.059211346836578\n",
            "n_iters: 9594 loss: 1.6697200536727905, accuracy: 0.421875\n",
            "n_iters: 9604 loss: 1.685742735862732, accuracy: 0.4453125\n",
            "n_iters: 9614 loss: 1.8236675262451172, accuracy: 0.3515625\n",
            "n_iters: 9624 loss: 1.4978585243225098, accuracy: 0.5\n",
            "n_iters: 9634 loss: 1.6817773580551147, accuracy: 0.421875\n",
            "n_iters: 9634 test accuracy: 0.431 gradient_norm: 12.707876303712483\n",
            "n_iters: 9644 loss: 1.616863489151001, accuracy: 0.4453125\n",
            "n_iters: 9654 loss: 1.5640060901641846, accuracy: 0.421875\n",
            "n_iters: 9664 loss: 1.726833462715149, accuracy: 0.390625\n",
            "n_iters: 9674 loss: 1.6374545097351074, accuracy: 0.4140625\n",
            "n_iters: 9684 loss: 1.6867886781692505, accuracy: 0.328125\n",
            "n_iters: 9684 test accuracy: 0.391 gradient_norm: 13.048387177103736\n",
            "n_iters: 9694 loss: 1.6618852615356445, accuracy: 0.4921875\n",
            "n_iters: 9704 loss: 1.6522349119186401, accuracy: 0.4609375\n",
            "n_iters: 9714 loss: 1.624723196029663, accuracy: 0.40625\n",
            "n_iters: 9724 loss: 1.600720763206482, accuracy: 0.4453125\n",
            "n_iters: 9734 loss: 1.7716872692108154, accuracy: 0.4140625\n",
            "n_iters: 9734 test accuracy: 0.375 gradient_norm: 12.243508139683383\n",
            "n_iters: 9744 loss: 1.8166801929473877, accuracy: 0.375\n",
            "n_iters: 9754 loss: 1.6417309045791626, accuracy: 0.3515625\n",
            "n_iters: 9764 loss: 1.6480469703674316, accuracy: 0.453125\n",
            "n_iters: 9774 loss: 1.7302592992782593, accuracy: 0.4125\n",
            "n_iters: 9775 loss: 1.805323839187622, accuracy: 0.390625\n",
            "n_iters: 9775 test accuracy: 0.407 gradient_norm: 11.224398244325052\n",
            "n_iters: 9785 loss: 1.5798271894454956, accuracy: 0.40625\n",
            "n_iters: 9795 loss: 1.5438511371612549, accuracy: 0.484375\n",
            "n_iters: 9805 loss: 1.9002069234848022, accuracy: 0.3671875\n",
            "n_iters: 9815 loss: 1.7887611389160156, accuracy: 0.3359375\n",
            "n_iters: 9825 loss: 1.5941195487976074, accuracy: 0.4453125\n",
            "n_iters: 9825 test accuracy: 0.399 gradient_norm: 9.698739660792729\n",
            "n_iters: 9835 loss: 1.723547101020813, accuracy: 0.34375\n",
            "n_iters: 9845 loss: 1.7759344577789307, accuracy: 0.359375\n",
            "n_iters: 9855 loss: 1.7076433897018433, accuracy: 0.390625\n",
            "n_iters: 9865 loss: 1.5583553314208984, accuracy: 0.4609375\n",
            "n_iters: 9875 loss: 1.5081356763839722, accuracy: 0.4453125\n",
            "n_iters: 9875 test accuracy: 0.402 gradient_norm: 11.1509196559448\n",
            "n_iters: 9885 loss: 1.744200587272644, accuracy: 0.390625\n",
            "n_iters: 9895 loss: 1.8041677474975586, accuracy: 0.4140625\n",
            "n_iters: 9905 loss: 1.818033218383789, accuracy: 0.3515625\n",
            "n_iters: 9915 loss: 1.6505333185195923, accuracy: 0.4140625\n",
            "n_iters: 9925 loss: 1.6653119325637817, accuracy: 0.3984375\n",
            "n_iters: 9925 test accuracy: 0.368 gradient_norm: 10.434325577019413\n",
            "n_iters: 9935 loss: 1.7707992792129517, accuracy: 0.4140625\n",
            "n_iters: 9945 loss: 1.5629429817199707, accuracy: 0.4140625\n",
            "n_iters: 9955 loss: 1.6752351522445679, accuracy: 0.453125\n",
            "n_iters: 9965 loss: 1.7813359498977661, accuracy: 0.375\n",
            "n_iters: 9975 loss: 1.687072992324829, accuracy: 0.3828125\n",
            "n_iters: 9975 test accuracy: 0.391 gradient_norm: 14.977834349752843\n",
            "n_iters: 9985 loss: 1.628484845161438, accuracy: 0.453125\n",
            "n_iters: 9995 loss: 1.6679637432098389, accuracy: 0.4453125\n",
            "decayed learning rate to 1e-05\n",
            "n_iters: 10005 loss: 1.564257264137268, accuracy: 0.4296875\n",
            "n_iters: 10015 loss: 1.5225369930267334, accuracy: 0.453125\n",
            "n_iters: 10025 loss: 1.613511085510254, accuracy: 0.3671875\n",
            "n_iters: 10025 test accuracy: 0.4 gradient_norm: 11.132961388825981\n",
            "n_iters: 10035 loss: 1.5416061878204346, accuracy: 0.5\n",
            "n_iters: 10045 loss: 1.6411138772964478, accuracy: 0.390625\n",
            "n_iters: 10055 loss: 1.462945580482483, accuracy: 0.4921875\n",
            "n_iters: 10065 loss: 1.5745187997817993, accuracy: 0.421875\n",
            "n_iters: 10075 loss: 1.763651728630066, accuracy: 0.375\n",
            "n_iters: 10075 test accuracy: 0.388 gradient_norm: 11.87033114167543\n",
            "n_iters: 10085 loss: 1.5335826873779297, accuracy: 0.4453125\n",
            "n_iters: 10095 loss: 1.6561907529830933, accuracy: 0.4375\n",
            "n_iters: 10105 loss: 1.5088214874267578, accuracy: 0.484375\n",
            "n_iters: 10115 loss: 1.4948673248291016, accuracy: 0.5234375\n",
            "n_iters: 10125 loss: 1.5571449995040894, accuracy: 0.484375\n",
            "n_iters: 10125 test accuracy: 0.406 gradient_norm: 10.675530348173117\n",
            "n_iters: 10135 loss: 1.4713197946548462, accuracy: 0.4921875\n",
            "n_iters: 10145 loss: 1.8118293285369873, accuracy: 0.3359375\n",
            "n_iters: 10155 loss: 1.6129419803619385, accuracy: 0.3515625\n",
            "n_iters: 10165 loss: 1.633988380432129, accuracy: 0.475\n",
            "n_iters: 10166 loss: 1.7233320474624634, accuracy: 0.390625\n",
            "n_iters: 10166 test accuracy: 0.423 gradient_norm: 11.914857048629106\n",
            "n_iters: 10176 loss: 1.6155378818511963, accuracy: 0.453125\n",
            "n_iters: 10186 loss: 1.6140468120574951, accuracy: 0.4609375\n",
            "n_iters: 10196 loss: 1.6753449440002441, accuracy: 0.4375\n",
            "n_iters: 10206 loss: 1.6611089706420898, accuracy: 0.40625\n",
            "n_iters: 10216 loss: 1.6671987771987915, accuracy: 0.4375\n",
            "n_iters: 10216 test accuracy: 0.392 gradient_norm: 11.148073203029217\n",
            "n_iters: 10226 loss: 1.7637783288955688, accuracy: 0.3984375\n",
            "n_iters: 10236 loss: 1.675710916519165, accuracy: 0.4140625\n",
            "n_iters: 10246 loss: 1.6535711288452148, accuracy: 0.3984375\n",
            "n_iters: 10256 loss: 1.7163033485412598, accuracy: 0.390625\n",
            "n_iters: 10266 loss: 1.60794997215271, accuracy: 0.421875\n",
            "n_iters: 10266 test accuracy: 0.416 gradient_norm: 12.242621763544964\n",
            "n_iters: 10276 loss: 1.6458789110183716, accuracy: 0.40625\n",
            "n_iters: 10286 loss: 1.523577332496643, accuracy: 0.4296875\n",
            "n_iters: 10296 loss: 1.6278873682022095, accuracy: 0.4296875\n",
            "n_iters: 10306 loss: 1.6162139177322388, accuracy: 0.5\n",
            "n_iters: 10316 loss: 1.698901891708374, accuracy: 0.3828125\n",
            "n_iters: 10316 test accuracy: 0.411 gradient_norm: 10.597299679054267\n",
            "n_iters: 10326 loss: 1.6244155168533325, accuracy: 0.4453125\n",
            "n_iters: 10336 loss: 1.6041219234466553, accuracy: 0.4765625\n",
            "n_iters: 10346 loss: 1.5027546882629395, accuracy: 0.484375\n",
            "n_iters: 10356 loss: 1.6129454374313354, accuracy: 0.4296875\n",
            "n_iters: 10366 loss: 1.5825754404067993, accuracy: 0.4296875\n",
            "n_iters: 10366 test accuracy: 0.406 gradient_norm: 9.57199411886519\n",
            "n_iters: 10376 loss: 1.559067964553833, accuracy: 0.453125\n",
            "n_iters: 10386 loss: 1.4566723108291626, accuracy: 0.484375\n",
            "n_iters: 10396 loss: 1.7205415964126587, accuracy: 0.359375\n",
            "n_iters: 10406 loss: 1.6355230808258057, accuracy: 0.3671875\n",
            "n_iters: 10416 loss: 1.4429233074188232, accuracy: 0.484375\n",
            "n_iters: 10416 test accuracy: 0.421 gradient_norm: 12.478056170547207\n",
            "n_iters: 10426 loss: 1.6790480613708496, accuracy: 0.3828125\n",
            "n_iters: 10436 loss: 1.6082838773727417, accuracy: 0.390625\n",
            "n_iters: 10446 loss: 1.6095921993255615, accuracy: 0.4140625\n",
            "n_iters: 10456 loss: 1.6655259132385254, accuracy: 0.421875\n",
            "n_iters: 10466 loss: 1.6700860261917114, accuracy: 0.3671875\n",
            "n_iters: 10466 test accuracy: 0.43 gradient_norm: 11.353765263361446\n",
            "n_iters: 10476 loss: 1.7584545612335205, accuracy: 0.4140625\n",
            "n_iters: 10486 loss: 1.6736842393875122, accuracy: 0.4765625\n",
            "n_iters: 10496 loss: 1.5030099153518677, accuracy: 0.453125\n",
            "n_iters: 10506 loss: 1.5607455968856812, accuracy: 0.4296875\n",
            "n_iters: 10516 loss: 1.757073163986206, accuracy: 0.390625\n",
            "n_iters: 10516 test accuracy: 0.41 gradient_norm: 12.172985383285807\n",
            "n_iters: 10526 loss: 1.6121389865875244, accuracy: 0.421875\n",
            "n_iters: 10536 loss: 1.6185355186462402, accuracy: 0.453125\n",
            "n_iters: 10546 loss: 1.5715163946151733, accuracy: 0.4453125\n",
            "n_iters: 10556 loss: 1.647637963294983, accuracy: 0.425\n",
            "n_iters: 10557 loss: 1.7469308376312256, accuracy: 0.4296875\n",
            "n_iters: 10557 test accuracy: 0.422 gradient_norm: 10.258443579428437\n",
            "n_iters: 10567 loss: 1.6038994789123535, accuracy: 0.3671875\n",
            "n_iters: 10577 loss: 1.5084880590438843, accuracy: 0.4765625\n",
            "n_iters: 10587 loss: 1.6439037322998047, accuracy: 0.4140625\n",
            "n_iters: 10597 loss: 1.4772039651870728, accuracy: 0.3984375\n",
            "n_iters: 10607 loss: 1.532132625579834, accuracy: 0.4765625\n",
            "n_iters: 10607 test accuracy: 0.423 gradient_norm: 9.11726713398337\n",
            "n_iters: 10617 loss: 1.6233279705047607, accuracy: 0.4296875\n",
            "n_iters: 10627 loss: 1.6075401306152344, accuracy: 0.453125\n",
            "n_iters: 10637 loss: 1.634340524673462, accuracy: 0.4140625\n",
            "n_iters: 10647 loss: 1.829505443572998, accuracy: 0.453125\n",
            "n_iters: 10657 loss: 1.723315954208374, accuracy: 0.3828125\n",
            "n_iters: 10657 test accuracy: 0.431 gradient_norm: 10.097444337468541\n",
            "n_iters: 10667 loss: 1.63784658908844, accuracy: 0.40625\n",
            "n_iters: 10677 loss: 1.4921239614486694, accuracy: 0.4296875\n",
            "n_iters: 10687 loss: 1.7415074110031128, accuracy: 0.40625\n",
            "n_iters: 10697 loss: 1.5678911209106445, accuracy: 0.4296875\n",
            "n_iters: 10707 loss: 1.6721469163894653, accuracy: 0.40625\n",
            "n_iters: 10707 test accuracy: 0.419 gradient_norm: 12.367423599147994\n",
            "n_iters: 10717 loss: 1.6448533535003662, accuracy: 0.390625\n",
            "n_iters: 10727 loss: 1.6927460432052612, accuracy: 0.3828125\n",
            "n_iters: 10737 loss: 1.8620761632919312, accuracy: 0.3515625\n",
            "n_iters: 10747 loss: 1.6804007291793823, accuracy: 0.3984375\n",
            "n_iters: 10757 loss: 1.580374002456665, accuracy: 0.4296875\n",
            "n_iters: 10757 test accuracy: 0.43 gradient_norm: 11.180590658599892\n",
            "n_iters: 10767 loss: 1.6728545427322388, accuracy: 0.3984375\n",
            "n_iters: 10777 loss: 1.517915964126587, accuracy: 0.46875\n",
            "n_iters: 10787 loss: 1.5182087421417236, accuracy: 0.5\n",
            "n_iters: 10797 loss: 1.5613065958023071, accuracy: 0.484375\n",
            "n_iters: 10807 loss: 1.571989893913269, accuracy: 0.421875\n",
            "n_iters: 10807 test accuracy: 0.391 gradient_norm: 9.112533331041549\n",
            "n_iters: 10817 loss: 1.5646487474441528, accuracy: 0.375\n",
            "n_iters: 10827 loss: 1.587235927581787, accuracy: 0.453125\n",
            "n_iters: 10837 loss: 1.6118158102035522, accuracy: 0.46875\n",
            "n_iters: 10847 loss: 1.6252853870391846, accuracy: 0.4140625\n",
            "n_iters: 10857 loss: 1.5093504190444946, accuracy: 0.46875\n",
            "n_iters: 10857 test accuracy: 0.449 gradient_norm: 8.864033991485327\n",
            "n_iters: 10867 loss: 1.4703279733657837, accuracy: 0.421875\n",
            "n_iters: 10877 loss: 1.5768074989318848, accuracy: 0.4609375\n",
            "n_iters: 10887 loss: 1.558630108833313, accuracy: 0.515625\n",
            "n_iters: 10897 loss: 1.6569957733154297, accuracy: 0.3984375\n",
            "n_iters: 10907 loss: 1.6223429441452026, accuracy: 0.4296875\n",
            "n_iters: 10907 test accuracy: 0.423 gradient_norm: 10.013442252141534\n",
            "n_iters: 10917 loss: 1.5531432628631592, accuracy: 0.4296875\n",
            "n_iters: 10927 loss: 1.7154207229614258, accuracy: 0.359375\n",
            "n_iters: 10937 loss: 1.6678533554077148, accuracy: 0.46875\n",
            "n_iters: 10947 loss: 1.6702938079833984, accuracy: 0.4125\n",
            "n_iters: 10948 loss: 1.6476857662200928, accuracy: 0.40625\n",
            "n_iters: 10948 test accuracy: 0.424 gradient_norm: 10.109725559210123\n",
            "n_iters: 10958 loss: 1.535665512084961, accuracy: 0.453125\n",
            "n_iters: 10968 loss: 1.7955825328826904, accuracy: 0.40625\n",
            "n_iters: 10978 loss: 1.5522701740264893, accuracy: 0.4375\n",
            "n_iters: 10988 loss: 1.5999383926391602, accuracy: 0.390625\n",
            "n_iters: 10998 loss: 1.67451012134552, accuracy: 0.3984375\n",
            "n_iters: 10998 test accuracy: 0.404 gradient_norm: 10.147442890967278\n",
            "n_iters: 11008 loss: 1.6136579513549805, accuracy: 0.4375\n",
            "n_iters: 11018 loss: 1.63588285446167, accuracy: 0.359375\n",
            "n_iters: 11028 loss: 1.5302259922027588, accuracy: 0.4296875\n",
            "n_iters: 11038 loss: 1.5726475715637207, accuracy: 0.4140625\n",
            "n_iters: 11048 loss: 1.6149829626083374, accuracy: 0.421875\n",
            "n_iters: 11048 test accuracy: 0.405 gradient_norm: 10.215969434432822\n",
            "n_iters: 11058 loss: 1.6886255741119385, accuracy: 0.4609375\n",
            "n_iters: 11068 loss: 1.5722754001617432, accuracy: 0.4609375\n",
            "n_iters: 11078 loss: 1.574533462524414, accuracy: 0.359375\n",
            "n_iters: 11088 loss: 1.5605207681655884, accuracy: 0.5078125\n",
            "n_iters: 11098 loss: 1.486469030380249, accuracy: 0.53125\n",
            "n_iters: 11098 test accuracy: 0.408 gradient_norm: 10.15694792706514\n",
            "n_iters: 11108 loss: 1.6772699356079102, accuracy: 0.4296875\n",
            "n_iters: 11118 loss: 1.5640522241592407, accuracy: 0.453125\n",
            "n_iters: 11128 loss: 1.632327914237976, accuracy: 0.4375\n",
            "n_iters: 11138 loss: 1.606373906135559, accuracy: 0.4140625\n",
            "n_iters: 11148 loss: 1.6468292474746704, accuracy: 0.390625\n",
            "n_iters: 11148 test accuracy: 0.424 gradient_norm: 11.923898827227017\n",
            "n_iters: 11158 loss: 1.5559591054916382, accuracy: 0.40625\n",
            "n_iters: 11168 loss: 1.6935622692108154, accuracy: 0.421875\n",
            "n_iters: 11178 loss: 1.5896247625350952, accuracy: 0.46875\n",
            "n_iters: 11188 loss: 1.592621088027954, accuracy: 0.421875\n",
            "n_iters: 11198 loss: 1.4693195819854736, accuracy: 0.46875\n",
            "n_iters: 11198 test accuracy: 0.428 gradient_norm: 12.377410978962708\n",
            "n_iters: 11208 loss: 1.630201816558838, accuracy: 0.453125\n",
            "n_iters: 11218 loss: 1.5552608966827393, accuracy: 0.453125\n",
            "n_iters: 11228 loss: 1.6633144617080688, accuracy: 0.3671875\n",
            "n_iters: 11238 loss: 1.8783668279647827, accuracy: 0.3125\n",
            "n_iters: 11248 loss: 1.554785132408142, accuracy: 0.46875\n",
            "n_iters: 11248 test accuracy: 0.418 gradient_norm: 10.727912091921104\n",
            "n_iters: 11258 loss: 1.5982484817504883, accuracy: 0.46875\n",
            "n_iters: 11268 loss: 1.4946433305740356, accuracy: 0.453125\n",
            "n_iters: 11278 loss: 1.605165719985962, accuracy: 0.4140625\n",
            "n_iters: 11288 loss: 1.657051682472229, accuracy: 0.4140625\n",
            "n_iters: 11298 loss: 1.5277870893478394, accuracy: 0.46875\n",
            "n_iters: 11298 test accuracy: 0.435 gradient_norm: 10.911787181222433\n",
            "n_iters: 11308 loss: 1.63909113407135, accuracy: 0.4140625\n",
            "n_iters: 11318 loss: 1.7344787120819092, accuracy: 0.359375\n",
            "n_iters: 11328 loss: 1.7232745885849, accuracy: 0.3984375\n",
            "n_iters: 11338 loss: 1.532271146774292, accuracy: 0.4375\n",
            "n_iters: 11339 loss: 1.604163646697998, accuracy: 0.4140625\n",
            "n_iters: 11339 test accuracy: 0.399 gradient_norm: 11.033416451611293\n",
            "n_iters: 11349 loss: 1.6520800590515137, accuracy: 0.4140625\n",
            "n_iters: 11359 loss: 1.7035009860992432, accuracy: 0.3671875\n",
            "n_iters: 11369 loss: 1.3854663372039795, accuracy: 0.5078125\n",
            "n_iters: 11379 loss: 1.7164387702941895, accuracy: 0.4140625\n",
            "n_iters: 11389 loss: 1.5887935161590576, accuracy: 0.4453125\n",
            "n_iters: 11389 test accuracy: 0.421 gradient_norm: 9.093018519585588\n",
            "n_iters: 11399 loss: 1.4667701721191406, accuracy: 0.484375\n",
            "n_iters: 11409 loss: 1.5668158531188965, accuracy: 0.484375\n",
            "n_iters: 11419 loss: 1.6261471509933472, accuracy: 0.3671875\n",
            "n_iters: 11429 loss: 1.5892515182495117, accuracy: 0.40625\n",
            "n_iters: 11439 loss: 1.631701111793518, accuracy: 0.4140625\n",
            "n_iters: 11439 test accuracy: 0.397 gradient_norm: 12.815119574913888\n",
            "n_iters: 11449 loss: 1.4544942378997803, accuracy: 0.5078125\n",
            "n_iters: 11459 loss: 1.4205650091171265, accuracy: 0.53125\n",
            "n_iters: 11469 loss: 1.708646297454834, accuracy: 0.390625\n",
            "n_iters: 11479 loss: 1.5249261856079102, accuracy: 0.4296875\n",
            "n_iters: 11489 loss: 1.878598690032959, accuracy: 0.4296875\n",
            "n_iters: 11489 test accuracy: 0.416 gradient_norm: 13.573832111282394\n",
            "n_iters: 11499 loss: 1.642429232597351, accuracy: 0.4140625\n",
            "n_iters: 11509 loss: 1.7265580892562866, accuracy: 0.3515625\n",
            "n_iters: 11519 loss: 1.6127581596374512, accuracy: 0.40625\n",
            "n_iters: 11529 loss: 1.5305745601654053, accuracy: 0.4609375\n",
            "n_iters: 11539 loss: 1.7267963886260986, accuracy: 0.390625\n",
            "n_iters: 11539 test accuracy: 0.415 gradient_norm: 9.736366023066122\n",
            "n_iters: 11549 loss: 1.6588141918182373, accuracy: 0.4296875\n",
            "n_iters: 11559 loss: 1.559726357460022, accuracy: 0.5\n",
            "n_iters: 11569 loss: 1.6322591304779053, accuracy: 0.3515625\n",
            "n_iters: 11579 loss: 1.6663812398910522, accuracy: 0.40625\n",
            "n_iters: 11589 loss: 1.6595244407653809, accuracy: 0.4375\n",
            "n_iters: 11589 test accuracy: 0.416 gradient_norm: 10.663889471828071\n",
            "n_iters: 11599 loss: 1.4997034072875977, accuracy: 0.453125\n",
            "n_iters: 11609 loss: 1.4123165607452393, accuracy: 0.4921875\n",
            "n_iters: 11619 loss: 1.5176271200180054, accuracy: 0.4765625\n",
            "n_iters: 11629 loss: 1.6512891054153442, accuracy: 0.421875\n",
            "n_iters: 11639 loss: 1.6731479167938232, accuracy: 0.421875\n",
            "n_iters: 11639 test accuracy: 0.399 gradient_norm: 10.379305157743193\n",
            "n_iters: 11649 loss: 1.5804945230484009, accuracy: 0.4765625\n",
            "n_iters: 11659 loss: 1.633082628250122, accuracy: 0.3984375\n",
            "n_iters: 11669 loss: 1.6246047019958496, accuracy: 0.46875\n",
            "n_iters: 11679 loss: 1.5109412670135498, accuracy: 0.5078125\n",
            "n_iters: 11689 loss: 1.7544254064559937, accuracy: 0.390625\n",
            "n_iters: 11689 test accuracy: 0.397 gradient_norm: 10.078259911489992\n",
            "n_iters: 11699 loss: 1.6596577167510986, accuracy: 0.3828125\n",
            "n_iters: 11709 loss: 1.6082044839859009, accuracy: 0.4453125\n",
            "n_iters: 11719 loss: 1.6769460439682007, accuracy: 0.40625\n",
            "n_iters: 11729 loss: 1.625257134437561, accuracy: 0.425\n",
            "n_iters: 11730 loss: 1.6928348541259766, accuracy: 0.390625\n",
            "n_iters: 11730 test accuracy: 0.419 gradient_norm: 11.504248796546282\n",
            "n_iters: 11740 loss: 1.787868618965149, accuracy: 0.4140625\n",
            "n_iters: 11750 loss: 1.527590036392212, accuracy: 0.4609375\n",
            "n_iters: 11760 loss: 1.6754461526870728, accuracy: 0.4453125\n",
            "n_iters: 11770 loss: 1.5943182706832886, accuracy: 0.4765625\n",
            "n_iters: 11780 loss: 1.7898997068405151, accuracy: 0.390625\n",
            "n_iters: 11780 test accuracy: 0.393 gradient_norm: 10.571950751132979\n",
            "n_iters: 11790 loss: 1.594894528388977, accuracy: 0.453125\n",
            "n_iters: 11800 loss: 1.4852910041809082, accuracy: 0.5234375\n",
            "n_iters: 11810 loss: 1.4772677421569824, accuracy: 0.4921875\n",
            "n_iters: 11820 loss: 1.631112813949585, accuracy: 0.4375\n",
            "n_iters: 11830 loss: 1.4528048038482666, accuracy: 0.4921875\n",
            "n_iters: 11830 test accuracy: 0.422 gradient_norm: 12.70191557666422\n",
            "n_iters: 11840 loss: 1.649868369102478, accuracy: 0.3359375\n",
            "n_iters: 11850 loss: 1.5309724807739258, accuracy: 0.4375\n",
            "n_iters: 11860 loss: 1.6061298847198486, accuracy: 0.390625\n",
            "n_iters: 11870 loss: 1.6272432804107666, accuracy: 0.4375\n",
            "n_iters: 11880 loss: 1.6276741027832031, accuracy: 0.375\n",
            "n_iters: 11880 test accuracy: 0.412 gradient_norm: 11.923099344219986\n",
            "n_iters: 11890 loss: 1.6460063457489014, accuracy: 0.421875\n",
            "n_iters: 11900 loss: 1.7449030876159668, accuracy: 0.390625\n",
            "n_iters: 11910 loss: 1.4039244651794434, accuracy: 0.5\n",
            "n_iters: 11920 loss: 1.7186442613601685, accuracy: 0.390625\n",
            "n_iters: 11930 loss: 1.6121071577072144, accuracy: 0.3984375\n",
            "n_iters: 11930 test accuracy: 0.395 gradient_norm: 10.779891243482579\n",
            "n_iters: 11940 loss: 1.6730009317398071, accuracy: 0.3828125\n",
            "n_iters: 11950 loss: 1.7055392265319824, accuracy: 0.40625\n",
            "n_iters: 11960 loss: 1.4119726419448853, accuracy: 0.4765625\n",
            "n_iters: 11970 loss: 1.6735544204711914, accuracy: 0.4140625\n",
            "n_iters: 11980 loss: 1.6322003602981567, accuracy: 0.4296875\n",
            "n_iters: 11980 test accuracy: 0.431 gradient_norm: 11.380831626991775\n",
            "n_iters: 11990 loss: 1.5980626344680786, accuracy: 0.4765625\n",
            "n_iters: 12000 loss: 1.5286741256713867, accuracy: 0.4453125\n",
            "n_iters: 12010 loss: 1.6919885873794556, accuracy: 0.390625\n",
            "n_iters: 12020 loss: 1.5725338459014893, accuracy: 0.4453125\n",
            "n_iters: 12030 loss: 1.4704737663269043, accuracy: 0.53125\n",
            "n_iters: 12030 test accuracy: 0.425 gradient_norm: 10.2372374185057\n",
            "n_iters: 12040 loss: 1.5892937183380127, accuracy: 0.46875\n",
            "n_iters: 12050 loss: 1.6549179553985596, accuracy: 0.3984375\n",
            "n_iters: 12060 loss: 1.7422360181808472, accuracy: 0.3203125\n",
            "n_iters: 12070 loss: 1.7334094047546387, accuracy: 0.375\n",
            "n_iters: 12080 loss: 1.6791695356369019, accuracy: 0.390625\n",
            "n_iters: 12080 test accuracy: 0.403 gradient_norm: 12.480789051407026\n",
            "n_iters: 12090 loss: 1.6645114421844482, accuracy: 0.4453125\n",
            "n_iters: 12100 loss: 1.6647136211395264, accuracy: 0.4375\n",
            "n_iters: 12110 loss: 1.7098041772842407, accuracy: 0.4375\n",
            "n_iters: 12120 loss: 1.5442845821380615, accuracy: 0.425\n",
            "n_iters: 12121 loss: 1.6100525856018066, accuracy: 0.4453125\n",
            "n_iters: 12121 test accuracy: 0.424 gradient_norm: 9.954063119291597\n",
            "n_iters: 12131 loss: 1.6062850952148438, accuracy: 0.421875\n",
            "n_iters: 12141 loss: 1.5420376062393188, accuracy: 0.4765625\n",
            "n_iters: 12151 loss: 1.5762858390808105, accuracy: 0.375\n",
            "n_iters: 12161 loss: 1.520319938659668, accuracy: 0.4453125\n",
            "n_iters: 12171 loss: 1.5464890003204346, accuracy: 0.5546875\n",
            "n_iters: 12171 test accuracy: 0.411 gradient_norm: 9.539024014279855\n",
            "n_iters: 12181 loss: 1.52298104763031, accuracy: 0.5078125\n",
            "n_iters: 12191 loss: 1.654415249824524, accuracy: 0.3984375\n",
            "n_iters: 12201 loss: 1.6048734188079834, accuracy: 0.4296875\n",
            "n_iters: 12211 loss: 1.7009329795837402, accuracy: 0.4375\n",
            "n_iters: 12221 loss: 1.466047763824463, accuracy: 0.4921875\n",
            "n_iters: 12221 test accuracy: 0.418 gradient_norm: 10.560241207615269\n",
            "n_iters: 12231 loss: 1.7686659097671509, accuracy: 0.4296875\n",
            "n_iters: 12241 loss: 1.5776495933532715, accuracy: 0.421875\n",
            "n_iters: 12251 loss: 1.6058828830718994, accuracy: 0.3984375\n",
            "n_iters: 12261 loss: 1.5098826885223389, accuracy: 0.484375\n",
            "n_iters: 12271 loss: 1.6106687784194946, accuracy: 0.4765625\n",
            "n_iters: 12271 test accuracy: 0.406 gradient_norm: 9.893997350234349\n",
            "n_iters: 12281 loss: 1.687391996383667, accuracy: 0.4296875\n",
            "n_iters: 12291 loss: 1.6858739852905273, accuracy: 0.3984375\n",
            "n_iters: 12301 loss: 1.5822043418884277, accuracy: 0.4765625\n",
            "n_iters: 12311 loss: 1.6201322078704834, accuracy: 0.4921875\n",
            "n_iters: 12321 loss: 1.5968303680419922, accuracy: 0.4609375\n",
            "n_iters: 12321 test accuracy: 0.406 gradient_norm: 10.680752341793088\n",
            "n_iters: 12331 loss: 1.570841670036316, accuracy: 0.46875\n",
            "n_iters: 12341 loss: 1.5662747621536255, accuracy: 0.4375\n",
            "n_iters: 12351 loss: 1.557978630065918, accuracy: 0.5\n",
            "n_iters: 12361 loss: 1.6577614545822144, accuracy: 0.453125\n",
            "n_iters: 12371 loss: 1.6907309293746948, accuracy: 0.40625\n",
            "n_iters: 12371 test accuracy: 0.412 gradient_norm: 12.417047006399807\n",
            "n_iters: 12381 loss: 1.630490779876709, accuracy: 0.46875\n",
            "n_iters: 12391 loss: 1.4951794147491455, accuracy: 0.484375\n",
            "n_iters: 12401 loss: 1.597468614578247, accuracy: 0.46875\n",
            "n_iters: 12411 loss: 1.4772132635116577, accuracy: 0.4921875\n",
            "n_iters: 12421 loss: 1.5270814895629883, accuracy: 0.375\n",
            "n_iters: 12421 test accuracy: 0.437 gradient_norm: 9.940231686599615\n",
            "n_iters: 12431 loss: 1.7636457681655884, accuracy: 0.328125\n",
            "n_iters: 12441 loss: 1.4868475198745728, accuracy: 0.515625\n",
            "n_iters: 12451 loss: 1.739384651184082, accuracy: 0.375\n",
            "n_iters: 12461 loss: 1.4913381338119507, accuracy: 0.5\n",
            "n_iters: 12471 loss: 1.5120495557785034, accuracy: 0.53125\n",
            "n_iters: 12471 test accuracy: 0.42 gradient_norm: 10.266719290767817\n",
            "n_iters: 12481 loss: 1.5641226768493652, accuracy: 0.40625\n",
            "n_iters: 12491 loss: 1.7317646741867065, accuracy: 0.4375\n",
            "n_iters: 12501 loss: 1.5239375829696655, accuracy: 0.4453125\n",
            "n_iters: 12511 loss: 1.3572906255722046, accuracy: 0.525\n",
            "n_iters: 12512 loss: 1.690422534942627, accuracy: 0.4296875\n",
            "n_iters: 12512 test accuracy: 0.407 gradient_norm: 11.044784340026416\n",
            "n_iters: 12522 loss: 1.653046727180481, accuracy: 0.4453125\n",
            "n_iters: 12532 loss: 1.5385302305221558, accuracy: 0.4140625\n",
            "n_iters: 12542 loss: 1.595713496208191, accuracy: 0.4375\n",
            "n_iters: 12552 loss: 1.615523338317871, accuracy: 0.5078125\n",
            "n_iters: 12562 loss: 1.6773324012756348, accuracy: 0.421875\n",
            "n_iters: 12562 test accuracy: 0.396 gradient_norm: 10.24403321948809\n",
            "n_iters: 12572 loss: 1.4333635568618774, accuracy: 0.5078125\n",
            "n_iters: 12582 loss: 1.6343638896942139, accuracy: 0.5\n",
            "n_iters: 12592 loss: 1.6943587064743042, accuracy: 0.3984375\n",
            "n_iters: 12602 loss: 1.6322660446166992, accuracy: 0.453125\n",
            "n_iters: 12612 loss: 1.520237684249878, accuracy: 0.5234375\n",
            "n_iters: 12612 test accuracy: 0.4 gradient_norm: 11.803214084972753\n",
            "n_iters: 12622 loss: 1.7593200206756592, accuracy: 0.359375\n",
            "n_iters: 12632 loss: 1.5846141576766968, accuracy: 0.453125\n",
            "n_iters: 12642 loss: 1.4984747171401978, accuracy: 0.4453125\n",
            "n_iters: 12652 loss: 1.5744420289993286, accuracy: 0.390625\n",
            "n_iters: 12662 loss: 1.5575149059295654, accuracy: 0.453125\n",
            "n_iters: 12662 test accuracy: 0.42 gradient_norm: 12.143267528663568\n",
            "n_iters: 12672 loss: 1.6362732648849487, accuracy: 0.484375\n",
            "n_iters: 12682 loss: 1.558193564414978, accuracy: 0.4453125\n",
            "n_iters: 12692 loss: 1.7054877281188965, accuracy: 0.40625\n",
            "n_iters: 12702 loss: 1.5843628644943237, accuracy: 0.5\n",
            "n_iters: 12712 loss: 1.5252556800842285, accuracy: 0.4453125\n",
            "n_iters: 12712 test accuracy: 0.414 gradient_norm: 11.798050707334653\n",
            "n_iters: 12722 loss: 1.4670965671539307, accuracy: 0.4921875\n",
            "n_iters: 12732 loss: 1.6406899690628052, accuracy: 0.4140625\n",
            "n_iters: 12742 loss: 1.5234758853912354, accuracy: 0.4921875\n",
            "n_iters: 12752 loss: 1.5274409055709839, accuracy: 0.4765625\n",
            "n_iters: 12762 loss: 1.656431794166565, accuracy: 0.40625\n",
            "n_iters: 12762 test accuracy: 0.391 gradient_norm: 9.418642961749345\n",
            "n_iters: 12772 loss: 1.4580421447753906, accuracy: 0.4921875\n",
            "n_iters: 12782 loss: 1.5876156091690063, accuracy: 0.453125\n",
            "n_iters: 12792 loss: 1.633497953414917, accuracy: 0.4296875\n",
            "n_iters: 12802 loss: 1.6418414115905762, accuracy: 0.453125\n",
            "n_iters: 12812 loss: 1.7722827196121216, accuracy: 0.4375\n",
            "n_iters: 12812 test accuracy: 0.397 gradient_norm: 11.686465712547147\n",
            "n_iters: 12822 loss: 1.4555213451385498, accuracy: 0.5\n",
            "n_iters: 12832 loss: 1.6348353624343872, accuracy: 0.4375\n",
            "n_iters: 12842 loss: 1.635799765586853, accuracy: 0.3671875\n",
            "n_iters: 12852 loss: 1.5681051015853882, accuracy: 0.421875\n",
            "n_iters: 12862 loss: 1.639359474182129, accuracy: 0.4609375\n",
            "n_iters: 12862 test accuracy: 0.422 gradient_norm: 10.318702757218\n",
            "n_iters: 12872 loss: 1.6037328243255615, accuracy: 0.4453125\n",
            "n_iters: 12882 loss: 1.65353524684906, accuracy: 0.3671875\n",
            "n_iters: 12892 loss: 1.8962693214416504, accuracy: 0.359375\n",
            "n_iters: 12902 loss: 1.5636330842971802, accuracy: 0.4\n",
            "n_iters: 12903 loss: 1.5202494859695435, accuracy: 0.484375\n",
            "n_iters: 12903 test accuracy: 0.415 gradient_norm: 10.037957488157195\n",
            "n_iters: 12913 loss: 1.5890934467315674, accuracy: 0.421875\n",
            "n_iters: 12923 loss: 1.6475310325622559, accuracy: 0.4140625\n",
            "n_iters: 12933 loss: 1.5924527645111084, accuracy: 0.46875\n",
            "n_iters: 12943 loss: 1.6169193983078003, accuracy: 0.4296875\n",
            "n_iters: 12953 loss: 1.6669166088104248, accuracy: 0.4453125\n",
            "n_iters: 12953 test accuracy: 0.417 gradient_norm: 12.437399768790854\n",
            "n_iters: 12963 loss: 1.6036441326141357, accuracy: 0.4296875\n",
            "n_iters: 12973 loss: 1.679958701133728, accuracy: 0.3984375\n",
            "n_iters: 12983 loss: 1.5416983366012573, accuracy: 0.390625\n",
            "n_iters: 12993 loss: 1.6369564533233643, accuracy: 0.3828125\n",
            "n_iters: 13003 loss: 1.5333430767059326, accuracy: 0.484375\n",
            "n_iters: 13003 test accuracy: 0.434 gradient_norm: 9.518882408211374\n",
            "n_iters: 13013 loss: 1.6368383169174194, accuracy: 0.4296875\n",
            "n_iters: 13023 loss: 1.520017385482788, accuracy: 0.4609375\n",
            "n_iters: 13033 loss: 1.572765827178955, accuracy: 0.375\n",
            "n_iters: 13043 loss: 1.5829033851623535, accuracy: 0.484375\n",
            "n_iters: 13053 loss: 1.5967170000076294, accuracy: 0.4140625\n",
            "n_iters: 13053 test accuracy: 0.412 gradient_norm: 10.756438770285094\n",
            "n_iters: 13063 loss: 1.622348427772522, accuracy: 0.4609375\n",
            "n_iters: 13073 loss: 1.6371040344238281, accuracy: 0.453125\n",
            "n_iters: 13083 loss: 1.7131038904190063, accuracy: 0.3984375\n",
            "n_iters: 13093 loss: 1.6932796239852905, accuracy: 0.4296875\n",
            "n_iters: 13103 loss: 1.653025507926941, accuracy: 0.421875\n",
            "n_iters: 13103 test accuracy: 0.417 gradient_norm: 10.467782060136564\n",
            "n_iters: 13113 loss: 1.50113046169281, accuracy: 0.5\n",
            "n_iters: 13123 loss: 1.5266025066375732, accuracy: 0.46875\n",
            "n_iters: 13133 loss: 1.5773155689239502, accuracy: 0.453125\n",
            "n_iters: 13143 loss: 1.5936905145645142, accuracy: 0.40625\n",
            "n_iters: 13153 loss: 1.5979175567626953, accuracy: 0.4609375\n",
            "n_iters: 13153 test accuracy: 0.419 gradient_norm: 10.03908389034694\n",
            "n_iters: 13163 loss: 1.6885405778884888, accuracy: 0.4609375\n",
            "n_iters: 13173 loss: 1.6940151453018188, accuracy: 0.3828125\n",
            "n_iters: 13183 loss: 1.470875859260559, accuracy: 0.53125\n",
            "n_iters: 13193 loss: 1.6834572553634644, accuracy: 0.46875\n",
            "n_iters: 13203 loss: 1.515763282775879, accuracy: 0.46875\n",
            "n_iters: 13203 test accuracy: 0.415 gradient_norm: 9.984958595972282\n",
            "n_iters: 13213 loss: 1.6124974489212036, accuracy: 0.375\n",
            "n_iters: 13223 loss: 1.6804769039154053, accuracy: 0.40625\n",
            "n_iters: 13233 loss: 1.4651843309402466, accuracy: 0.4765625\n",
            "n_iters: 13243 loss: 1.7129193544387817, accuracy: 0.4609375\n",
            "n_iters: 13253 loss: 1.5739160776138306, accuracy: 0.421875\n",
            "n_iters: 13253 test accuracy: 0.422 gradient_norm: 10.884452606010885\n",
            "n_iters: 13263 loss: 1.7176471948623657, accuracy: 0.4140625\n",
            "n_iters: 13273 loss: 1.4586694240570068, accuracy: 0.46875\n",
            "n_iters: 13283 loss: 1.5993895530700684, accuracy: 0.40625\n",
            "n_iters: 13293 loss: 1.8238903284072876, accuracy: 0.4625\n",
            "n_iters: 13294 loss: 1.6133599281311035, accuracy: 0.390625\n",
            "n_iters: 13294 test accuracy: 0.409 gradient_norm: 9.887660172263217\n",
            "n_iters: 13304 loss: 1.6381303071975708, accuracy: 0.40625\n",
            "n_iters: 13314 loss: 1.6271507740020752, accuracy: 0.4453125\n",
            "n_iters: 13324 loss: 1.5235366821289062, accuracy: 0.4609375\n",
            "n_iters: 13334 loss: 1.588141918182373, accuracy: 0.4921875\n",
            "n_iters: 13344 loss: 1.4449703693389893, accuracy: 0.4921875\n",
            "n_iters: 13344 test accuracy: 0.399 gradient_norm: 9.675508933312527\n",
            "n_iters: 13354 loss: 1.547711968421936, accuracy: 0.4921875\n",
            "n_iters: 13364 loss: 1.6035630702972412, accuracy: 0.453125\n",
            "n_iters: 13374 loss: 1.6272227764129639, accuracy: 0.4140625\n",
            "n_iters: 13384 loss: 1.4046894311904907, accuracy: 0.4375\n",
            "n_iters: 13394 loss: 1.6243771314620972, accuracy: 0.4296875\n",
            "n_iters: 13394 test accuracy: 0.404 gradient_norm: 11.126181846664437\n",
            "n_iters: 13404 loss: 1.5799700021743774, accuracy: 0.421875\n",
            "n_iters: 13414 loss: 1.7578424215316772, accuracy: 0.375\n",
            "n_iters: 13424 loss: 1.8401421308517456, accuracy: 0.359375\n",
            "n_iters: 13434 loss: 1.6100784540176392, accuracy: 0.40625\n",
            "n_iters: 13444 loss: 1.5741530656814575, accuracy: 0.375\n",
            "n_iters: 13444 test accuracy: 0.396 gradient_norm: 10.591741324515631\n",
            "n_iters: 13454 loss: 1.5321383476257324, accuracy: 0.46875\n",
            "n_iters: 13464 loss: 1.6455695629119873, accuracy: 0.3828125\n",
            "n_iters: 13474 loss: 1.787023901939392, accuracy: 0.3984375\n",
            "n_iters: 13484 loss: 1.6994556188583374, accuracy: 0.390625\n",
            "n_iters: 13494 loss: 1.657393455505371, accuracy: 0.375\n",
            "n_iters: 13494 test accuracy: 0.446 gradient_norm: 11.377697523986765\n",
            "n_iters: 13504 loss: 1.709293007850647, accuracy: 0.4453125\n",
            "n_iters: 13514 loss: 1.826323390007019, accuracy: 0.3046875\n",
            "n_iters: 13524 loss: 1.5826703310012817, accuracy: 0.4453125\n",
            "n_iters: 13534 loss: 1.402233600616455, accuracy: 0.5\n",
            "n_iters: 13544 loss: 1.6127904653549194, accuracy: 0.4453125\n",
            "n_iters: 13544 test accuracy: 0.397 gradient_norm: 10.104052673512825\n",
            "n_iters: 13554 loss: 1.5588570833206177, accuracy: 0.375\n",
            "n_iters: 13564 loss: 1.7136876583099365, accuracy: 0.5078125\n",
            "n_iters: 13574 loss: 1.5887306928634644, accuracy: 0.40625\n",
            "n_iters: 13584 loss: 1.5510761737823486, accuracy: 0.46875\n",
            "n_iters: 13594 loss: 1.4776594638824463, accuracy: 0.5\n",
            "n_iters: 13594 test accuracy: 0.421 gradient_norm: 10.210933225197536\n",
            "n_iters: 13604 loss: 1.4457199573516846, accuracy: 0.5078125\n",
            "n_iters: 13614 loss: 1.6181199550628662, accuracy: 0.4375\n",
            "n_iters: 13624 loss: 1.5862013101577759, accuracy: 0.4140625\n",
            "n_iters: 13634 loss: 1.6397343873977661, accuracy: 0.40625\n",
            "n_iters: 13644 loss: 1.6187843084335327, accuracy: 0.4375\n",
            "n_iters: 13644 test accuracy: 0.407 gradient_norm: 10.46675194888052\n",
            "n_iters: 13654 loss: 1.7423895597457886, accuracy: 0.3984375\n",
            "n_iters: 13664 loss: 1.5285645723342896, accuracy: 0.4609375\n",
            "n_iters: 13674 loss: 1.629615306854248, accuracy: 0.3828125\n",
            "n_iters: 13684 loss: 1.3750131130218506, accuracy: 0.5\n",
            "n_iters: 13685 loss: 1.6282416582107544, accuracy: 0.4140625\n",
            "n_iters: 13685 test accuracy: 0.416 gradient_norm: 9.80509993866137\n",
            "n_iters: 13695 loss: 1.6405668258666992, accuracy: 0.421875\n",
            "n_iters: 13705 loss: 1.7460095882415771, accuracy: 0.3359375\n",
            "n_iters: 13715 loss: 1.5660738945007324, accuracy: 0.484375\n",
            "n_iters: 13725 loss: 1.5564391613006592, accuracy: 0.4609375\n",
            "n_iters: 13735 loss: 1.5608971118927002, accuracy: 0.4296875\n",
            "n_iters: 13735 test accuracy: 0.417 gradient_norm: 12.397007388099466\n",
            "n_iters: 13745 loss: 1.6291074752807617, accuracy: 0.453125\n",
            "n_iters: 13755 loss: 1.5716396570205688, accuracy: 0.4375\n",
            "n_iters: 13765 loss: 1.6510796546936035, accuracy: 0.421875\n",
            "n_iters: 13775 loss: 1.651915431022644, accuracy: 0.453125\n",
            "n_iters: 13785 loss: 1.5762333869934082, accuracy: 0.4765625\n",
            "n_iters: 13785 test accuracy: 0.424 gradient_norm: 10.871042090359023\n",
            "n_iters: 13795 loss: 1.5778924226760864, accuracy: 0.421875\n",
            "n_iters: 13805 loss: 1.6853554248809814, accuracy: 0.4296875\n",
            "n_iters: 13815 loss: 1.878550410270691, accuracy: 0.3671875\n",
            "n_iters: 13825 loss: 1.6469221115112305, accuracy: 0.4296875\n",
            "n_iters: 13835 loss: 1.509658694267273, accuracy: 0.453125\n",
            "n_iters: 13835 test accuracy: 0.408 gradient_norm: 11.095205817983075\n",
            "n_iters: 13845 loss: 1.6095569133758545, accuracy: 0.40625\n",
            "n_iters: 13855 loss: 1.4578254222869873, accuracy: 0.5234375\n",
            "n_iters: 13865 loss: 1.5584994554519653, accuracy: 0.453125\n",
            "n_iters: 13875 loss: 1.7170246839523315, accuracy: 0.4140625\n",
            "n_iters: 13885 loss: 1.635055422782898, accuracy: 0.359375\n",
            "n_iters: 13885 test accuracy: 0.419 gradient_norm: 10.28462995492377\n",
            "n_iters: 13895 loss: 1.5329450368881226, accuracy: 0.46875\n",
            "n_iters: 13905 loss: 1.796120047569275, accuracy: 0.375\n",
            "n_iters: 13915 loss: 1.6000381708145142, accuracy: 0.3828125\n",
            "n_iters: 13925 loss: 1.7328705787658691, accuracy: 0.4296875\n",
            "n_iters: 13935 loss: 1.5593390464782715, accuracy: 0.453125\n",
            "n_iters: 13935 test accuracy: 0.433 gradient_norm: 10.367783511224204\n",
            "n_iters: 13945 loss: 1.7149930000305176, accuracy: 0.4375\n",
            "n_iters: 13955 loss: 1.6911019086837769, accuracy: 0.4765625\n",
            "n_iters: 13965 loss: 1.5642133951187134, accuracy: 0.3828125\n",
            "n_iters: 13975 loss: 1.5796620845794678, accuracy: 0.40625\n",
            "n_iters: 13985 loss: 1.689441204071045, accuracy: 0.421875\n",
            "n_iters: 13985 test accuracy: 0.432 gradient_norm: 9.982330360450186\n",
            "n_iters: 13995 loss: 1.5389200448989868, accuracy: 0.4453125\n",
            "n_iters: 14005 loss: 1.7085546255111694, accuracy: 0.3828125\n",
            "n_iters: 14015 loss: 1.6141278743743896, accuracy: 0.4921875\n",
            "n_iters: 14025 loss: 1.7380143404006958, accuracy: 0.4140625\n",
            "n_iters: 14035 loss: 1.5867199897766113, accuracy: 0.46875\n",
            "n_iters: 14035 test accuracy: 0.403 gradient_norm: 9.9905577822731\n",
            "n_iters: 14045 loss: 1.6282429695129395, accuracy: 0.4609375\n",
            "n_iters: 14055 loss: 1.5696730613708496, accuracy: 0.390625\n",
            "n_iters: 14065 loss: 1.785999059677124, accuracy: 0.390625\n",
            "n_iters: 14075 loss: 1.6608848571777344, accuracy: 0.4875\n",
            "n_iters: 14076 loss: 1.444925308227539, accuracy: 0.4453125\n",
            "n_iters: 14076 test accuracy: 0.421 gradient_norm: 10.371886782159851\n",
            "n_iters: 14086 loss: 1.532790184020996, accuracy: 0.5078125\n",
            "n_iters: 14096 loss: 1.605610728263855, accuracy: 0.375\n",
            "n_iters: 14106 loss: 1.727321743965149, accuracy: 0.390625\n",
            "n_iters: 14116 loss: 1.4921990633010864, accuracy: 0.46875\n",
            "n_iters: 14126 loss: 1.6879031658172607, accuracy: 0.359375\n",
            "n_iters: 14126 test accuracy: 0.42 gradient_norm: 10.20565290902914\n",
            "n_iters: 14136 loss: 1.613548755645752, accuracy: 0.4375\n",
            "n_iters: 14146 loss: 1.6493852138519287, accuracy: 0.46875\n",
            "n_iters: 14156 loss: 1.8081361055374146, accuracy: 0.4375\n",
            "n_iters: 14166 loss: 1.6207506656646729, accuracy: 0.4609375\n",
            "n_iters: 14176 loss: 1.6128374338150024, accuracy: 0.421875\n",
            "n_iters: 14176 test accuracy: 0.416 gradient_norm: 9.224207765575372\n",
            "n_iters: 14186 loss: 1.6556304693222046, accuracy: 0.4609375\n",
            "n_iters: 14196 loss: 1.6468919515609741, accuracy: 0.4296875\n",
            "n_iters: 14206 loss: 1.6659324169158936, accuracy: 0.4296875\n",
            "n_iters: 14216 loss: 1.554440975189209, accuracy: 0.3984375\n",
            "n_iters: 14226 loss: 1.5693610906600952, accuracy: 0.421875\n",
            "n_iters: 14226 test accuracy: 0.42 gradient_norm: 10.127227904697095\n",
            "n_iters: 14236 loss: 1.6742497682571411, accuracy: 0.4609375\n",
            "n_iters: 14246 loss: 1.7250938415527344, accuracy: 0.390625\n",
            "n_iters: 14256 loss: 1.6673225164413452, accuracy: 0.40625\n",
            "n_iters: 14266 loss: 1.6023486852645874, accuracy: 0.4375\n",
            "n_iters: 14276 loss: 1.3649629354476929, accuracy: 0.5078125\n",
            "n_iters: 14276 test accuracy: 0.409 gradient_norm: 9.07394341630793\n",
            "n_iters: 14286 loss: 1.6248443126678467, accuracy: 0.4765625\n",
            "n_iters: 14296 loss: 1.5783227682113647, accuracy: 0.46875\n",
            "n_iters: 14306 loss: 1.632075548171997, accuracy: 0.484375\n",
            "n_iters: 14316 loss: 1.6003475189208984, accuracy: 0.40625\n",
            "n_iters: 14326 loss: 1.5079972743988037, accuracy: 0.4609375\n",
            "n_iters: 14326 test accuracy: 0.436 gradient_norm: 11.17427141514157\n",
            "n_iters: 14336 loss: 1.5578773021697998, accuracy: 0.421875\n",
            "n_iters: 14346 loss: 1.6420493125915527, accuracy: 0.375\n",
            "n_iters: 14356 loss: 1.6047523021697998, accuracy: 0.4375\n",
            "n_iters: 14366 loss: 1.6947624683380127, accuracy: 0.359375\n",
            "n_iters: 14376 loss: 1.4467177391052246, accuracy: 0.5\n",
            "n_iters: 14376 test accuracy: 0.427 gradient_norm: 10.730535391587063\n",
            "n_iters: 14386 loss: 1.6238125562667847, accuracy: 0.4296875\n",
            "n_iters: 14396 loss: 1.5532782077789307, accuracy: 0.4765625\n",
            "n_iters: 14406 loss: 1.6348594427108765, accuracy: 0.4296875\n",
            "n_iters: 14416 loss: 1.469835638999939, accuracy: 0.5\n",
            "n_iters: 14426 loss: 1.6558138132095337, accuracy: 0.46875\n",
            "n_iters: 14426 test accuracy: 0.41 gradient_norm: 11.583710576084352\n",
            "n_iters: 14436 loss: 1.5165481567382812, accuracy: 0.46875\n",
            "n_iters: 14446 loss: 1.5761656761169434, accuracy: 0.5078125\n",
            "n_iters: 14456 loss: 1.5716094970703125, accuracy: 0.46875\n",
            "n_iters: 14466 loss: 1.5346957445144653, accuracy: 0.4875\n",
            "n_iters: 14467 loss: 1.6049290895462036, accuracy: 0.375\n",
            "n_iters: 14467 test accuracy: 0.409 gradient_norm: 9.953700915132224\n",
            "n_iters: 14477 loss: 1.4204237461090088, accuracy: 0.53125\n",
            "n_iters: 14487 loss: 1.6408411264419556, accuracy: 0.453125\n",
            "n_iters: 14497 loss: 1.6993398666381836, accuracy: 0.3984375\n",
            "n_iters: 14507 loss: 1.6807520389556885, accuracy: 0.4140625\n",
            "n_iters: 14517 loss: 1.6174485683441162, accuracy: 0.4140625\n",
            "n_iters: 14517 test accuracy: 0.427 gradient_norm: 9.453140705096084\n",
            "n_iters: 14527 loss: 1.5759460926055908, accuracy: 0.453125\n",
            "n_iters: 14537 loss: 1.505747675895691, accuracy: 0.4765625\n",
            "n_iters: 14547 loss: 1.4636505842208862, accuracy: 0.390625\n",
            "n_iters: 14557 loss: 1.5116522312164307, accuracy: 0.5234375\n",
            "n_iters: 14567 loss: 1.6774028539657593, accuracy: 0.3515625\n",
            "n_iters: 14567 test accuracy: 0.432 gradient_norm: 11.608623454013902\n",
            "n_iters: 14577 loss: 1.6777520179748535, accuracy: 0.4140625\n",
            "n_iters: 14587 loss: 1.6221975088119507, accuracy: 0.3828125\n",
            "n_iters: 14597 loss: 1.4652910232543945, accuracy: 0.484375\n",
            "n_iters: 14607 loss: 1.4272571802139282, accuracy: 0.453125\n",
            "n_iters: 14617 loss: 1.595557689666748, accuracy: 0.4375\n",
            "n_iters: 14617 test accuracy: 0.415 gradient_norm: 11.884576633139982\n",
            "n_iters: 14627 loss: 1.7118364572525024, accuracy: 0.4140625\n",
            "n_iters: 14637 loss: 1.6245583295822144, accuracy: 0.40625\n",
            "n_iters: 14647 loss: 1.5444233417510986, accuracy: 0.4609375\n",
            "n_iters: 14657 loss: 1.7044310569763184, accuracy: 0.3828125\n",
            "n_iters: 14667 loss: 1.4742401838302612, accuracy: 0.515625\n",
            "n_iters: 14667 test accuracy: 0.411 gradient_norm: 9.480664813481889\n",
            "n_iters: 14677 loss: 1.6089483499526978, accuracy: 0.4296875\n",
            "n_iters: 14687 loss: 1.4938592910766602, accuracy: 0.453125\n",
            "n_iters: 14697 loss: 1.507565975189209, accuracy: 0.515625\n",
            "n_iters: 14707 loss: 1.6027337312698364, accuracy: 0.4296875\n",
            "n_iters: 14717 loss: 1.6854000091552734, accuracy: 0.4140625\n",
            "n_iters: 14717 test accuracy: 0.412 gradient_norm: 11.371732903521396\n",
            "n_iters: 14727 loss: 1.7280141115188599, accuracy: 0.3671875\n",
            "n_iters: 14737 loss: 1.5993266105651855, accuracy: 0.4453125\n",
            "n_iters: 14747 loss: 1.497714877128601, accuracy: 0.4375\n",
            "n_iters: 14757 loss: 1.6729942560195923, accuracy: 0.3671875\n",
            "n_iters: 14767 loss: 1.5434809923171997, accuracy: 0.453125\n",
            "n_iters: 14767 test accuracy: 0.433 gradient_norm: 10.955687676982992\n",
            "n_iters: 14777 loss: 1.7512176036834717, accuracy: 0.3984375\n",
            "n_iters: 14787 loss: 1.6346814632415771, accuracy: 0.5\n",
            "n_iters: 14797 loss: 1.7504539489746094, accuracy: 0.359375\n",
            "n_iters: 14807 loss: 1.7972900867462158, accuracy: 0.3203125\n",
            "n_iters: 14817 loss: 1.6827930212020874, accuracy: 0.421875\n",
            "n_iters: 14817 test accuracy: 0.416 gradient_norm: 9.856802292419143\n",
            "n_iters: 14827 loss: 1.7514992952346802, accuracy: 0.40625\n",
            "n_iters: 14837 loss: 1.5308802127838135, accuracy: 0.5078125\n",
            "n_iters: 14847 loss: 1.6473580598831177, accuracy: 0.421875\n",
            "n_iters: 14857 loss: 1.5221748352050781, accuracy: 0.3875\n",
            "n_iters: 14858 loss: 1.7214809656143188, accuracy: 0.3515625\n",
            "n_iters: 14858 test accuracy: 0.413 gradient_norm: 10.734160739136342\n",
            "n_iters: 14868 loss: 1.496221899986267, accuracy: 0.484375\n",
            "n_iters: 14878 loss: 1.6070218086242676, accuracy: 0.421875\n",
            "n_iters: 14888 loss: 1.5189971923828125, accuracy: 0.4296875\n",
            "n_iters: 14898 loss: 1.4690109491348267, accuracy: 0.5390625\n",
            "n_iters: 14908 loss: 1.5780681371688843, accuracy: 0.4375\n",
            "n_iters: 14908 test accuracy: 0.43 gradient_norm: 12.156974928048545\n",
            "n_iters: 14918 loss: 1.5586227178573608, accuracy: 0.4609375\n",
            "n_iters: 14928 loss: 1.6262880563735962, accuracy: 0.4453125\n",
            "n_iters: 14938 loss: 1.5595457553863525, accuracy: 0.4140625\n",
            "n_iters: 14948 loss: 1.4944469928741455, accuracy: 0.5234375\n",
            "n_iters: 14958 loss: 1.4054464101791382, accuracy: 0.5234375\n",
            "n_iters: 14958 test accuracy: 0.412 gradient_norm: 9.697428022208681\n",
            "n_iters: 14968 loss: 1.6151418685913086, accuracy: 0.421875\n",
            "n_iters: 14978 loss: 1.6847503185272217, accuracy: 0.453125\n",
            "n_iters: 14988 loss: 1.6254918575286865, accuracy: 0.4375\n",
            "n_iters: 14998 loss: 1.5353004932403564, accuracy: 0.3671875\n",
            "n_iters: 15008 loss: 1.653049111366272, accuracy: 0.390625\n",
            "n_iters: 15008 test accuracy: 0.412 gradient_norm: 9.424516883389034\n",
            "n_iters: 15018 loss: 1.5022456645965576, accuracy: 0.484375\n",
            "n_iters: 15028 loss: 1.5175704956054688, accuracy: 0.4296875\n",
            "n_iters: 15038 loss: 1.7223966121673584, accuracy: 0.4140625\n",
            "n_iters: 15048 loss: 1.5175824165344238, accuracy: 0.4375\n",
            "n_iters: 15058 loss: 1.5508263111114502, accuracy: 0.4140625\n",
            "n_iters: 15058 test accuracy: 0.415 gradient_norm: 10.73746640672474\n",
            "n_iters: 15068 loss: 1.7220451831817627, accuracy: 0.390625\n",
            "n_iters: 15078 loss: 1.5914727449417114, accuracy: 0.4375\n",
            "n_iters: 15088 loss: 1.5210047960281372, accuracy: 0.46875\n",
            "n_iters: 15098 loss: 1.5260998010635376, accuracy: 0.4765625\n",
            "n_iters: 15108 loss: 1.5622711181640625, accuracy: 0.390625\n",
            "n_iters: 15108 test accuracy: 0.414 gradient_norm: 10.154459653575667\n",
            "n_iters: 15118 loss: 1.5709214210510254, accuracy: 0.4921875\n",
            "n_iters: 15128 loss: 1.6768313646316528, accuracy: 0.4296875\n",
            "n_iters: 15138 loss: 1.8366904258728027, accuracy: 0.359375\n",
            "n_iters: 15148 loss: 1.5154798030853271, accuracy: 0.4453125\n",
            "n_iters: 15158 loss: 1.6712329387664795, accuracy: 0.453125\n",
            "n_iters: 15158 test accuracy: 0.405 gradient_norm: 11.675516188130793\n",
            "n_iters: 15168 loss: 1.4339951276779175, accuracy: 0.484375\n",
            "n_iters: 15178 loss: 1.7288423776626587, accuracy: 0.3671875\n",
            "n_iters: 15188 loss: 1.4563602209091187, accuracy: 0.5546875\n",
            "n_iters: 15198 loss: 1.4711180925369263, accuracy: 0.484375\n",
            "n_iters: 15208 loss: 1.5626771450042725, accuracy: 0.3984375\n",
            "n_iters: 15208 test accuracy: 0.441 gradient_norm: 10.350589883463279\n",
            "n_iters: 15218 loss: 1.5852676630020142, accuracy: 0.3984375\n",
            "n_iters: 15228 loss: 1.5156831741333008, accuracy: 0.453125\n",
            "n_iters: 15238 loss: 1.595657467842102, accuracy: 0.4453125\n",
            "n_iters: 15248 loss: 1.5469186305999756, accuracy: 0.4625\n",
            "n_iters: 15249 loss: 1.6002780199050903, accuracy: 0.46875\n",
            "n_iters: 15249 test accuracy: 0.413 gradient_norm: 10.765495138229896\n",
            "n_iters: 15259 loss: 1.4410045146942139, accuracy: 0.4921875\n",
            "n_iters: 15269 loss: 1.7638977766036987, accuracy: 0.390625\n",
            "n_iters: 15279 loss: 1.5581319332122803, accuracy: 0.46875\n",
            "n_iters: 15289 loss: 1.6401252746582031, accuracy: 0.421875\n",
            "n_iters: 15299 loss: 1.5709680318832397, accuracy: 0.5\n",
            "n_iters: 15299 test accuracy: 0.414 gradient_norm: 10.165433925711685\n",
            "n_iters: 15309 loss: 1.6079015731811523, accuracy: 0.4453125\n",
            "n_iters: 15319 loss: 1.5839266777038574, accuracy: 0.4375\n",
            "n_iters: 15329 loss: 1.5705204010009766, accuracy: 0.484375\n",
            "n_iters: 15339 loss: 1.6135244369506836, accuracy: 0.421875\n",
            "n_iters: 15349 loss: 1.5685644149780273, accuracy: 0.46875\n",
            "n_iters: 15349 test accuracy: 0.419 gradient_norm: 11.247774089108484\n",
            "n_iters: 15359 loss: 1.602795124053955, accuracy: 0.421875\n",
            "n_iters: 15369 loss: 1.6018166542053223, accuracy: 0.4453125\n",
            "n_iters: 15379 loss: 1.59956693649292, accuracy: 0.4296875\n",
            "n_iters: 15389 loss: 1.5923629999160767, accuracy: 0.453125\n",
            "n_iters: 15399 loss: 1.5788171291351318, accuracy: 0.4296875\n",
            "n_iters: 15399 test accuracy: 0.408 gradient_norm: 13.22987255403915\n",
            "n_iters: 15409 loss: 1.5585358142852783, accuracy: 0.4921875\n",
            "n_iters: 15419 loss: 1.6287912130355835, accuracy: 0.421875\n",
            "n_iters: 15429 loss: 1.6081463098526, accuracy: 0.5\n",
            "n_iters: 15439 loss: 1.6382288932800293, accuracy: 0.3828125\n",
            "n_iters: 15449 loss: 1.5404598712921143, accuracy: 0.4375\n",
            "n_iters: 15449 test accuracy: 0.427 gradient_norm: 9.620341924442227\n",
            "n_iters: 15459 loss: 1.633455753326416, accuracy: 0.421875\n",
            "n_iters: 15469 loss: 1.5492254495620728, accuracy: 0.40625\n",
            "n_iters: 15479 loss: 1.5669325590133667, accuracy: 0.453125\n",
            "n_iters: 15489 loss: 1.6064754724502563, accuracy: 0.4453125\n",
            "n_iters: 15499 loss: 1.472362756729126, accuracy: 0.515625\n",
            "n_iters: 15499 test accuracy: 0.406 gradient_norm: 11.2420841035073\n",
            "n_iters: 15509 loss: 1.5638700723648071, accuracy: 0.484375\n",
            "n_iters: 15519 loss: 1.661784052848816, accuracy: 0.4296875\n",
            "n_iters: 15529 loss: 1.5988218784332275, accuracy: 0.4140625\n",
            "n_iters: 15539 loss: 1.6979413032531738, accuracy: 0.4140625\n",
            "n_iters: 15549 loss: 1.5907906293869019, accuracy: 0.4609375\n",
            "n_iters: 15549 test accuracy: 0.424 gradient_norm: 9.597321390788077\n",
            "n_iters: 15559 loss: 1.4793367385864258, accuracy: 0.46875\n",
            "n_iters: 15569 loss: 1.5147979259490967, accuracy: 0.46875\n",
            "n_iters: 15579 loss: 1.3934227228164673, accuracy: 0.53125\n",
            "n_iters: 15589 loss: 1.6714720726013184, accuracy: 0.4140625\n",
            "n_iters: 15599 loss: 1.5409369468688965, accuracy: 0.5\n",
            "n_iters: 15599 test accuracy: 0.431 gradient_norm: 10.326193652904246\n",
            "n_iters: 15609 loss: 1.6796092987060547, accuracy: 0.3671875\n",
            "n_iters: 15619 loss: 1.5693894624710083, accuracy: 0.46875\n",
            "n_iters: 15629 loss: 1.712367296218872, accuracy: 0.4140625\n",
            "n_iters: 15639 loss: 1.6431796550750732, accuracy: 0.5125\n",
            "n_iters: 15640 loss: 1.4891036748886108, accuracy: 0.5234375\n",
            "n_iters: 15640 test accuracy: 0.411 gradient_norm: 10.514368027068972\n",
            "n_iters: 15650 loss: 1.452637791633606, accuracy: 0.4765625\n",
            "n_iters: 15660 loss: 1.567561149597168, accuracy: 0.4609375\n",
            "n_iters: 15670 loss: 1.5758641958236694, accuracy: 0.3828125\n",
            "n_iters: 15680 loss: 1.4337069988250732, accuracy: 0.4609375\n",
            "n_iters: 15690 loss: 1.598297357559204, accuracy: 0.40625\n",
            "n_iters: 15690 test accuracy: 0.426 gradient_norm: 11.116667845578316\n",
            "n_iters: 15700 loss: 1.7896121740341187, accuracy: 0.3671875\n",
            "n_iters: 15710 loss: 1.5568801164627075, accuracy: 0.4296875\n",
            "n_iters: 15720 loss: 1.7736241817474365, accuracy: 0.34375\n",
            "n_iters: 15730 loss: 1.5473185777664185, accuracy: 0.4921875\n",
            "n_iters: 15740 loss: 1.7120695114135742, accuracy: 0.4296875\n",
            "n_iters: 15740 test accuracy: 0.417 gradient_norm: 12.396506238385966\n",
            "n_iters: 15750 loss: 1.600061058998108, accuracy: 0.4375\n",
            "n_iters: 15760 loss: 1.6132041215896606, accuracy: 0.4609375\n",
            "n_iters: 15770 loss: 1.5525805950164795, accuracy: 0.421875\n",
            "n_iters: 15780 loss: 1.616039514541626, accuracy: 0.4453125\n",
            "n_iters: 15790 loss: 1.5761438608169556, accuracy: 0.4453125\n",
            "n_iters: 15790 test accuracy: 0.388 gradient_norm: 10.03801383781344\n",
            "n_iters: 15800 loss: 1.5228157043457031, accuracy: 0.4453125\n",
            "n_iters: 15810 loss: 1.5922324657440186, accuracy: 0.4609375\n",
            "n_iters: 15820 loss: 1.371994972229004, accuracy: 0.515625\n",
            "n_iters: 15830 loss: 1.5681055784225464, accuracy: 0.4375\n",
            "n_iters: 15840 loss: 1.6704648733139038, accuracy: 0.390625\n",
            "n_iters: 15840 test accuracy: 0.406 gradient_norm: 11.119671362601041\n",
            "n_iters: 15850 loss: 1.5732741355895996, accuracy: 0.4375\n",
            "n_iters: 15860 loss: 1.644871711730957, accuracy: 0.4765625\n",
            "n_iters: 15870 loss: 1.699738621711731, accuracy: 0.3515625\n",
            "n_iters: 15880 loss: 1.6572362184524536, accuracy: 0.3984375\n",
            "n_iters: 15890 loss: 1.6020300388336182, accuracy: 0.3828125\n",
            "n_iters: 15890 test accuracy: 0.428 gradient_norm: 10.845292783022504\n",
            "n_iters: 15900 loss: 1.500080943107605, accuracy: 0.4921875\n",
            "n_iters: 15910 loss: 1.48953115940094, accuracy: 0.515625\n",
            "n_iters: 15920 loss: 1.544419527053833, accuracy: 0.4765625\n",
            "n_iters: 15930 loss: 1.6043394804000854, accuracy: 0.390625\n",
            "n_iters: 15940 loss: 1.6654990911483765, accuracy: 0.421875\n",
            "n_iters: 15940 test accuracy: 0.412 gradient_norm: 11.332490895753915\n",
            "n_iters: 15950 loss: 1.7050012350082397, accuracy: 0.4140625\n",
            "n_iters: 15960 loss: 1.713109016418457, accuracy: 0.3984375\n",
            "n_iters: 15970 loss: 1.505466341972351, accuracy: 0.4609375\n",
            "n_iters: 15980 loss: 1.5438127517700195, accuracy: 0.5\n",
            "n_iters: 15990 loss: 1.5554999113082886, accuracy: 0.453125\n",
            "n_iters: 15990 test accuracy: 0.432 gradient_norm: 10.84845955218393\n",
            "decayed learning rate to 1.0000000000000002e-06\n",
            "n_iters: 16000 loss: 1.5743508338928223, accuracy: 0.4453125\n",
            "n_iters: 16010 loss: 1.6939681768417358, accuracy: 0.40625\n",
            "n_iters: 16020 loss: 1.6557538509368896, accuracy: 0.4140625\n",
            "n_iters: 16030 loss: 1.8014276027679443, accuracy: 0.3875\n",
            "n_iters: 16031 loss: 1.5429823398590088, accuracy: 0.4453125\n",
            "n_iters: 16031 test accuracy: 0.403 gradient_norm: 9.913982753065776\n",
            "n_iters: 16041 loss: 1.6490172147750854, accuracy: 0.421875\n",
            "n_iters: 16051 loss: 1.5453861951828003, accuracy: 0.4921875\n",
            "n_iters: 16061 loss: 1.618752121925354, accuracy: 0.40625\n",
            "n_iters: 16071 loss: 1.543721318244934, accuracy: 0.453125\n",
            "n_iters: 16081 loss: 1.552053451538086, accuracy: 0.4921875\n",
            "n_iters: 16081 test accuracy: 0.427 gradient_norm: 10.82887138698232\n",
            "n_iters: 16091 loss: 1.6496448516845703, accuracy: 0.4453125\n",
            "n_iters: 16101 loss: 1.553126573562622, accuracy: 0.484375\n",
            "n_iters: 16111 loss: 1.5695459842681885, accuracy: 0.4609375\n",
            "n_iters: 16121 loss: 1.5566041469573975, accuracy: 0.4296875\n",
            "n_iters: 16131 loss: 1.4979099035263062, accuracy: 0.4609375\n",
            "n_iters: 16131 test accuracy: 0.433 gradient_norm: 11.888200279886034\n",
            "n_iters: 16141 loss: 1.570215106010437, accuracy: 0.4609375\n",
            "n_iters: 16151 loss: 1.6582061052322388, accuracy: 0.4140625\n",
            "n_iters: 16161 loss: 1.7421187162399292, accuracy: 0.375\n",
            "n_iters: 16171 loss: 1.6772488355636597, accuracy: 0.4375\n",
            "n_iters: 16181 loss: 1.6721925735473633, accuracy: 0.421875\n",
            "n_iters: 16181 test accuracy: 0.429 gradient_norm: 9.467868236910082\n",
            "n_iters: 16191 loss: 1.625909686088562, accuracy: 0.40625\n",
            "n_iters: 16201 loss: 1.652510166168213, accuracy: 0.4453125\n",
            "n_iters: 16211 loss: 1.600340485572815, accuracy: 0.453125\n",
            "n_iters: 16221 loss: 1.4388493299484253, accuracy: 0.5078125\n",
            "n_iters: 16231 loss: 1.600282907485962, accuracy: 0.4765625\n",
            "n_iters: 16231 test accuracy: 0.397 gradient_norm: 9.568007452453505\n",
            "n_iters: 16241 loss: 1.385778784751892, accuracy: 0.4921875\n",
            "n_iters: 16251 loss: 1.506422758102417, accuracy: 0.4453125\n",
            "n_iters: 16261 loss: 1.4696892499923706, accuracy: 0.4375\n",
            "n_iters: 16271 loss: 1.7176389694213867, accuracy: 0.5078125\n",
            "n_iters: 16281 loss: 1.6926697492599487, accuracy: 0.3671875\n",
            "n_iters: 16281 test accuracy: 0.43 gradient_norm: 11.65729846640969\n",
            "n_iters: 16291 loss: 1.4412614107131958, accuracy: 0.46875\n",
            "n_iters: 16301 loss: 1.6649365425109863, accuracy: 0.359375\n",
            "n_iters: 16311 loss: 1.5942633152008057, accuracy: 0.421875\n",
            "n_iters: 16321 loss: 1.475573182106018, accuracy: 0.453125\n",
            "n_iters: 16331 loss: 1.6391994953155518, accuracy: 0.4140625\n",
            "n_iters: 16331 test accuracy: 0.418 gradient_norm: 11.47210772203872\n",
            "n_iters: 16341 loss: 1.5982158184051514, accuracy: 0.46875\n",
            "n_iters: 16351 loss: 1.59256911277771, accuracy: 0.421875\n",
            "n_iters: 16361 loss: 1.5838638544082642, accuracy: 0.4296875\n",
            "n_iters: 16371 loss: 1.5852086544036865, accuracy: 0.5078125\n",
            "n_iters: 16381 loss: 1.5686159133911133, accuracy: 0.40625\n",
            "n_iters: 16381 test accuracy: 0.442 gradient_norm: 11.063650044479479\n",
            "n_iters: 16391 loss: 1.6195214986801147, accuracy: 0.453125\n",
            "n_iters: 16401 loss: 1.5966548919677734, accuracy: 0.4375\n",
            "n_iters: 16411 loss: 1.4907770156860352, accuracy: 0.515625\n",
            "n_iters: 16421 loss: 1.5457079410552979, accuracy: 0.5125\n",
            "n_iters: 16422 loss: 1.5066910982131958, accuracy: 0.453125\n",
            "n_iters: 16422 test accuracy: 0.423 gradient_norm: 11.16192084700158\n",
            "n_iters: 16432 loss: 1.6667522192001343, accuracy: 0.4296875\n",
            "n_iters: 16442 loss: 1.5157101154327393, accuracy: 0.46875\n",
            "n_iters: 16452 loss: 1.605436086654663, accuracy: 0.4296875\n",
            "n_iters: 16462 loss: 1.6013189554214478, accuracy: 0.4375\n",
            "n_iters: 16472 loss: 1.5251035690307617, accuracy: 0.4609375\n",
            "n_iters: 16472 test accuracy: 0.384 gradient_norm: 9.51847315408531\n",
            "n_iters: 16482 loss: 1.6296700239181519, accuracy: 0.4296875\n",
            "n_iters: 16492 loss: 1.5527440309524536, accuracy: 0.4375\n",
            "n_iters: 16502 loss: 1.5684869289398193, accuracy: 0.4609375\n",
            "n_iters: 16512 loss: 1.6812046766281128, accuracy: 0.4609375\n",
            "n_iters: 16522 loss: 1.5706839561462402, accuracy: 0.4296875\n",
            "n_iters: 16522 test accuracy: 0.407 gradient_norm: 10.825090277258367\n",
            "n_iters: 16532 loss: 1.5565677881240845, accuracy: 0.4765625\n",
            "n_iters: 16542 loss: 1.5750327110290527, accuracy: 0.5\n",
            "n_iters: 16552 loss: 1.4948999881744385, accuracy: 0.453125\n",
            "n_iters: 16562 loss: 1.5740206241607666, accuracy: 0.46875\n",
            "n_iters: 16572 loss: 1.3984748125076294, accuracy: 0.5625\n",
            "n_iters: 16572 test accuracy: 0.438 gradient_norm: 7.951398145578038\n",
            "n_iters: 16582 loss: 1.7484550476074219, accuracy: 0.4140625\n",
            "n_iters: 16592 loss: 1.6326971054077148, accuracy: 0.453125\n",
            "n_iters: 16602 loss: 1.4859503507614136, accuracy: 0.484375\n",
            "n_iters: 16612 loss: 1.6275806427001953, accuracy: 0.3828125\n",
            "n_iters: 16622 loss: 1.6980493068695068, accuracy: 0.4375\n",
            "n_iters: 16622 test accuracy: 0.428 gradient_norm: 8.828393297624926\n",
            "n_iters: 16632 loss: 1.58782160282135, accuracy: 0.4375\n",
            "n_iters: 16642 loss: 1.5334101915359497, accuracy: 0.4609375\n",
            "n_iters: 16652 loss: 1.5598582029342651, accuracy: 0.46875\n",
            "n_iters: 16662 loss: 1.6679301261901855, accuracy: 0.3828125\n",
            "n_iters: 16672 loss: 1.4147803783416748, accuracy: 0.515625\n",
            "n_iters: 16672 test accuracy: 0.408 gradient_norm: 10.632592681923057\n",
            "n_iters: 16682 loss: 1.7764194011688232, accuracy: 0.3828125\n",
            "n_iters: 16692 loss: 1.5141196250915527, accuracy: 0.4375\n",
            "n_iters: 16702 loss: 1.6382668018341064, accuracy: 0.4296875\n",
            "n_iters: 16712 loss: 1.5084713697433472, accuracy: 0.4765625\n",
            "n_iters: 16722 loss: 1.6043821573257446, accuracy: 0.375\n",
            "n_iters: 16722 test accuracy: 0.405 gradient_norm: 10.980345881924\n",
            "n_iters: 16732 loss: 1.4840772151947021, accuracy: 0.4921875\n",
            "n_iters: 16742 loss: 1.6983914375305176, accuracy: 0.4296875\n",
            "n_iters: 16752 loss: 1.6419591903686523, accuracy: 0.4140625\n",
            "n_iters: 16762 loss: 1.6594067811965942, accuracy: 0.4921875\n",
            "n_iters: 16772 loss: 1.6024134159088135, accuracy: 0.4140625\n",
            "n_iters: 16772 test accuracy: 0.408 gradient_norm: 9.829542540045187\n",
            "n_iters: 16782 loss: 1.5875815153121948, accuracy: 0.40625\n",
            "n_iters: 16792 loss: 1.6299761533737183, accuracy: 0.40625\n",
            "n_iters: 16802 loss: 1.4464147090911865, accuracy: 0.515625\n",
            "n_iters: 16812 loss: 1.5450260639190674, accuracy: 0.4375\n",
            "n_iters: 16813 loss: 1.608805537223816, accuracy: 0.4140625\n",
            "n_iters: 16813 test accuracy: 0.393 gradient_norm: 9.951963486234208\n",
            "n_iters: 16823 loss: 1.63217031955719, accuracy: 0.4375\n",
            "n_iters: 16833 loss: 1.6457074880599976, accuracy: 0.4453125\n",
            "n_iters: 16843 loss: 1.6278034448623657, accuracy: 0.453125\n",
            "n_iters: 16853 loss: 1.6254870891571045, accuracy: 0.40625\n",
            "n_iters: 16863 loss: 1.5836670398712158, accuracy: 0.4765625\n",
            "n_iters: 16863 test accuracy: 0.424 gradient_norm: 9.531305230289306\n",
            "n_iters: 16873 loss: 1.5817850828170776, accuracy: 0.3984375\n",
            "n_iters: 16883 loss: 1.7207245826721191, accuracy: 0.4140625\n",
            "n_iters: 16893 loss: 1.5842219591140747, accuracy: 0.4609375\n",
            "n_iters: 16903 loss: 1.546196460723877, accuracy: 0.4140625\n",
            "n_iters: 16913 loss: 1.7148407697677612, accuracy: 0.4296875\n",
            "n_iters: 16913 test accuracy: 0.428 gradient_norm: 9.35814136922284\n",
            "n_iters: 16923 loss: 1.733975887298584, accuracy: 0.390625\n",
            "n_iters: 16933 loss: 1.4006270170211792, accuracy: 0.5\n",
            "n_iters: 16943 loss: 1.5748170614242554, accuracy: 0.453125\n",
            "n_iters: 16953 loss: 1.5129287242889404, accuracy: 0.4765625\n",
            "n_iters: 16963 loss: 1.5402631759643555, accuracy: 0.484375\n",
            "n_iters: 16963 test accuracy: 0.414 gradient_norm: 10.438126063355545\n",
            "n_iters: 16973 loss: 1.518618106842041, accuracy: 0.4609375\n",
            "n_iters: 16983 loss: 1.6987204551696777, accuracy: 0.46875\n",
            "n_iters: 16993 loss: 1.7013813257217407, accuracy: 0.390625\n",
            "n_iters: 17003 loss: 1.6201564073562622, accuracy: 0.421875\n",
            "n_iters: 17013 loss: 1.563617467880249, accuracy: 0.421875\n",
            "n_iters: 17013 test accuracy: 0.416 gradient_norm: 10.355065154634142\n",
            "n_iters: 17023 loss: 1.5056800842285156, accuracy: 0.4140625\n",
            "n_iters: 17033 loss: 1.6974385976791382, accuracy: 0.390625\n",
            "n_iters: 17043 loss: 1.5803968906402588, accuracy: 0.453125\n",
            "n_iters: 17053 loss: 1.6030848026275635, accuracy: 0.4140625\n",
            "n_iters: 17063 loss: 1.489324927330017, accuracy: 0.4921875\n",
            "n_iters: 17063 test accuracy: 0.411 gradient_norm: 8.759183752258295\n",
            "n_iters: 17073 loss: 1.6072427034378052, accuracy: 0.4375\n",
            "n_iters: 17083 loss: 1.66519296169281, accuracy: 0.4296875\n",
            "n_iters: 17093 loss: 1.6407899856567383, accuracy: 0.375\n",
            "n_iters: 17103 loss: 1.5284414291381836, accuracy: 0.4609375\n",
            "n_iters: 17113 loss: 1.71454918384552, accuracy: 0.4453125\n",
            "n_iters: 17113 test accuracy: 0.426 gradient_norm: 10.493267127470844\n",
            "n_iters: 17123 loss: 1.5559481382369995, accuracy: 0.5\n",
            "n_iters: 17133 loss: 1.5100117921829224, accuracy: 0.4453125\n",
            "n_iters: 17143 loss: 1.5859978199005127, accuracy: 0.4296875\n",
            "n_iters: 17153 loss: 1.5460753440856934, accuracy: 0.4140625\n",
            "n_iters: 17163 loss: 1.5204633474349976, accuracy: 0.484375\n",
            "n_iters: 17163 test accuracy: 0.418 gradient_norm: 10.144814755728005\n",
            "n_iters: 17173 loss: 1.577580451965332, accuracy: 0.453125\n",
            "n_iters: 17183 loss: 1.4903227090835571, accuracy: 0.4921875\n",
            "n_iters: 17193 loss: 1.5673425197601318, accuracy: 0.4453125\n",
            "n_iters: 17203 loss: 1.6481033563613892, accuracy: 0.3875\n",
            "n_iters: 17204 loss: 1.6886719465255737, accuracy: 0.40625\n",
            "n_iters: 17204 test accuracy: 0.424 gradient_norm: 10.700709426336374\n",
            "n_iters: 17214 loss: 1.6536263227462769, accuracy: 0.40625\n",
            "n_iters: 17224 loss: 1.575568675994873, accuracy: 0.4375\n",
            "n_iters: 17234 loss: 1.572209119796753, accuracy: 0.421875\n",
            "n_iters: 17244 loss: 1.563630223274231, accuracy: 0.3984375\n",
            "n_iters: 17254 loss: 1.5446821451187134, accuracy: 0.4375\n",
            "n_iters: 17254 test accuracy: 0.415 gradient_norm: 10.107558004486789\n",
            "n_iters: 17264 loss: 1.6674977540969849, accuracy: 0.3828125\n",
            "n_iters: 17274 loss: 1.5562591552734375, accuracy: 0.484375\n",
            "n_iters: 17284 loss: 1.595028042793274, accuracy: 0.40625\n",
            "n_iters: 17294 loss: 1.5746219158172607, accuracy: 0.359375\n",
            "n_iters: 17304 loss: 1.5590291023254395, accuracy: 0.453125\n",
            "n_iters: 17304 test accuracy: 0.416 gradient_norm: 9.934575394504852\n",
            "n_iters: 17314 loss: 1.614782452583313, accuracy: 0.421875\n",
            "n_iters: 17324 loss: 1.694353461265564, accuracy: 0.4140625\n",
            "n_iters: 17334 loss: 1.6912591457366943, accuracy: 0.4140625\n",
            "n_iters: 17344 loss: 1.4713019132614136, accuracy: 0.4765625\n",
            "n_iters: 17354 loss: 1.472150444984436, accuracy: 0.4140625\n",
            "n_iters: 17354 test accuracy: 0.415 gradient_norm: 9.085795905493711\n",
            "n_iters: 17364 loss: 1.6298121213912964, accuracy: 0.4296875\n",
            "n_iters: 17374 loss: 1.597314476966858, accuracy: 0.390625\n",
            "n_iters: 17384 loss: 1.5812777280807495, accuracy: 0.4296875\n",
            "n_iters: 17394 loss: 1.6593666076660156, accuracy: 0.3828125\n",
            "n_iters: 17404 loss: 1.656627893447876, accuracy: 0.4140625\n",
            "n_iters: 17404 test accuracy: 0.407 gradient_norm: 10.740608925370648\n",
            "n_iters: 17414 loss: 1.709438681602478, accuracy: 0.3671875\n",
            "n_iters: 17424 loss: 1.7092417478561401, accuracy: 0.375\n",
            "n_iters: 17434 loss: 1.6493465900421143, accuracy: 0.4453125\n",
            "n_iters: 17444 loss: 1.4715652465820312, accuracy: 0.4609375\n",
            "n_iters: 17454 loss: 1.5667270421981812, accuracy: 0.484375\n",
            "n_iters: 17454 test accuracy: 0.421 gradient_norm: 9.599904682112305\n",
            "n_iters: 17464 loss: 1.5468510389328003, accuracy: 0.5\n",
            "n_iters: 17474 loss: 1.5535932779312134, accuracy: 0.4375\n",
            "n_iters: 17484 loss: 1.5222327709197998, accuracy: 0.484375\n",
            "n_iters: 17494 loss: 1.7450264692306519, accuracy: 0.34375\n",
            "n_iters: 17504 loss: 1.6307178735733032, accuracy: 0.484375\n",
            "n_iters: 17504 test accuracy: 0.408 gradient_norm: 10.68352278350089\n",
            "n_iters: 17514 loss: 1.594555377960205, accuracy: 0.484375\n",
            "n_iters: 17524 loss: 1.552673101425171, accuracy: 0.484375\n",
            "n_iters: 17534 loss: 1.6697969436645508, accuracy: 0.3671875\n",
            "n_iters: 17544 loss: 1.755308747291565, accuracy: 0.4140625\n",
            "n_iters: 17554 loss: 1.7014135122299194, accuracy: 0.3359375\n",
            "n_iters: 17554 test accuracy: 0.406 gradient_norm: 10.987511651805425\n",
            "n_iters: 17564 loss: 1.541968584060669, accuracy: 0.453125\n",
            "n_iters: 17574 loss: 1.6620157957077026, accuracy: 0.421875\n",
            "n_iters: 17584 loss: 1.6148556470870972, accuracy: 0.4609375\n",
            "n_iters: 17594 loss: 1.4794540405273438, accuracy: 0.5375\n",
            "n_iters: 17595 loss: 1.57642662525177, accuracy: 0.46875\n",
            "n_iters: 17595 test accuracy: 0.445 gradient_norm: 9.81851914465607\n",
            "n_iters: 17605 loss: 1.5935472249984741, accuracy: 0.46875\n",
            "n_iters: 17615 loss: 1.6443499326705933, accuracy: 0.40625\n",
            "n_iters: 17625 loss: 1.5423039197921753, accuracy: 0.4921875\n",
            "n_iters: 17635 loss: 1.7341573238372803, accuracy: 0.3828125\n",
            "n_iters: 17645 loss: 1.5862263441085815, accuracy: 0.4453125\n",
            "n_iters: 17645 test accuracy: 0.409 gradient_norm: 10.00918506035706\n",
            "n_iters: 17655 loss: 1.5318560600280762, accuracy: 0.4375\n",
            "n_iters: 17665 loss: 1.5414538383483887, accuracy: 0.4765625\n",
            "n_iters: 17675 loss: 1.566774845123291, accuracy: 0.4375\n",
            "n_iters: 17685 loss: 1.4498851299285889, accuracy: 0.5\n",
            "n_iters: 17695 loss: 1.6655192375183105, accuracy: 0.4375\n",
            "n_iters: 17695 test accuracy: 0.414 gradient_norm: 9.535614878204349\n",
            "n_iters: 17705 loss: 1.4864616394042969, accuracy: 0.46875\n",
            "n_iters: 17715 loss: 1.581263542175293, accuracy: 0.4765625\n",
            "n_iters: 17725 loss: 1.6616889238357544, accuracy: 0.40625\n",
            "n_iters: 17735 loss: 1.5130705833435059, accuracy: 0.40625\n",
            "n_iters: 17745 loss: 1.593896508216858, accuracy: 0.4296875\n",
            "n_iters: 17745 test accuracy: 0.418 gradient_norm: 9.61080351799795\n",
            "n_iters: 17755 loss: 1.6484880447387695, accuracy: 0.4375\n",
            "n_iters: 17765 loss: 1.4517902135849, accuracy: 0.53125\n",
            "n_iters: 17775 loss: 1.6534287929534912, accuracy: 0.4296875\n",
            "n_iters: 17785 loss: 1.7311680316925049, accuracy: 0.453125\n",
            "n_iters: 17795 loss: 1.4059674739837646, accuracy: 0.53125\n",
            "n_iters: 17795 test accuracy: 0.426 gradient_norm: 10.961032904701419\n",
            "n_iters: 17805 loss: 1.4235972166061401, accuracy: 0.4921875\n",
            "n_iters: 17815 loss: 1.3627440929412842, accuracy: 0.4921875\n",
            "n_iters: 17825 loss: 1.7099751234054565, accuracy: 0.46875\n",
            "n_iters: 17835 loss: 1.7477680444717407, accuracy: 0.4140625\n",
            "n_iters: 17845 loss: 1.5237829685211182, accuracy: 0.4609375\n",
            "n_iters: 17845 test accuracy: 0.421 gradient_norm: 8.543966374136641\n",
            "n_iters: 17855 loss: 1.502427101135254, accuracy: 0.484375\n",
            "n_iters: 17865 loss: 1.666266918182373, accuracy: 0.4140625\n",
            "n_iters: 17875 loss: 1.5919350385665894, accuracy: 0.453125\n",
            "n_iters: 17885 loss: 1.7086235284805298, accuracy: 0.4140625\n",
            "n_iters: 17895 loss: 1.5556633472442627, accuracy: 0.4765625\n",
            "n_iters: 17895 test accuracy: 0.405 gradient_norm: 9.664498075690249\n",
            "n_iters: 17905 loss: 1.5545090436935425, accuracy: 0.5078125\n",
            "n_iters: 17915 loss: 1.551293969154358, accuracy: 0.46875\n",
            "n_iters: 17925 loss: 1.5578863620758057, accuracy: 0.453125\n",
            "n_iters: 17935 loss: 1.5727903842926025, accuracy: 0.4765625\n",
            "n_iters: 17945 loss: 1.533715844154358, accuracy: 0.453125\n",
            "n_iters: 17945 test accuracy: 0.434 gradient_norm: 9.89883321829141\n",
            "n_iters: 17955 loss: 1.5938794612884521, accuracy: 0.453125\n",
            "n_iters: 17965 loss: 1.5315260887145996, accuracy: 0.5\n",
            "n_iters: 17975 loss: 1.5862678289413452, accuracy: 0.3828125\n",
            "n_iters: 17985 loss: 1.4970401525497437, accuracy: 0.475\n",
            "n_iters: 17986 loss: 1.5910955667495728, accuracy: 0.4296875\n",
            "n_iters: 17986 test accuracy: 0.428 gradient_norm: 10.9215721092719\n",
            "n_iters: 17996 loss: 1.6106005907058716, accuracy: 0.4296875\n",
            "n_iters: 18006 loss: 1.7038581371307373, accuracy: 0.4140625\n",
            "n_iters: 18016 loss: 1.4273079633712769, accuracy: 0.53125\n",
            "n_iters: 18026 loss: 1.6555770635604858, accuracy: 0.40625\n",
            "n_iters: 18036 loss: 1.5453922748565674, accuracy: 0.453125\n",
            "n_iters: 18036 test accuracy: 0.414 gradient_norm: 10.972685753521008\n",
            "n_iters: 18046 loss: 1.58734929561615, accuracy: 0.4453125\n",
            "n_iters: 18056 loss: 1.7268614768981934, accuracy: 0.4453125\n",
            "n_iters: 18066 loss: 1.597852110862732, accuracy: 0.40625\n",
            "n_iters: 18076 loss: 1.5537301301956177, accuracy: 0.5\n",
            "n_iters: 18086 loss: 1.6900489330291748, accuracy: 0.4140625\n",
            "n_iters: 18086 test accuracy: 0.412 gradient_norm: 10.507505970077885\n",
            "n_iters: 18096 loss: 1.612742304801941, accuracy: 0.4375\n",
            "n_iters: 18106 loss: 1.542004108428955, accuracy: 0.5\n",
            "n_iters: 18116 loss: 1.605978012084961, accuracy: 0.4296875\n",
            "n_iters: 18126 loss: 1.6913115978240967, accuracy: 0.359375\n",
            "n_iters: 18136 loss: 1.5288444757461548, accuracy: 0.515625\n",
            "n_iters: 18136 test accuracy: 0.436 gradient_norm: 11.368410548290472\n",
            "n_iters: 18146 loss: 1.6519386768341064, accuracy: 0.421875\n",
            "n_iters: 18156 loss: 1.6045806407928467, accuracy: 0.3984375\n",
            "n_iters: 18166 loss: 1.450716257095337, accuracy: 0.4921875\n",
            "n_iters: 18176 loss: 1.7184319496154785, accuracy: 0.3828125\n",
            "n_iters: 18186 loss: 1.5320695638656616, accuracy: 0.46875\n",
            "n_iters: 18186 test accuracy: 0.418 gradient_norm: 11.367283868190004\n",
            "n_iters: 18196 loss: 1.5749726295471191, accuracy: 0.453125\n",
            "n_iters: 18206 loss: 1.5634251832962036, accuracy: 0.46875\n",
            "n_iters: 18216 loss: 1.6135584115982056, accuracy: 0.4296875\n",
            "n_iters: 18226 loss: 1.5879249572753906, accuracy: 0.4609375\n",
            "n_iters: 18236 loss: 1.625662088394165, accuracy: 0.40625\n",
            "n_iters: 18236 test accuracy: 0.403 gradient_norm: 10.169606991085011\n",
            "n_iters: 18246 loss: 1.5513765811920166, accuracy: 0.4453125\n",
            "n_iters: 18256 loss: 1.6814404726028442, accuracy: 0.421875\n",
            "n_iters: 18266 loss: 1.7051875591278076, accuracy: 0.3828125\n",
            "n_iters: 18276 loss: 1.4347457885742188, accuracy: 0.53125\n",
            "n_iters: 18286 loss: 1.621411681175232, accuracy: 0.453125\n",
            "n_iters: 18286 test accuracy: 0.418 gradient_norm: 9.551972772890752\n",
            "n_iters: 18296 loss: 1.78682541847229, accuracy: 0.375\n",
            "n_iters: 18306 loss: 1.5931214094161987, accuracy: 0.4453125\n",
            "n_iters: 18316 loss: 1.6529747247695923, accuracy: 0.375\n",
            "n_iters: 18326 loss: 1.6742585897445679, accuracy: 0.4140625\n",
            "n_iters: 18336 loss: 1.6687597036361694, accuracy: 0.40625\n",
            "n_iters: 18336 test accuracy: 0.442 gradient_norm: 11.350722008318542\n",
            "n_iters: 18346 loss: 1.5184592008590698, accuracy: 0.5234375\n",
            "n_iters: 18356 loss: 1.4365694522857666, accuracy: 0.484375\n",
            "n_iters: 18366 loss: 1.6468919515609741, accuracy: 0.40625\n",
            "n_iters: 18376 loss: 1.4899808168411255, accuracy: 0.425\n",
            "n_iters: 18377 loss: 1.6011357307434082, accuracy: 0.453125\n",
            "n_iters: 18377 test accuracy: 0.423 gradient_norm: 11.597099436538455\n",
            "n_iters: 18387 loss: 1.613501787185669, accuracy: 0.3984375\n",
            "n_iters: 18397 loss: 1.7309496402740479, accuracy: 0.359375\n",
            "n_iters: 18407 loss: 1.6596277952194214, accuracy: 0.5078125\n",
            "n_iters: 18417 loss: 1.554197907447815, accuracy: 0.421875\n",
            "n_iters: 18427 loss: 1.5996824502944946, accuracy: 0.453125\n",
            "n_iters: 18427 test accuracy: 0.407 gradient_norm: 10.154450480267501\n",
            "n_iters: 18437 loss: 1.5606893301010132, accuracy: 0.4921875\n",
            "n_iters: 18447 loss: 1.5633360147476196, accuracy: 0.4140625\n",
            "n_iters: 18457 loss: 1.6968235969543457, accuracy: 0.4296875\n",
            "n_iters: 18467 loss: 1.6036931276321411, accuracy: 0.421875\n",
            "n_iters: 18477 loss: 1.6728109121322632, accuracy: 0.4140625\n",
            "n_iters: 18477 test accuracy: 0.412 gradient_norm: 10.841440414694713\n",
            "n_iters: 18487 loss: 1.4819930791854858, accuracy: 0.46875\n",
            "n_iters: 18497 loss: 1.5565028190612793, accuracy: 0.4296875\n",
            "n_iters: 18507 loss: 1.5050578117370605, accuracy: 0.484375\n",
            "n_iters: 18517 loss: 1.7143864631652832, accuracy: 0.3359375\n",
            "n_iters: 18527 loss: 1.581679105758667, accuracy: 0.3984375\n",
            "n_iters: 18527 test accuracy: 0.424 gradient_norm: 10.945542950032543\n",
            "n_iters: 18537 loss: 1.6314032077789307, accuracy: 0.3984375\n",
            "n_iters: 18547 loss: 1.641385793685913, accuracy: 0.3828125\n",
            "n_iters: 18557 loss: 1.7263263463974, accuracy: 0.421875\n",
            "n_iters: 18567 loss: 1.4284054040908813, accuracy: 0.453125\n",
            "n_iters: 18577 loss: 1.4864648580551147, accuracy: 0.5390625\n",
            "n_iters: 18577 test accuracy: 0.432 gradient_norm: 10.939208377408722\n",
            "n_iters: 18587 loss: 1.522904872894287, accuracy: 0.4921875\n",
            "n_iters: 18597 loss: 1.4151670932769775, accuracy: 0.5\n",
            "n_iters: 18607 loss: 1.4473445415496826, accuracy: 0.4921875\n",
            "n_iters: 18617 loss: 1.631925344467163, accuracy: 0.4140625\n",
            "n_iters: 18627 loss: 1.4466696977615356, accuracy: 0.3828125\n",
            "n_iters: 18627 test accuracy: 0.409 gradient_norm: 9.922820196377476\n",
            "n_iters: 18637 loss: 1.5862596035003662, accuracy: 0.4140625\n",
            "n_iters: 18647 loss: 1.602539300918579, accuracy: 0.4296875\n",
            "n_iters: 18657 loss: 1.580627202987671, accuracy: 0.5\n",
            "n_iters: 18667 loss: 1.626713752746582, accuracy: 0.40625\n",
            "n_iters: 18677 loss: 1.6887092590332031, accuracy: 0.34375\n",
            "n_iters: 18677 test accuracy: 0.414 gradient_norm: 10.632096757921433\n",
            "n_iters: 18687 loss: 1.46514892578125, accuracy: 0.4609375\n",
            "n_iters: 18697 loss: 1.7029268741607666, accuracy: 0.40625\n",
            "n_iters: 18707 loss: 1.4777683019638062, accuracy: 0.453125\n",
            "n_iters: 18717 loss: 1.7232636213302612, accuracy: 0.4453125\n",
            "n_iters: 18727 loss: 1.4804072380065918, accuracy: 0.5\n",
            "n_iters: 18727 test accuracy: 0.432 gradient_norm: 11.005755404846598\n",
            "n_iters: 18737 loss: 1.514241337776184, accuracy: 0.4375\n",
            "n_iters: 18747 loss: 1.6336520910263062, accuracy: 0.375\n",
            "n_iters: 18757 loss: 1.648066759109497, accuracy: 0.4609375\n",
            "n_iters: 18767 loss: 1.7336976528167725, accuracy: 0.3875\n",
            "n_iters: 18768 loss: 1.452610969543457, accuracy: 0.5234375\n",
            "n_iters: 18768 test accuracy: 0.43 gradient_norm: 9.878676103573662\n",
            "n_iters: 18778 loss: 1.6599016189575195, accuracy: 0.4609375\n",
            "n_iters: 18788 loss: 1.5434067249298096, accuracy: 0.4921875\n",
            "n_iters: 18798 loss: 1.5551522970199585, accuracy: 0.421875\n",
            "n_iters: 18808 loss: 1.467638611793518, accuracy: 0.453125\n",
            "n_iters: 18818 loss: 1.5787485837936401, accuracy: 0.484375\n",
            "n_iters: 18818 test accuracy: 0.416 gradient_norm: 11.791051533515015\n",
            "n_iters: 18828 loss: 1.559302806854248, accuracy: 0.5\n",
            "n_iters: 18838 loss: 1.5628739595413208, accuracy: 0.4453125\n",
            "n_iters: 18848 loss: 1.5293958187103271, accuracy: 0.484375\n",
            "n_iters: 18858 loss: 1.5739059448242188, accuracy: 0.421875\n",
            "n_iters: 18868 loss: 1.496313214302063, accuracy: 0.5078125\n",
            "n_iters: 18868 test accuracy: 0.417 gradient_norm: 10.04021168540546\n",
            "n_iters: 18878 loss: 1.8205077648162842, accuracy: 0.3984375\n",
            "n_iters: 18888 loss: 1.5013184547424316, accuracy: 0.5\n",
            "n_iters: 18898 loss: 1.4074655771255493, accuracy: 0.5390625\n",
            "n_iters: 18908 loss: 1.6400843858718872, accuracy: 0.4296875\n",
            "n_iters: 18918 loss: 1.6424087285995483, accuracy: 0.34375\n",
            "n_iters: 18918 test accuracy: 0.408 gradient_norm: 10.922911215842218\n",
            "n_iters: 18928 loss: 1.5582327842712402, accuracy: 0.484375\n",
            "n_iters: 18938 loss: 1.5103272199630737, accuracy: 0.46875\n",
            "n_iters: 18948 loss: 1.4934589862823486, accuracy: 0.453125\n",
            "n_iters: 18958 loss: 1.5336837768554688, accuracy: 0.4453125\n",
            "n_iters: 18968 loss: 1.671800136566162, accuracy: 0.3984375\n",
            "n_iters: 18968 test accuracy: 0.418 gradient_norm: 10.780702186278502\n",
            "n_iters: 18978 loss: 1.4673172235488892, accuracy: 0.5703125\n",
            "n_iters: 18988 loss: 1.6385222673416138, accuracy: 0.4375\n",
            "n_iters: 18998 loss: 1.5960955619812012, accuracy: 0.484375\n",
            "n_iters: 19008 loss: 1.5011546611785889, accuracy: 0.46875\n",
            "n_iters: 19018 loss: 1.6005371809005737, accuracy: 0.40625\n",
            "n_iters: 19018 test accuracy: 0.41 gradient_norm: 9.957201942922108\n",
            "n_iters: 19028 loss: 1.5596979856491089, accuracy: 0.40625\n",
            "n_iters: 19038 loss: 1.6629106998443604, accuracy: 0.40625\n",
            "n_iters: 19048 loss: 1.6485918760299683, accuracy: 0.40625\n",
            "n_iters: 19058 loss: 1.4329538345336914, accuracy: 0.4296875\n",
            "n_iters: 19068 loss: 1.556921362876892, accuracy: 0.40625\n",
            "n_iters: 19068 test accuracy: 0.404 gradient_norm: 10.579611423155432\n",
            "n_iters: 19078 loss: 1.6017627716064453, accuracy: 0.4609375\n",
            "n_iters: 19088 loss: 1.6802712678909302, accuracy: 0.453125\n",
            "n_iters: 19098 loss: 1.5103744268417358, accuracy: 0.4765625\n",
            "n_iters: 19108 loss: 1.6373370885849, accuracy: 0.40625\n",
            "n_iters: 19118 loss: 1.4181667566299438, accuracy: 0.484375\n",
            "n_iters: 19118 test accuracy: 0.403 gradient_norm: 10.148936956095245\n",
            "n_iters: 19128 loss: 1.6260271072387695, accuracy: 0.453125\n",
            "n_iters: 19138 loss: 1.5797948837280273, accuracy: 0.46875\n",
            "n_iters: 19148 loss: 1.600246787071228, accuracy: 0.4609375\n",
            "n_iters: 19158 loss: 1.4294463396072388, accuracy: 0.425\n",
            "n_iters: 19159 loss: 1.5308955907821655, accuracy: 0.4140625\n",
            "n_iters: 19159 test accuracy: 0.411 gradient_norm: 10.317380182889337\n",
            "n_iters: 19169 loss: 1.651785969734192, accuracy: 0.4296875\n",
            "n_iters: 19179 loss: 1.5370888710021973, accuracy: 0.4609375\n",
            "n_iters: 19189 loss: 1.6588234901428223, accuracy: 0.4609375\n",
            "n_iters: 19199 loss: 1.7043436765670776, accuracy: 0.3671875\n",
            "n_iters: 19209 loss: 1.5514030456542969, accuracy: 0.421875\n",
            "n_iters: 19209 test accuracy: 0.42 gradient_norm: 10.079992626066742\n",
            "n_iters: 19219 loss: 1.4630005359649658, accuracy: 0.5234375\n",
            "n_iters: 19229 loss: 1.453245759010315, accuracy: 0.5078125\n",
            "n_iters: 19239 loss: 1.5060313940048218, accuracy: 0.5\n",
            "n_iters: 19249 loss: 1.5360174179077148, accuracy: 0.4375\n",
            "n_iters: 19259 loss: 1.571362018585205, accuracy: 0.40625\n",
            "n_iters: 19259 test accuracy: 0.417 gradient_norm: 10.304327230220784\n",
            "n_iters: 19269 loss: 1.5614641904830933, accuracy: 0.4921875\n",
            "n_iters: 19279 loss: 1.5225911140441895, accuracy: 0.4609375\n",
            "n_iters: 19289 loss: 1.5714917182922363, accuracy: 0.4140625\n",
            "n_iters: 19299 loss: 1.5319781303405762, accuracy: 0.484375\n",
            "n_iters: 19309 loss: 1.679991364479065, accuracy: 0.4375\n",
            "n_iters: 19309 test accuracy: 0.417 gradient_norm: 9.261751689490378\n",
            "n_iters: 19319 loss: 1.718979835510254, accuracy: 0.390625\n",
            "n_iters: 19329 loss: 1.5730791091918945, accuracy: 0.4296875\n",
            "n_iters: 19339 loss: 1.5498868227005005, accuracy: 0.453125\n",
            "n_iters: 19349 loss: 1.5388729572296143, accuracy: 0.4609375\n",
            "n_iters: 19359 loss: 1.4779826402664185, accuracy: 0.4453125\n",
            "n_iters: 19359 test accuracy: 0.417 gradient_norm: 8.802364233010533\n",
            "n_iters: 19369 loss: 1.4603022336959839, accuracy: 0.5234375\n",
            "n_iters: 19379 loss: 1.4749137163162231, accuracy: 0.5078125\n",
            "n_iters: 19389 loss: 1.5487996339797974, accuracy: 0.4765625\n",
            "n_iters: 19399 loss: 1.6089216470718384, accuracy: 0.453125\n",
            "n_iters: 19409 loss: 1.5635942220687866, accuracy: 0.40625\n",
            "n_iters: 19409 test accuracy: 0.4 gradient_norm: 10.196612938192391\n",
            "n_iters: 19419 loss: 1.647912859916687, accuracy: 0.4140625\n",
            "n_iters: 19429 loss: 1.8500478267669678, accuracy: 0.390625\n",
            "n_iters: 19439 loss: 1.4887245893478394, accuracy: 0.5078125\n",
            "n_iters: 19449 loss: 1.6949644088745117, accuracy: 0.4296875\n",
            "n_iters: 19459 loss: 1.570971131324768, accuracy: 0.40625\n",
            "n_iters: 19459 test accuracy: 0.422 gradient_norm: 9.719237672036911\n",
            "n_iters: 19469 loss: 1.5972669124603271, accuracy: 0.484375\n",
            "n_iters: 19479 loss: 1.6367968320846558, accuracy: 0.4296875\n",
            "n_iters: 19489 loss: 1.62595534324646, accuracy: 0.4140625\n",
            "n_iters: 19499 loss: 1.484572410583496, accuracy: 0.4609375\n",
            "n_iters: 19509 loss: 1.6916027069091797, accuracy: 0.3984375\n",
            "n_iters: 19509 test accuracy: 0.407 gradient_norm: 9.716314220283598\n",
            "n_iters: 19519 loss: 1.4843716621398926, accuracy: 0.53125\n",
            "n_iters: 19529 loss: 1.5195039510726929, accuracy: 0.484375\n",
            "n_iters: 19539 loss: 1.6302815675735474, accuracy: 0.421875\n",
            "n_iters: 19549 loss: 1.6667810678482056, accuracy: 0.45\n",
            "n_iters: 19550 loss: 1.4155805110931396, accuracy: 0.4765625\n",
            "n_iters: 19550 test accuracy: 0.392 gradient_norm: 9.406170192448288\n",
            "n_iters: 19560 loss: 1.657883644104004, accuracy: 0.40625\n",
            "n_iters: 19570 loss: 1.5398106575012207, accuracy: 0.5546875\n",
            "n_iters: 19580 loss: 1.688596248626709, accuracy: 0.390625\n",
            "n_iters: 19590 loss: 1.6489148139953613, accuracy: 0.421875\n",
            "n_iters: 19600 loss: 1.4576706886291504, accuracy: 0.5078125\n",
            "n_iters: 19600 test accuracy: 0.418 gradient_norm: 9.347183521413926\n",
            "n_iters: 19610 loss: 1.586232304573059, accuracy: 0.4375\n",
            "n_iters: 19620 loss: 1.6231327056884766, accuracy: 0.421875\n",
            "n_iters: 19630 loss: 1.5373215675354004, accuracy: 0.4765625\n",
            "n_iters: 19640 loss: 1.6033596992492676, accuracy: 0.3828125\n",
            "n_iters: 19650 loss: 1.4780305624008179, accuracy: 0.46875\n",
            "n_iters: 19650 test accuracy: 0.432 gradient_norm: 10.399943776225054\n",
            "n_iters: 19660 loss: 1.7118014097213745, accuracy: 0.4453125\n",
            "n_iters: 19670 loss: 1.6351091861724854, accuracy: 0.3984375\n",
            "n_iters: 19680 loss: 1.6369298696517944, accuracy: 0.421875\n",
            "n_iters: 19690 loss: 1.5039393901824951, accuracy: 0.4609375\n",
            "n_iters: 19700 loss: 1.621947169303894, accuracy: 0.421875\n",
            "n_iters: 19700 test accuracy: 0.42 gradient_norm: 12.586322129723323\n",
            "n_iters: 19710 loss: 1.5425174236297607, accuracy: 0.4921875\n",
            "n_iters: 19720 loss: 1.6277953386306763, accuracy: 0.4375\n",
            "n_iters: 19730 loss: 1.5460129976272583, accuracy: 0.390625\n",
            "n_iters: 19740 loss: 1.636000156402588, accuracy: 0.421875\n",
            "n_iters: 19750 loss: 1.5765262842178345, accuracy: 0.4296875\n",
            "n_iters: 19750 test accuracy: 0.417 gradient_norm: 10.927653242028631\n",
            "n_iters: 19760 loss: 1.621429443359375, accuracy: 0.4609375\n",
            "n_iters: 19770 loss: 1.5272012948989868, accuracy: 0.4765625\n",
            "n_iters: 19780 loss: 1.5790306329727173, accuracy: 0.4140625\n",
            "n_iters: 19790 loss: 1.659324288368225, accuracy: 0.40625\n",
            "n_iters: 19800 loss: 1.4869245290756226, accuracy: 0.484375\n",
            "n_iters: 19800 test accuracy: 0.431 gradient_norm: 10.620946045595405\n",
            "n_iters: 19810 loss: 1.6543340682983398, accuracy: 0.421875\n",
            "n_iters: 19820 loss: 1.639471411705017, accuracy: 0.4921875\n",
            "n_iters: 19830 loss: 1.482359766960144, accuracy: 0.5390625\n",
            "n_iters: 19840 loss: 1.669363260269165, accuracy: 0.421875\n",
            "n_iters: 19850 loss: 1.8085941076278687, accuracy: 0.3671875\n",
            "n_iters: 19850 test accuracy: 0.407 gradient_norm: 11.70225157898066\n",
            "n_iters: 19860 loss: 1.4253392219543457, accuracy: 0.5234375\n",
            "n_iters: 19870 loss: 1.6020400524139404, accuracy: 0.4140625\n",
            "n_iters: 19880 loss: 1.6315146684646606, accuracy: 0.453125\n",
            "n_iters: 19890 loss: 1.5443700551986694, accuracy: 0.421875\n",
            "n_iters: 19900 loss: 1.4943795204162598, accuracy: 0.46875\n",
            "n_iters: 19900 test accuracy: 0.416 gradient_norm: 9.551566430887375\n",
            "n_iters: 19910 loss: 1.6895451545715332, accuracy: 0.4921875\n",
            "n_iters: 19920 loss: 1.6099575757980347, accuracy: 0.4140625\n",
            "n_iters: 19930 loss: 1.5963659286499023, accuracy: 0.484375\n",
            "n_iters: 19940 loss: 1.694382667541504, accuracy: 0.45\n",
            "n_iters: 19941 loss: 1.5052143335342407, accuracy: 0.4765625\n",
            "n_iters: 19941 test accuracy: 0.41 gradient_norm: 10.347677425435098\n",
            "n_iters: 19951 loss: 1.5097911357879639, accuracy: 0.484375\n",
            "n_iters: 19961 loss: 1.6332334280014038, accuracy: 0.3671875\n",
            "n_iters: 19971 loss: 1.6701785326004028, accuracy: 0.4296875\n",
            "n_iters: 19981 loss: 1.6276586055755615, accuracy: 0.3984375\n",
            "n_iters: 19991 loss: 1.6108875274658203, accuracy: 0.40625\n",
            "n_iters: 19991 test accuracy: 0.402 gradient_norm: 9.813019638742983\n",
            "n_iters: 20001 loss: 1.565716028213501, accuracy: 0.46875\n",
            "n_iters: 20011 loss: 1.6732361316680908, accuracy: 0.4140625\n",
            "n_iters: 20021 loss: 1.4596872329711914, accuracy: 0.4765625\n",
            "n_iters: 20031 loss: 1.6283899545669556, accuracy: 0.3828125\n",
            "n_iters: 20041 loss: 1.4171624183654785, accuracy: 0.4921875\n",
            "n_iters: 20041 test accuracy: 0.419 gradient_norm: 9.93102161662933\n",
            "n_iters: 20051 loss: 1.6389179229736328, accuracy: 0.4375\n",
            "n_iters: 20061 loss: 1.6750011444091797, accuracy: 0.46875\n",
            "n_iters: 20071 loss: 1.447701096534729, accuracy: 0.515625\n",
            "n_iters: 20081 loss: 1.630795955657959, accuracy: 0.4453125\n",
            "n_iters: 20091 loss: 1.5637296438217163, accuracy: 0.4453125\n",
            "n_iters: 20091 test accuracy: 0.402 gradient_norm: 9.05225347981533\n",
            "n_iters: 20101 loss: 1.534246802330017, accuracy: 0.3828125\n",
            "n_iters: 20111 loss: 1.6906015872955322, accuracy: 0.3828125\n",
            "n_iters: 20121 loss: 1.5119671821594238, accuracy: 0.5\n",
            "n_iters: 20131 loss: 1.6694285869598389, accuracy: 0.390625\n",
            "n_iters: 20141 loss: 1.5676648616790771, accuracy: 0.4765625\n",
            "n_iters: 20141 test accuracy: 0.422 gradient_norm: 9.643361930643799\n",
            "n_iters: 20151 loss: 1.6224387884140015, accuracy: 0.4375\n",
            "n_iters: 20161 loss: 1.4844566583633423, accuracy: 0.484375\n",
            "n_iters: 20171 loss: 1.677849531173706, accuracy: 0.4609375\n",
            "n_iters: 20181 loss: 1.6647891998291016, accuracy: 0.453125\n",
            "n_iters: 20191 loss: 1.6388053894042969, accuracy: 0.421875\n",
            "n_iters: 20191 test accuracy: 0.421 gradient_norm: 10.025214919713182\n",
            "n_iters: 20201 loss: 1.7046637535095215, accuracy: 0.359375\n",
            "n_iters: 20211 loss: 1.5940426588058472, accuracy: 0.453125\n",
            "n_iters: 20221 loss: 1.624771237373352, accuracy: 0.4140625\n",
            "n_iters: 20231 loss: 1.7070897817611694, accuracy: 0.4140625\n",
            "n_iters: 20241 loss: 1.7543294429779053, accuracy: 0.3671875\n",
            "n_iters: 20241 test accuracy: 0.416 gradient_norm: 10.255473260795997\n",
            "n_iters: 20251 loss: 1.6347503662109375, accuracy: 0.4453125\n",
            "n_iters: 20261 loss: 1.6113680601119995, accuracy: 0.40625\n",
            "n_iters: 20271 loss: 1.4185152053833008, accuracy: 0.5078125\n",
            "n_iters: 20281 loss: 1.5908395051956177, accuracy: 0.3828125\n",
            "n_iters: 20291 loss: 1.5885238647460938, accuracy: 0.5\n",
            "n_iters: 20291 test accuracy: 0.439 gradient_norm: 11.724504209299281\n",
            "n_iters: 20301 loss: 1.4966317415237427, accuracy: 0.4453125\n",
            "n_iters: 20311 loss: 1.4573321342468262, accuracy: 0.5234375\n",
            "n_iters: 20321 loss: 1.4121779203414917, accuracy: 0.4453125\n",
            "n_iters: 20331 loss: 1.6328325271606445, accuracy: 0.4125\n",
            "n_iters: 20332 loss: 1.6322005987167358, accuracy: 0.4140625\n",
            "n_iters: 20332 test accuracy: 0.428 gradient_norm: 11.919737834011174\n",
            "n_iters: 20342 loss: 1.6177194118499756, accuracy: 0.4609375\n",
            "n_iters: 20352 loss: 1.5744595527648926, accuracy: 0.421875\n",
            "n_iters: 20362 loss: 1.6487457752227783, accuracy: 0.40625\n",
            "n_iters: 20372 loss: 1.6816191673278809, accuracy: 0.375\n",
            "n_iters: 20382 loss: 1.7012702226638794, accuracy: 0.4375\n",
            "n_iters: 20382 test accuracy: 0.405 gradient_norm: 10.441654801734384\n",
            "n_iters: 20392 loss: 1.6554131507873535, accuracy: 0.4375\n",
            "n_iters: 20402 loss: 1.4654390811920166, accuracy: 0.484375\n",
            "n_iters: 20412 loss: 1.5639638900756836, accuracy: 0.4375\n",
            "n_iters: 20422 loss: 1.443859577178955, accuracy: 0.4453125\n",
            "n_iters: 20432 loss: 1.5951879024505615, accuracy: 0.453125\n",
            "n_iters: 20432 test accuracy: 0.412 gradient_norm: 10.967928074720092\n",
            "n_iters: 20442 loss: 1.6547068357467651, accuracy: 0.4453125\n",
            "n_iters: 20452 loss: 1.6165478229522705, accuracy: 0.40625\n",
            "n_iters: 20462 loss: 1.5568842887878418, accuracy: 0.421875\n",
            "n_iters: 20472 loss: 1.5686794519424438, accuracy: 0.4453125\n",
            "n_iters: 20482 loss: 1.5843861103057861, accuracy: 0.4453125\n",
            "n_iters: 20482 test accuracy: 0.382 gradient_norm: 12.016172152752397\n",
            "n_iters: 20492 loss: 1.4923900365829468, accuracy: 0.515625\n",
            "n_iters: 20502 loss: 1.5309025049209595, accuracy: 0.421875\n",
            "n_iters: 20512 loss: 1.4657018184661865, accuracy: 0.5078125\n",
            "n_iters: 20522 loss: 1.7109407186508179, accuracy: 0.40625\n",
            "n_iters: 20532 loss: 1.5800248384475708, accuracy: 0.4375\n",
            "n_iters: 20532 test accuracy: 0.432 gradient_norm: 8.599352784072419\n",
            "n_iters: 20542 loss: 1.7346785068511963, accuracy: 0.4140625\n",
            "n_iters: 20552 loss: 1.5359323024749756, accuracy: 0.4375\n",
            "n_iters: 20562 loss: 1.67967689037323, accuracy: 0.3671875\n",
            "n_iters: 20572 loss: 1.6430413722991943, accuracy: 0.4453125\n",
            "n_iters: 20582 loss: 1.4070020914077759, accuracy: 0.5625\n",
            "n_iters: 20582 test accuracy: 0.421 gradient_norm: 9.892652492498264\n",
            "n_iters: 20592 loss: 1.5112334489822388, accuracy: 0.4453125\n",
            "n_iters: 20602 loss: 1.6949139833450317, accuracy: 0.3828125\n",
            "n_iters: 20612 loss: 1.4607138633728027, accuracy: 0.453125\n",
            "n_iters: 20622 loss: 1.6796612739562988, accuracy: 0.3984375\n",
            "n_iters: 20632 loss: 1.70133376121521, accuracy: 0.359375\n",
            "n_iters: 20632 test accuracy: 0.419 gradient_norm: 11.281831436637031\n",
            "n_iters: 20642 loss: 1.49431312084198, accuracy: 0.484375\n",
            "n_iters: 20652 loss: 1.572920322418213, accuracy: 0.4453125\n",
            "n_iters: 20662 loss: 1.5253452062606812, accuracy: 0.4453125\n",
            "n_iters: 20672 loss: 1.5978155136108398, accuracy: 0.3828125\n",
            "n_iters: 20682 loss: 1.5246641635894775, accuracy: 0.46875\n",
            "n_iters: 20682 test accuracy: 0.395 gradient_norm: 9.880953767610482\n",
            "n_iters: 20692 loss: 1.7139652967453003, accuracy: 0.4140625\n",
            "n_iters: 20702 loss: 1.4074819087982178, accuracy: 0.4453125\n",
            "n_iters: 20712 loss: 1.8987382650375366, accuracy: 0.328125\n",
            "n_iters: 20722 loss: 1.4343528747558594, accuracy: 0.6\n",
            "n_iters: 20723 loss: 1.485646367073059, accuracy: 0.4765625\n",
            "n_iters: 20723 test accuracy: 0.446 gradient_norm: 12.625012634400447\n",
            "n_iters: 20733 loss: 1.6684842109680176, accuracy: 0.46875\n",
            "n_iters: 20743 loss: 1.559374213218689, accuracy: 0.4375\n",
            "n_iters: 20753 loss: 1.5784095525741577, accuracy: 0.4921875\n",
            "n_iters: 20763 loss: 1.484806776046753, accuracy: 0.453125\n",
            "n_iters: 20773 loss: 1.5968458652496338, accuracy: 0.3828125\n",
            "n_iters: 20773 test accuracy: 0.417 gradient_norm: 9.46374855325669\n",
            "n_iters: 20783 loss: 1.65635347366333, accuracy: 0.375\n",
            "n_iters: 20793 loss: 1.6727744340896606, accuracy: 0.4453125\n",
            "n_iters: 20803 loss: 1.5496536493301392, accuracy: 0.484375\n",
            "n_iters: 20813 loss: 1.6742149591445923, accuracy: 0.421875\n",
            "n_iters: 20823 loss: 1.538092851638794, accuracy: 0.515625\n",
            "n_iters: 20823 test accuracy: 0.419 gradient_norm: 9.002376348082423\n",
            "n_iters: 20833 loss: 1.6405515670776367, accuracy: 0.4375\n",
            "n_iters: 20843 loss: 1.564021110534668, accuracy: 0.4140625\n",
            "n_iters: 20853 loss: 1.4849272966384888, accuracy: 0.46875\n",
            "n_iters: 20863 loss: 1.5523369312286377, accuracy: 0.4296875\n",
            "n_iters: 20873 loss: 1.5640830993652344, accuracy: 0.4609375\n",
            "n_iters: 20873 test accuracy: 0.409 gradient_norm: 10.651359045877715\n",
            "n_iters: 20883 loss: 1.4935482740402222, accuracy: 0.5\n",
            "n_iters: 20893 loss: 1.4126214981079102, accuracy: 0.4765625\n",
            "n_iters: 20903 loss: 1.7719824314117432, accuracy: 0.421875\n",
            "n_iters: 20913 loss: 1.7200795412063599, accuracy: 0.359375\n",
            "n_iters: 20923 loss: 1.451535940170288, accuracy: 0.46875\n",
            "n_iters: 20923 test accuracy: 0.423 gradient_norm: 10.598844715994574\n",
            "n_iters: 20933 loss: 1.5289729833602905, accuracy: 0.4375\n",
            "n_iters: 20943 loss: 1.6541343927383423, accuracy: 0.3828125\n",
            "n_iters: 20953 loss: 1.6789746284484863, accuracy: 0.4453125\n",
            "n_iters: 20963 loss: 1.6440576314926147, accuracy: 0.4453125\n",
            "n_iters: 20973 loss: 1.7037076950073242, accuracy: 0.40625\n",
            "n_iters: 20973 test accuracy: 0.432 gradient_norm: 11.230425388159661\n",
            "n_iters: 20983 loss: 1.5805057287216187, accuracy: 0.484375\n",
            "n_iters: 20993 loss: 1.5566434860229492, accuracy: 0.453125\n",
            "n_iters: 21003 loss: 1.6029995679855347, accuracy: 0.4921875\n",
            "n_iters: 21013 loss: 1.5024811029434204, accuracy: 0.484375\n",
            "n_iters: 21023 loss: 1.637434720993042, accuracy: 0.4140625\n",
            "n_iters: 21023 test accuracy: 0.405 gradient_norm: 10.965359571149333\n",
            "n_iters: 21033 loss: 1.682403326034546, accuracy: 0.375\n",
            "n_iters: 21043 loss: 1.5915091037750244, accuracy: 0.421875\n",
            "n_iters: 21053 loss: 1.5693676471710205, accuracy: 0.4140625\n",
            "n_iters: 21063 loss: 1.48463773727417, accuracy: 0.4375\n",
            "n_iters: 21073 loss: 1.6550920009613037, accuracy: 0.421875\n",
            "n_iters: 21073 test accuracy: 0.425 gradient_norm: 11.2790648559935\n",
            "n_iters: 21083 loss: 1.6366674900054932, accuracy: 0.3984375\n",
            "n_iters: 21093 loss: 1.6427308320999146, accuracy: 0.375\n",
            "n_iters: 21103 loss: 1.7315548658370972, accuracy: 0.359375\n",
            "n_iters: 21113 loss: 1.6818431615829468, accuracy: 0.4375\n",
            "n_iters: 21114 loss: 1.6351003646850586, accuracy: 0.421875\n",
            "n_iters: 21114 test accuracy: 0.4 gradient_norm: 9.084025728847351\n",
            "n_iters: 21124 loss: 1.5476292371749878, accuracy: 0.46875\n",
            "n_iters: 21134 loss: 1.75533926486969, accuracy: 0.359375\n",
            "n_iters: 21144 loss: 1.7496131658554077, accuracy: 0.4375\n",
            "n_iters: 21154 loss: 1.6050962209701538, accuracy: 0.4375\n",
            "n_iters: 21164 loss: 1.5460680723190308, accuracy: 0.4140625\n",
            "n_iters: 21164 test accuracy: 0.428 gradient_norm: 9.087761598615042\n",
            "n_iters: 21174 loss: 1.481368064880371, accuracy: 0.5234375\n",
            "n_iters: 21184 loss: 1.621476411819458, accuracy: 0.4453125\n",
            "n_iters: 21194 loss: 1.4395642280578613, accuracy: 0.4765625\n",
            "n_iters: 21204 loss: 1.5346461534500122, accuracy: 0.4140625\n",
            "n_iters: 21214 loss: 1.5535869598388672, accuracy: 0.4296875\n",
            "n_iters: 21214 test accuracy: 0.417 gradient_norm: 8.971539120597122\n",
            "n_iters: 21224 loss: 1.594128131866455, accuracy: 0.4453125\n",
            "n_iters: 21234 loss: 1.526657223701477, accuracy: 0.4375\n",
            "n_iters: 21244 loss: 1.538034439086914, accuracy: 0.4375\n",
            "n_iters: 21254 loss: 1.5891046524047852, accuracy: 0.453125\n",
            "n_iters: 21264 loss: 1.6122115850448608, accuracy: 0.390625\n",
            "n_iters: 21264 test accuracy: 0.424 gradient_norm: 10.31229500994747\n",
            "n_iters: 21274 loss: 1.4998708963394165, accuracy: 0.4453125\n",
            "n_iters: 21284 loss: 1.6771312952041626, accuracy: 0.421875\n",
            "n_iters: 21294 loss: 1.661940574645996, accuracy: 0.359375\n",
            "n_iters: 21304 loss: 1.572811484336853, accuracy: 0.40625\n",
            "n_iters: 21314 loss: 1.5519582033157349, accuracy: 0.4375\n",
            "n_iters: 21314 test accuracy: 0.406 gradient_norm: 11.765064921570987\n",
            "n_iters: 21324 loss: 1.55743408203125, accuracy: 0.3828125\n",
            "n_iters: 21334 loss: 1.7542967796325684, accuracy: 0.453125\n",
            "n_iters: 21344 loss: 1.6241049766540527, accuracy: 0.4609375\n",
            "n_iters: 21354 loss: 1.6719748973846436, accuracy: 0.4453125\n",
            "n_iters: 21364 loss: 1.5888208150863647, accuracy: 0.421875\n",
            "n_iters: 21364 test accuracy: 0.428 gradient_norm: 10.594427948336138\n",
            "n_iters: 21374 loss: 1.522444486618042, accuracy: 0.453125\n",
            "n_iters: 21384 loss: 1.5639175176620483, accuracy: 0.421875\n",
            "n_iters: 21394 loss: 1.733762502670288, accuracy: 0.3671875\n",
            "n_iters: 21404 loss: 1.6634397506713867, accuracy: 0.453125\n",
            "n_iters: 21414 loss: 1.5080513954162598, accuracy: 0.46875\n",
            "n_iters: 21414 test accuracy: 0.416 gradient_norm: 9.948722537945406\n",
            "n_iters: 21424 loss: 1.5266315937042236, accuracy: 0.4453125\n",
            "n_iters: 21434 loss: 1.5708421468734741, accuracy: 0.453125\n",
            "n_iters: 21444 loss: 1.6652165651321411, accuracy: 0.375\n",
            "n_iters: 21454 loss: 1.4733182191848755, accuracy: 0.4765625\n",
            "n_iters: 21464 loss: 1.573764443397522, accuracy: 0.4453125\n",
            "n_iters: 21464 test accuracy: 0.417 gradient_norm: 9.668333418027393\n",
            "n_iters: 21474 loss: 1.5143214464187622, accuracy: 0.5\n",
            "n_iters: 21484 loss: 1.7659084796905518, accuracy: 0.4296875\n",
            "n_iters: 21494 loss: 1.5560752153396606, accuracy: 0.4765625\n",
            "n_iters: 21504 loss: 1.5718038082122803, accuracy: 0.5\n",
            "n_iters: 21505 loss: 1.559326410293579, accuracy: 0.515625\n",
            "n_iters: 21505 test accuracy: 0.408 gradient_norm: 10.135381827821297\n",
            "n_iters: 21515 loss: 1.6446470022201538, accuracy: 0.3828125\n",
            "n_iters: 21525 loss: 1.5353574752807617, accuracy: 0.3984375\n",
            "n_iters: 21535 loss: 1.6908643245697021, accuracy: 0.453125\n",
            "n_iters: 21545 loss: 1.5410749912261963, accuracy: 0.375\n",
            "n_iters: 21555 loss: 1.5943721532821655, accuracy: 0.4140625\n",
            "n_iters: 21555 test accuracy: 0.401 gradient_norm: 9.632134700155198\n",
            "n_iters: 21565 loss: 1.6120507717132568, accuracy: 0.3984375\n",
            "n_iters: 21575 loss: 1.5846468210220337, accuracy: 0.484375\n",
            "n_iters: 21585 loss: 1.5777474641799927, accuracy: 0.421875\n",
            "n_iters: 21595 loss: 1.6055078506469727, accuracy: 0.484375\n",
            "n_iters: 21605 loss: 1.7172350883483887, accuracy: 0.375\n",
            "n_iters: 21605 test accuracy: 0.415 gradient_norm: 10.462231895031993\n",
            "n_iters: 21615 loss: 1.4639393091201782, accuracy: 0.4921875\n",
            "n_iters: 21625 loss: 1.5460622310638428, accuracy: 0.453125\n",
            "n_iters: 21635 loss: 1.7107045650482178, accuracy: 0.390625\n",
            "n_iters: 21645 loss: 1.6867307424545288, accuracy: 0.453125\n",
            "n_iters: 21655 loss: 1.6139450073242188, accuracy: 0.3984375\n",
            "n_iters: 21655 test accuracy: 0.417 gradient_norm: 10.966113585340814\n",
            "n_iters: 21665 loss: 1.6290472745895386, accuracy: 0.4375\n",
            "n_iters: 21675 loss: 1.546039342880249, accuracy: 0.453125\n",
            "n_iters: 21685 loss: 1.7007770538330078, accuracy: 0.3984375\n",
            "n_iters: 21695 loss: 1.5965907573699951, accuracy: 0.390625\n",
            "n_iters: 21705 loss: 1.7570809125900269, accuracy: 0.4375\n",
            "n_iters: 21705 test accuracy: 0.423 gradient_norm: 12.500507448023361\n",
            "n_iters: 21715 loss: 1.775061011314392, accuracy: 0.3515625\n",
            "n_iters: 21725 loss: 1.6656588315963745, accuracy: 0.4453125\n",
            "n_iters: 21735 loss: 1.6599199771881104, accuracy: 0.359375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MQizB6IiwreG",
        "colab_type": "code",
        "outputId": "e75d049f-4fec-4a2f-c46d-070fafa815c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(train_accs), len(test_accs), len(gradient_norms))\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(range(len(train_accs)), train_accs)\n",
        "plt.show()\n",
        "plt.plot(range(len(test_accs)), test_accs)\n",
        "plt.show()\n",
        "plt.plot(range(len(gradient_norms)), gradient_norms)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2779 556 556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD7CAYAAAB37B+tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f600079e278>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXecHMWZ9389abN2V9KuchYqIQkQ\nFiIaEME2GGMbg7ENDvhs45zD3XvnszF+z9wL53DG+A6wDzgwOBEswIDJQRZBQiRJtAIorKSVdrU5\nzs50v3/MdE91dVV3T9qdGT3fz0cfzfb0dFd1+NVTTz31lGaaJgiCIIjyIjTRBSAIgiCyh8SbIAii\nDCHxJgiCKENIvAmCIMoQEm+CIIgyJDIeJ+no6M8rpKW5uRbd3UOFKk5JUal1q9R6AVS3cqUc69bS\n0qCpvisLyzsSCU90EYpGpdatUusFUN3KlUqrW1mIN0EQBOGExJsgCKIMIfEmCIIoQ0i8CYIgyhAS\nb4IgiDKExJsgCKIMIfEmCIIoQypWvJ95dT9e2dE50cUgCIIoCoHEmzG2gjG2kzH2FWH7exhjJZkQ\n/NaH3sQv//zaRBeDIAiiKPiKN2OsDsD1AB4XtlcD+D8ADhSnaARBEISKIJb3KID3AtgvbP9nADcA\niBe6UARBEIQ3vompdF1PAEgwxuxtjLElAI7Tdf0HjLHr/I7R3Fybd16BlpaGcf3deFIOZcyFSq0X\nQHUrVyqpbrlmFfw5gK8F3TnfTF4tLQ3o6OjP6be5/m68yKdupUyl1gugupUr5Vg3r8Ym62gTxtgs\nAEsB/I4x9jyAGYyxp3MvHkEQBJEtWVveuq7vA7DI+psxtkvX9TMLWqo8McySDIAhCIIoGL7izRhb\nBeCnAOYDGGOMXQLgQ7qudxW5bDljkngTBFHhBBmw3Ahgjcf38wtYnoJA2k0QRKVTkTMsyfImCKLS\nqUjxNoyJLgFBEERxqUzxJsubIIgKpyLFm7SbIIhKpzLFG6TeBEFUNhUp3oZB4k0QRGVTkeJNbhOC\nICqdChVvUm+CICqbihRv8poQBFHpVKR4k+VNEESlU5HiTXHeBEFUOhUp3qTdBEFUOhUq3qTeBEFU\nNhUq3hNdAoIgiOJSkeJNPm+CICqdChXviS4BQRBEcalI8SafN0EQlU6FivdEl4AgCKK4VKR4U2Iq\ngiAqnYoUb0oJSxBEpVOZ4k3aTRBEhVOR4k2hggRBVDoVKd4mLUBMEESFU5HiTZY3QRCVTkWKN8V5\nEwRR6VSoeE90CQiCIIpLRYo3uU0Igqh0KlK8SbsJgqh0KlS8Sb0JgqhsIkF2YoytAPAXAD/Xdf1X\njLE5AG4BEAUwBuDjuq63F6+YKZ59dT9qqiI4YWmr5340O54giErH1/JmjNUBuB7A49zm/wvgJl3X\nzwRwL4BvFad4Tm556E38+r43fPcjnzdBEJVOELfJKID3AtjPbfsSgLvTnzsATClwufKC3CYEQVQ6\nvm4TXdcTABKMMX7bIAAwxsIAvgzgaq9jNDfXIhIJ51XQlpYGx+eReAKdPcOY3drg2rehfUD6u1Kl\nHMqYC5VaL4DqVq5UUt0C+bxlpIX7dgBP6Lr+uNe+3d1DuZ4GQOqCd3T02393dPTjqltexJ6DA7j2\ni6dgamMNACBpGLhx7RbUVkUc+5YyYt0qhUqtF0B1K1fKsW5ejU0+0Sa3ANiu6/qP8jhGzuw5mLKu\nD/eO2Nu27u7GhjcP4ZlX96t+5mDTtg786p7XkTQoGQpBEOVFTpY3Y+xyAHFd139Y4PIEQrXYQrYa\nfP09rwMAdrT1gs1tzrdYBEEQ44aveDPGVgH4KYD5AMYYY5cAaAUwwhh7Kr3bFl3Xv1SsQoqMJchS\nJgjiyCbIgOVGAGuKX5TgjCVJvAmCOLIpyxmW8bGkdLumjXNBCIIgJoiSFm/DMPHA33dhf+eAI3ab\nLG+CII50Slq82zoGcM8zb+GR9bsdSwqPjWXEWyNzmyCII5CSFu9QWphHx5KBLG+ScYIgjhRKWrwj\nkVTxxhKGIwxQ5fPOFbLeCYIoN0pbvMMpUR1LCJY3hQoSBHGEU9LiHQ1nLG8+1xSJN0EQRzolLd4O\ntwmn3nGVeEu8H5RhkCCISqSkxbsQljdJN0EQlUhJi7fK8uY/+441msCD63fhO79eR+4WgiAqhpIW\n75CmIRzSXAOW2bhCTJi4++m30NU3igOHB4tRTIIgiHGnpMUbACLhEMaSTrdJNm5scnkTBFGJlIF4\na4iPGUrLW+NGKTWapkMQxBFCyYt3NBJCImE4VoTPZnV4srwJgqhESl68I+FQXj7vPz6xoxjFIgiC\nmFBKXryjkRA6e0fw4PO77W0O/7dPMODjL7dJf8dDseAEQZQbJS/ekXSs95Mv77O3GQ4rPP9zZOOG\nIQiCKAXKRrx5nJEn3B85jleS5U0QRLlR8uIdjcjEu7BiS9JNEES5UfriHXab087ZltwXOaowWd4E\nQZQbJS/efm4T/g+/wUsVpN0EQZQbpS/ePm4ThY5nRSmLt0GjqQRBSCh58Y76WN73PvsWnt/c7tqe\nDaWasOpPT+3AZ699Er2D8YkuCkEQJUbJi7fM8uZ93jv39eGm+7ek//JWb5Vb5YZ7Xy/JpFUPPb8H\nALBzX+8El4QgiFKj5MXbz/Lm8fMweFnmr7/VlUWpSo/XdnbixrWbyc1CEEcIJS/e0gFLlYWdh26V\ne0qrX/zpNbyw5SDebu+b6KIQBDEOlL54R9yyqpzm7qPehpfpXe7qnUbWUyEIovIo+TddJkYqEfYd\nsKx87UYsGpZu7+wZRh8NfBJExVD64i0NFZTv6yfepRwSWGy+99/r8Y3rn5voYhAEUSAiQXZijK0A\n8BcAP9d1/VeMsTkAbgcQBnAAwCd0XR8tSgGlA5Yqyzt3t4kmLIa552A/mhqqMKk2FqCUpQPNFiWI\nIwNfy5sxVgfgegCPc5uvBnCDruunA9gB4B+KU7wAMyyzIKiwjcaTuOqWl/CdG9bldqIJhLSbII4M\ngrhNRgG8F8B+btsaAGvTn+8HcG5hi5VB5jZR+rx9juU5XskZ3qOJJAAgkSw/JSTLmyCODHzdJrqu\nJwAkGGP85jrOTXIIwAyvYzQ31yISkQ+k+dHcVOvaVl0ddW1raWlAQ5t3mNykxhq0tDRIv2toqLa/\nq+IG9lT7+5E0TIRDwYZB/c4xaVK15z5j6cYGAJqb6zz3zbU+uTCe5xpvqG7lSSXVLZDP2wdfheru\nHsr54CPD7giJoSH3to6OfvT2DXuXo2cIHR390u8GB0bt74ZGxhzHzZab79+C9ZvbceN31kh7Djwt\nLQ2+5+jtHfHcp4+7Hoe7BlHLhVeue/0AHlyfWYUol/rkQpB6lStUt/KkHOvm1djkGm0ywBirSX+e\nBadLpaBEJClhc43z9vyaO02+jof16VwrPQPuMdzegVEkktnlUtF8mseR0YT9WXSb/PbBrWjvyr3x\nJAiiNMlVvB8DcHH688UAHi5McdzIp8fnFuft5Q/WHPsFKZk/4lT13sE4vvmrdbjurk1ZHcevPPGx\nTGNALm+CODLwdZswxlYB+CmA+QDGGGOXALgcwK2Msc8D2A3gtmIVUOY3Vqbv8BEuwwQeen43aqoi\nWHP8LMd3fKhgoQb9kkJBO3pSbp3tbYVNNOVY05PWBSKII4IgA5YbkYouEXlXwUsjQYy/BryiTXyy\nCpom/vTUTgBwiTdP70BhZiKOV5Io55qehTnmrQ+9iVBIwyffw/x3Jghi3Cn5GZYyf2/ubhOP86T/\n7+wdxg/+58VghfNBtLz5qty4dnNBzgE4G62uvpGCHPOZV/fjqU37CnIsgiAKTxmIdxYDlnn4vC32\ndTjzer+yo9P3Nyq8ZnS+sOVgFkfy61FkPt9w7xt4az9lFiSISqcMxNu9Lffp8V4nSh9D2PzLP7+W\ndXSIRXKcJvmIjcT2tp5xOS9BEBNHyYt3a7N7ko46VNAb72gTTbnPldc9hSdfbvM5upuxHEXfjXes\noFhkijghiMqn5MW7sS6G2686z7HNMy+3B56Gt2V5K3a66/HtWZ8vKYp3znln/QdiCYI4sih58QaA\npoYqx9/qZdC8RSxp+FvCKiHMJc9JoSzv2x7Wcf/fdym/J+0miCOPshBvEaWl6SNiXj7okGa5TXIt\nVXbny4aB4THc+8xbyu8LbXnTOpgEUfqUqXgrtvv8znPgUTFgmQ+F83l7I2ptvhN1gvRQCIKYWMpS\nvNXLoHmLlqfrwwx2jGwQGwtNcHpne65co2yypRxT4RLEkUZZineQOG9ZiCE/gCgKnmWtevnNs3Un\n+ImgOInHj5F4UrrdVeQ8tTfbchEEMf6UjXg7E0f5W6AhiXrzoqQMr/PQrWzjvV2Wt1CksUR2xxvk\nUtXyuBuiYPzhie3SyUIk3gRR+hQin/e4oGmaLVIyaUkkDcf21MxM554JTpREC9s0nf/LSCRNxNzr\nQHjs7y3OWYv3cAJTG93bc9HasYSBR17cCwA4adk0x3euEEeCIEqO8rG8OatV5tr4xi+fc2h1SFIz\np9vE+Z3VMHi5TbK3vL1VtWiWdwAfuJcLiCxvgih9yki8+ZSt7u+HRhMO0ZLlROHFVOVq8La8sxRv\nQZzFY2cbjTIwrBDvLI5h1dtLoEm8CaL0KRvx5tN6K33ejv1l4m1I9+WP6WW1JrIdsBRC7sQQvviY\nfABSBb/oguO4CheQdN/0/17hgOQ2IYjSp2zEm7ekVV1+XrRka/8mvSxvy+ftUYZsRS2RcAVgO8jW\n8lZZ/tlEClrXjixvgihvyki8M5+V4s2po9RtwlmbouGZcZt4+bzV311zx0bc9Zgz/4nL8hZ+LrpV\ngNRqOz+5fSP2HHQvlKrykWcTbWLt6jX7k8SbIEqfshHvmqpMYEyQZdBCEtObt1zd0SaW20RdBi+f\n9/a2Xjy6Ya9zf9HnLbpNJGL8xyd3YMe+Xtz8wBbXdypL3R3nra5Exuft4TYh8SaIkqdsxPubHz4O\nq1gLALW4OH3e7u/534nHyIQKZh9toopQEcVW3M0qw+ZdXa7BSNkhVZZ3NlkWO3tHHOeW4TWZiSCI\n0qBsxHt2az2+fNExCIc0D5+3t9uEdxWI/utMqKC6DCq3iao8oti63BuGCX13F376+1dw7Z0vO8ot\nE82gPm8vuf3+b14AENznTdJNEKVJ2Yi3haZ5WN6OAUsft4loeQc4t2rAUiyPteJ972AcY4lMRIko\nsoZpov3wEACgLb38mqbYFwju8w5SmcA+b1JvgihJylC8tUCJqaS5TXi3iSLaxMsFofI5iw1BJJy6\nrDvaenHNHS9nziH87sa1m3G4d9j+++EX9th1aO8acp9fKd7KIisRG5yegVHsbk8NkvLCnuvCFwRB\nFJeymR5vEdK0QD5vGbzl7bI8AwxYqqxVV3m4hmNXeyZqRLSQE0kTt3ADk398cof65Mje5+0lvOKA\n5bd+tQ4AcON31ji+I+0miNKkDC3vlK9YxhMb2xz7ifA+65e3dTi+sw8ZcMAykTTw+MY29A3G3S6Y\nAJOIciFotIn1p8oXb5im+jvDFBJ4kXoTRClShuKttrz7hjIRG2LubMDps75HWJnGhAnDND0nzvDi\n//Qr+/G7R7fhhntfd5VHFYWXrxDK4sJlx7UHXxXXKZk0lLNFDdMUJjPlUtLyJGkY1FgRZUMZuk0C\n+mF9LG8XJnD1rS9hz8EB5S78pJvD6ZC73e39vpZ3Immk/OB56oLS8lbsr2rkxhKm0gVkmsJkpiNE\nzEzTxOeufQorFk7Gty5dOdHFIQhfytLyznWNRa+JKYZpego3IFi+XOPAD34apukSPCuGO9+5L6/t\nPIyB4TFs3dWFX/75NTs3SrY+74RhOK4Ffz0N0zwiJ+lYdX7jra4JLglBBKPsLG+vUEEHkl3yFSWZ\n5W7C6Y5JJk2Xq6F/aAxN9VUF6ZLf9tCb2Jj212/QD+HUFTOUC0uo6ptIGM6Yd8M5KccrB0ylcqTU\nc6J4+IU96BkYxUfPOWqii1IxlKXlrUqNyiOzOr2mtwfRdZnlPpYw7Bht628Rq7yFkIeewVH7s9+s\nUFUPJZE0HNdHXGEo6bDE8ylt+XCk1HOi+OOTO/C3l/b670gEJifLmzFWD+B/ATQDqALwI13XHylk\nwVTIokiC4u3z9n97Vb//r/vesD/zk3Iy24z0KfJXiOaGagB9ADL5W9zRJt4DlmNJp3WdEOK6RUv8\nSCBXVxxBTBS5Wt5XANB1XT8LwCUA/rNgJfJBNnNShkx0vFK6BtGoIIsxyCxv+3cF0Ifm+ir7s3Ut\nxF6Gpb1KyzthOAYlR0YT9mfTdPr2+UPv3N+Lb9+wTprxsNw5UhoponLIVbw7AUxJf25O/z0uBLW8\nZbrlZXnLMvy5f5/ZR1UM2XGs3xXCuKuKhe3PKsvbEnNVdEoiaThcI1fftiHzW8OUrjh099M78W//\nuxHd/aO4+2lnmGUlQIY3UW7k5DbRdf33jLErGGM7kBLvC7z2b26uRSQS9trFl5aWBgCZqed+SBNT\neVhXWoDjVlVH7XLU1sak+9Q3VLu21dTG0NLSgEkHcrdYUyGSQHV1ZgXkxsYatLQ0oK7OWZbqdDnb\ne0fFwwAA6hqqUTsQt//mxxCaJ9chxqXfbZ5ch1g0jAfX786UJazZ18GLIPuUCtH+zLWqtLplSzHr\nNtHXbaLPX0hy9Xl/HMAeXdfPY4wdB+C3AE5Q7d/d7c7TkQ0tLQ3o6EgJX1DfpGxwUTXJBQB6uBwj\nKgYH4nY5hobj0n0OdbjDDbu6h9DR0Y/eAOdQsfroaXhhy0EMDGREpr9vBB0d/ejvd4r04GCqnG/v\n7ZYe6/DhQfT0jUi/e3bjHtzNTdM/fHjQtc/wSMK+Dir4e1YO9HDXtdLqlg3FrttEXrdyvG9ejU2u\nbpPTADwCALquvwpgJmMsP9M6IMF93u5tXqGCI3H/9SSDTFiJSwYsLTdEPj3zSDhVb74OofTdE/21\nVjm7+uUCPZY0lI3gbQ/rjr/1Pd0YFdba9IqXL1fI5U0Ukh37enHdXZsCRcblSq7ivQPASQDAGJsH\nYEDX9exW082RoD7vbAegRgIsBhwkTtzyeS+d24TzTpwLoDDRJtG024kXXdWApWmLt9xtkkgYgQX4\npvu3uBZK5iNVtu7qwv+56Xl09uTeqygFKNpkfDhSBoZ/8cdXsXV3Nx5+YU/RzpGreN8IYD5j7GkA\ndwL4QuGK5I3Mly0j22dkNIjlHeAFt4S6viaKZfObAaRiXO98dFte1p1teUsWnHAPWKbq88r2TunA\naiJpeObzFhkaSTj+5huxX9/3Bg52DeGhF4v3kI4HR0oagInmSLnMVjVlPfFCkeuA5QCASwtclkAU\nzfIukNvkzke3AUgJKz+4+tjGNiycOSmrMvFEI6ljPbVpn71N01INykZdyJBomHj7QB+6+0dx0rKU\nr5xnzCMxlYy+Iad/nxdvuzG1Z3Ua+NkfXsXZq+di1eIpKBdIvMcHwzQRUsZqVQ7RSAjDo97jbPlS\ndjMs8/F5ezEST/juw1vesqyFQGr1HCAVxmcJrl2m7IrkICqJhjFNE+veOIDdQty1YZq2+2Z2S53r\nd4mkmZXfum9QEG9JCKJVt/auYWzd3Y0b/vxq4OOXAuQ2GR+OlDYymu4pq3LwF4LyE2/ZysISxJXa\n/RBdAzK6+kbw/Jb2QMfTNHdYYz4CIQuRNExgX4c7GsQ0TfuhiUpCNBMeA5YyekXxlv3WWpU+wESm\nUqSURaWzdxgvvXloootREI6UHo71vnqlmM77HEU7cpEQrVkV2erk0Ki/eG9r68W2tl5URcK+7puQ\npiEilDWfGymrt2nIF1UwDGAsmXIDxSS/y9bnLVrenb0jePiFPTjvpLkIOb0mZZuRsJRF5fs3v4B4\nwsCMz5yI2S31E12cvCjGgKVpmoHHwsYL630ly5sj6CSdYi6c+9aBPt99NC3TdbKIB/Crq1BZ3jLR\nMU0TY2OW5e3+XTLpTlvrxbOvHXBt++OTO1IvojBoKkamlAvFdpsMjybw/Jb2QCkWRCwXWJDeYalT\naO1+a38fPvP/nsSrO8ZtkncgxsPyLjvxDmp5B23hrzh/adZlONw74vsQhoQBSwCueOlskFrepulY\nI9OCXxFIKt6GWZDp4EnD5Dz/qQNmW8fd7f0YGileLGxQ+PtZDOvwjr9tw01rt+CRPKJygo73lDKF\n7uE8/EJq5q/f+q/jjfXe0YAlh2zgTkZQcYpFs78EnX0jvg+hJnGbjI7l4TaR1Pu2h9/EW/vdvQDD\nzHTXZL9LGtn5vFUkkoad5GVXez+2t/UEitqx6OobwY9ufQlX3fKSve3Jl9tww72vewroaDyJ9W+0\nSzM45gp/P4vhQnlrfy8AYO8h7wU/vNDK7m11U+hLax2vFBo20zTxyvZODI2MkeUtwxqw9LtZQa2n\nqmj2E0MP9474il8opLmEMx/LW2wIAGBQ0Y02jUy0SVTSOCWydJuoSCQzlveegwO45o6XA8XLW1gD\noZ29mZmgt/9tGzbqHRj2GIP4w5M7cPMDW7B23a5cii3FId7FcKEUQFxKQaBU7NzXi5/+3n9GYaEb\nRut4pXBpNm3vxC/vfg3X3/06+bxlZAbIvB+C6VNqAx2vOgfx7u4f9fU/aprbZZGX2ySorx9pt4mX\n5Z00YRZAoJJJwzVQlE0dvcTIq3S721O9jXysWNf5uHesGIOuVk3z0a5SFu9r79qEzbu68dgG7wUX\nKtnybu9K5XDS9/Zk3CZZBAZkS9mJtzgpRMa3P7ISrU01gY4XCyje4qPx3OvuQTzxAQoLYY3PSQb+\nghKJBH84+ZzcslDBpJFZSUcso4qG2qhrm+zBVPUGZHidO5tomEJQbMs7wGPrSylHxAS1MAt9bU3b\n8p548eZLYBlNhXTtiZSdeNs5rD32aW2uCdzC8/mxW5tqcPU/nOh5Xu9jZS7n8EiioA9UWNOyyGVu\n2tNyZaGC/IBl0Lj5+hqJeBuG66UVwwrbOgbw5m55dkMvvKMy5GkB8oEXlaJY3rZ6537sUhZvL3gX\nZqEHg61bVQLa7VBvy+dNljdHkJukwf8hmTW1Djd+50yH9ddUH8PsVnkcrbWfl488xlm5A+kIikWz\ncp8S7zh/OBTYSjYMfpKOIs7byM7ylvVQEknTlbtBnEr/g9++iGvv2iQvp8c98pq+rwV0nWVD0S3v\n9P/5aJdZBvOfZO4LZyRPYc9nvedBjZBxI10c8nlzBPJtaf7dU03TEI2EHcdraqhS759+OOpqIlgy\nu1G6Dx+5Mjicch/8yydOyCkcUSQaCQWfXRogVNDMUryrJMcZHkkgLkTQ9A/K85zL8LJwvWZq2iUu\noBDwolLMiUaGaeL5Le3oHwp+nfjfljqy19MoouVtltCAJZ8yw3q/conrD0rZiXeQmxTSNP84bKvm\n3PGa6p3izQtbOH3iaDiE6ir5xFT+nINc7HIu4YgikWwsbxP2JB1ZlAo/SSdogxCLuS3vJze1ubYN\nclEi19/9muv7to4BO32st3h73MAC+I9FeIEpjtsk9f+m7Z24ae0W/OJP7mvjR1nkX/GxvAvZAG3a\n1oGd6VDZUvB581i3qpj3rOzEO58FiHmsm83fdFG8+e8skYtGwqgVxNsaHJ3amFkCbZALmcolHFEk\nEtYC190wMpa30uedteXtroMsrwof4rdpe2bWm3U/fvDbF/G9/15vl1NFwiNxlm3hFFAI+LIU54Vz\nXue3A8zSFSkHy1v2ODkt78Kd6/p7XrfnFZSCkPGvp+XSK2YvrhTqnBVBWlhN03zF23rI+KM11TvX\nggyF+M+WeIdQI4j3ly5agX/73EmOvBO8u4LNafItsx/RLCxvZ2Iquc/beqZUx3zvyfMcf8t6D7Kw\nwOFR+ei6CffIu9eDbQ30GIaJ7W090u5nKVveYrxzIVyy5SDeMvh3sVh1KDnL2zAd/xeDshPvIN18\nTQsyw9KyvDNbRHcIb+laH2OREKqrnFZoNBLCjCl1jgfz65ccZ3+urY7i4+9e4ltuLyJZ+LyT6WiT\nkKYhHFJFm2TpNpH0HuTiLQ8VNAwTA8MJ1zYVls/7yU37cM0dL+OeZ7gV67M0vF/cehAbfLLy8YZ+\nvi/c+jfa8bX/fBbPvro/s7EQ4l1mA5YHDg/ivmffcjSGhdIysREoBe3mi2AVr5jNbfmJd4GiTazj\n8A9bREgkxVul1gBENBJyuU3C6bAg66VvqI1i3nTnwqErF0/1L7gH2fi8E0kTQyMJ1FTJ3TVJLiVs\nSCLuIUnPReb6sab7i9dNhmmarkE6r5ziVrTJm3tSYYav7Txsf5fte/rff9mMX9/3hm/5MuXK75V7\n9rWUaPNzAVT537OhHCxv3gK++tYNWLtul6PhLNSApdjAlkS0CVf38VjurQzFO4h6B3OtiIhWqu0X\nR+aFlrlNrMFML2s234crGs7C8k4a6BmIu6JnLn/XErsu1sMVkZbVbTHI3CZWBsEgycIMA+gXXAlO\ni8x5RruBseJ4ue+yuZJBF4AtZG4TO4aef8ZyvP28SBViVux4YvXM+IlbhdI0UbxLwW3Cl2A8blXZ\nibfsJh1/1FRhH/nFm1QbxYfPWgQAOGX5NNfxRAvSzqMS0mxLUCre6d9ZhqSsgcl3+m4kokmtZBlD\nowkMjybQVBdzfRcOa6ncJoY6xCqRNF2WQ0w6UzN9TQJM3TdMEwNDTiHlX8BEwpks64H1u/HZa59E\ne/eQ+2B2Glr/N6Qj4MLIhfR5y8LXcr37vK+/HCxvmX1RjEk6Xm6Ttw/04cBh92D6eFLMiUkWZbcY\nQ5UkZE1Mvapym/zia6cDAE5ZPt2OLOFvungc27US0uzp5rFoGHXVztmGlshbL71UvPO0vMOh4G6T\nrr7UqvFi9IxhmgiHQ0gaBsIhK8Zdfkzx8nmlEQhkeZumywrmRXIsaTjuxY62VBY+O6IlRyE81B1M\nvM0sfd5DIwls3d2N45dMdd1vWQ8s17abn6FXFoa3pKJ8uQvVALncJtx5f3zbBgDAR85ejFOWT8ck\niRFTFPhoE6HO4SL0DMrO8j7vxDlYvbQVX/zgCnub+DhoPnHevKjxlrcojtZ3IU2zhaYqGnZNFbdc\nD1aDIRPZQiTOkR1j+mR1Aq4utNUJAAAgAElEQVRGQbxhpsqaNEwkDROhkHrKvXj9qjxi1YMskGGa\nzth3wG15ZzuVWHaPd+zrxRtvZfzjvQOjnscYGhlDZ89w1pb3zfdvxg33vo7nN7uXxZMlS8q1W++w\nvIug3sOjiZzCFlXIqum0QgtzHvEeya7uH55IZZ/0YtveHnz7hnXY15F/kjOn24R7tos0Rb7sxLu2\nOoovfnAFFs/iZjkqVpMBgMmTUgJ23olzpcfjL7hrzUmJBVUVDaOuxtlh0QSft6bwI2fDlEnVrm2y\nRmH5/MnKY4ihj2b6GNYknVBIbcW63CZZWN6yYxqG6Zp447C8E4bnbDQNwMHuITzxcpsdAy57JX5y\n+0b87I+ZxY/jPtOTv3H9c/jef6/POs77zb09AIDd7e6XPuOS4sRbcowgs++K7Tb5j9+/gh/ftgF7\nDroX9cgFWT2LMUlHvEWqnu3+Tm/3yW0Pv4nu/lH8Jcf0womkIU2O5RTv4oQJlZ14WzQ3VOFLH1yB\na6482aXdY4lMHPPMKXX4r2+diUvPXiw9jtNt4nwAMhZUZltVzO02sX6XcZu4zyNazZPqYrb/Xcb7\nT5vvPobkboU9Ij1Et4mZdpskkgZMw0yVSWV5C3/LJvtYuNxNIc3V0BimO4e46DbxtlA0/PC3L+KO\nv23Dzn19VoU89k/hJ97WOfmyeEXBWFjRN2JuF4Br9H2M7SDpc4tteVtWt5XONF9kPQzHoGuRByxF\no8Ovx2uN1+Sy4s3waAJXXvcUbn3oTdd3fDHI8pZwwtJWTJtci0bOwqypiqTTl2ZaQ5mf3MI5YClf\n7V20vGurM5b3tz+y0k67uub4WQCAC0+d7zqPaBlcfMZCnH/SPGW55JZEdr50t3inLe90nHdI05Td\neddL4HEemVCLxzWExZJN0/n3WMLwXXleFGKvV8I6Nj8xyMvq4xMIWT0Er4Emy40kE2DrZ5qP2ySI\ne8bh8y6i0zufFLx+liWfQEysQyJpeDZiB7uH8Nfnd0safuc5VUkb/Xq8VvqIXFa8sRo+2RqvjiyV\nRbK8y27AUsaHzliIqlgY7z5hDibVxaBpmh0RwgutDP6dCgviLctYVhUNOVrz5Qsybovl8yfjpu+u\nkfqAsx2wlO1vndYSYOuzisZ6WbRJCPFEAoaZOofSbSL8XRNTX0dRmEzTbXXeuHazYwKPYZqCtWsG\nyiQYlLGEgapY2CHKhmEipOip8AI0ljTQNxTHN375HC49azHOO8ntcrMsb9nKQdKQUclpg4jxeEWb\n3PzAFrz05iF8/WPv8N03PpbEpu2dWH10K0KahhvXbvbcn9dZsUH80S0vYV/nIP7nn87O7G+a0Hd3\nY/HsRlxz+0b0DY2htakGJyxt5fZxnsN6BkVR97O8I3lY3geF3opjejxXz2IthVbWlrdFbXUUH16z\nGI31VfZNvOL8pThtxXR8VOEusXD6vOVuE14g/RZvUA3eZTtgKds/IkwGEssm4vJ5m2ZqwDIdQx3S\n1KLIv2PfuvQ4NHiM2EuPIWzc3taLNi4XimiJG6aZtW/QRErcrrljI57hZzMi88LEJRa1DP4FSyQN\nbNnVBUC9sG3Mdpu4y2xI3GeqcQA/ijE7UcUrOzrxm7XqyUyjY0mYpok7H9uOG9duxl/X7wYAbNQ7\nuDKqx5+AVPTP+jcyg7z70j5pvmFf99oBXPf7V3D7I9vQlw4v7RWyVbqjTVL/i70ZP6MpmoflbZXd\nOgN/pvFwm1SE5S1jamMNPvO+Zb77OdwmQh/L6u7xQmr5yC49a7Fr4YFCInvorAeNfxRU4h0Ja65V\ndKwBy0TabZIaWPV2m0yqi2HFwilZx836dTQSSaflLRvQ5JEPhJnY1zGI7W292N7WizOOm2l/Fx9L\nAjVRh+X98vYOHLdoqrQ3lnCIt4moT2NrjQEEdptIjmH1NHoHRvGbB7bgkjWLXTNzxWtUbIYVKyF1\n9Y3gO7/+O9YcPwu70u6Cnft6XfuZJnDnY9vQxi1RxwvZLWn/8JzWekfu/N7BuN1b3tWeGjx9ZUcm\nsZnoenBFm1hBA9mKdx6W91C6wZFl3OSX6CuW26QiLO98cLpNRN9t6n/+AbCs3/NOmqscBC1IuSTb\nZJNhVA9ntcTNYQ1YJtOTdEIeq/NkBMj7PCr8OhqpcMXMQz0ST3ouOqxCFWNuiXacE9eb79+CG+59\nXbp/IuH0v/sJpT1gGdhtoh7Ie3D9bmze1Y3r73GmiU0kDTz6UmZNyPGYpPPGzk6s46b1j8aTePqV\nfdiaXg3pqU377DEkWcNlGCYe29CGN/f0ZLZJyj0cd95rPqTTum6OUFLhfoizTTXFfv5uk9T3uVje\nluBbZeFPPcQ9y8Vym1Ss5R0U3usrWrGmIbG8A0xIKUi5JM+cbIq6LPEUoC5nOJx2m1gDlorz2+FP\n1u+ydPv4xTUnBbfJz7nwvsCY6sEyy50hvjhbFUuy8RZ6Kuti5vqZ3ABsV98IJk+uywiYxGKzx0p8\n3CaW9Whpm+g/f+bV/XiJzwsyDpZ3PGHgtw9uxbGLpqChNoZ7nnkLj27Ya4fcApmJciOShksWqSNr\nCEX3Yjcv3kLoLeC+z6Llbe3rNXlHRiSPVd7FSCVV41qs9VhzViLG2OWMsVcZYxsZYxcUslDjiWMK\ns2KmXNhheec/2SYYarcJj8ptIrPSDROY3FAFE6lZmKGQ9DQAMq4Ze6KSh+Utd3n7iDe3FFsQpL5U\nOMXZMUhkifdYsJdS9Hnzp/vm9c/h+rtfw9bd3fjOr/+Oux7VbfGxLPvu/lFOQFK/c0abSOpkDTqn\nnynRajzcN+Lc3+dyPbZhL26+f4trUNA0TTzy4p6swgGtSIq29OQVa9YuAFRHM5a3eK6OHmeZAfmS\ndqKo9g5kXJDWM22Ypv1siQIoPg/WKbJ1m1jnyiUW2/qN9Rz3D8nz6JRUnDdjbAqAHwJ4J4D3AfhA\nIQs1nnhpjOwllK1Mk9uJfb7mvv/MBUcDkK8Er4rzlm43TSyYkVlTMxQK+YYKWl9nO0PQz8vCp6UN\ngswyMk2nr/LqWzdw+6dE1S/O28IRbZIwHKLUNzSGTds78fQr+wAAj724x/5+LGFgz8F+fPuGdbjl\nr1sB8HHe3qGC4oIYokCJ/SK/63XnY9uxfnO7IxEUkOpt/OGJHfj+zS94/p7nrf3qWZeWy0jm6jok\nyUUjEy+x4eZTJ/Buk7C9kK/zGPwgKZDqlSQNA7/4k7MH5xcqaJUjl0FFvkwb3jyEB/6+y3e/QpKr\nEp0L4DFd1/t1XT+g6/qVhSzUeOIlSrzlbU2JbxannBetXJnPpx0zA4Da520l2eKRWekmgHnTMgNi\nIc2jDbF83tx5ssPfbZJNd1L+ApgOi3k3N0vQdptIxPu6uzbhkRf3KI8/xi1WwfPyttQA2oKZjZzL\nw8T2dB6Wda+3475n3/JM+uU4Z9o6CNuTvOSxyxZBByzFZFzWtTBMUzqhRMbadbtwL59DXUJ8LOmy\nNsWc7YBKvJ3begfj2LKrC2OJJF5ND1QmDdPu6fLi2t0/ir8+v9vxe8M0sWlbpyOiCfB391nPoMpt\nYpombn1oK77/mxdc+/DP3jOv7Rd/alNq0SbzAdQyxtYCaAZwla7rjxesVCVGKAT8+LMn4WDXECZL\npq0XA5k3WuU2+dyFy/HZ9y1Dz0AcN67djG17e6RCb5rO2G+vOG9LJ6zGLWhSrMyxvb+38qsE5XCf\nO0eJaapfOmthZHH1HiBliW7d3Y13rZ5jb3P4vBOmVCgtEWpuqEJ3b0YgeSt9LTfN2i8xVcbyDtn1\n4XGJd8CeSkfPsLOHxR3omVf3OxbE5ucMiNz/9104el6za7slWqNjBnqE3DEjcZl4y66lc9tzrx3A\nc68dwJLZjXYInmlavvEkHt2wF5esWYhoJIzeQfmzsHWPezzDz+gQXR8ibR2DeObV1ABu32AcU7il\nDvk6dEmeT/EchSZX8dYATAFwEYB5AJ5kjM3TdV16BZqbaxGRdPmzoaWlwX+nHOBvmuocVbEIFs+f\ngsXzpxTsvA311Z51akyvi8mXq0nScDQ11trft7YC0XSXtqYm6jp+bW0Ms2ZkcsLEomFEFZNvqtJh\nW5FICC0tDagRMgIumDkJb6e71rJj+C0+0NhYgyohzYBFfU00UB7uSCSE2jp5T6imNoaWlgbPBmLy\n5Dr7c4hr7CKxMOoUxwVSDYNVZ03T1GWoidn3IBZ1X6OGSTVoaWlAA5d3nb9ndbXO4/LH82JozHDs\nV73fmbeE/y6VZVI9wzEqmdeg2S2zCUNopWWRFeLkNwCob5A//9vanOGHsWgISLeTG3d04cLTF+Kg\nRCgj0bDjHtq/j0Uc5xHPGU7rkmGa0vLw55rUVIOWqfWufQDvHCrVAe9btuQq3gcB/F3X9QSAnYyx\nfgAtAKRrTXXLcjJnQUtLAzo6CpM4R4S3ZlTnMJJGwc/f3z+Cjo5+/ORLp+HOh7bijbe7HN/3cZad\nde64xKoZGhx1lG0oLXqmYdrbY5EQ4gkDI8NxDA1kBpSMpImkxDIFgJF0BkAjfRzet3ntF05Be9eQ\nnQBqTBIy5ucy6OgcwIDEggKAuoDiHY8ncbhL/tIc7hpER0e/NCLC4kB7xq87yK3y0z8wiv4ad8MS\nSeeFiSeSGB7OXJ/+fvcgHQAMDWXuzajk3nV1DaJjUhWGuXM77qWw8lD/wGig53DXvl7Hfoc6ncmz\n+O/8EkLKnrmBdLlME9i73ym2Q5I48cFh93yIrvT98YM3AoaH4+jo6McBSRKtkdExhCQJE4ZHxvC6\nfhDb9/bgonOWoFO4FsPpsvHvy/o32nHzA1vwfz97Erq4QeNDHQOIcnoxEjC0teNwsLrK8BL9XH3e\nfwNwNmMslB68rAfQ6fObkiSIM6AoSyylD3nMoql4jyTjoUz8gkSbWF00foWc7132Dhy3aArOWTUb\nVdEw58dWF8/OkGjvmzne1KYaqTXlLL/3NXt5W4fSh1tfE8ymSJqm2m3i4fO2f8/HEXNd4JF4Evc8\n6/b3WjNW42NJ+/qYUCdb8lsX09om9g4OHB7EjWs3O2KFAfdCvqPxJJKGgc50Q9+YngVr+bwN08Rr\nOzs94+dVoaZeWPHdScNEz4BTmGXXWza2EdRlxkd3WY+UmFoYULvQOnqG8c83PY9bHnrTnlzEY913\nE5ln/rZHUuMCT72yz3HMG+553TFvIKgvW+ZKKgQ5Wd66ru9jjP0ZwPPpTV/Vdb0Mlkd1EySKotjr\n48nCD9ncZsyYUusQ9tNWzMCLWw/ixKXTcNfj2wGoxZsX+oUzJ+HrH84siFxdFcHwaAIhTcOCmZPw\n5p4ezJpaZ/saHSh83n6XxO+yPrh+N96ZHogVqVe4U0T2dQzioGKxhd89ug3DownPpEfiIKWFarHi\n2uoI0Jtym2TE2L3qkIXf0moJRaTD7Y/ojkku9jHSETob9Q48vrEN2/b24PyT5+Kh5/fgGx8+zn5O\nLTH/6/rdzoWb4c4Ome1YBgAMcr2ijl7/xS5ksfW5rFZkTYiS9cp2HeiTXmO+cRkYHkN91OmKSnAt\n7FjCQFU0jOpYBPGxOB7b0ObI3d/eNYR1b7TjrHQCuqC+bK/eXz7kPElH1/UbAdxYwLKULIVYSEGE\nzc0MBMms2KpoGP/2uZMd22qrI/iXT5yAQ91DtniLDcuSOU04cHgIi/h85wI1VWEMjyaghTR88J0L\nsGB6anCLX6RXTIcrXgO/Ri9Io6h6gcXFLrx4+IU9yu8s4ZrWXCMVed4iDfIiVkXDiIQ1xMeS9nUx\nTfVAIr9dtotteQvnloWEWsdb+9zbjkHRR19qA5AaiLSu5+HeUSQNw46C4Zkxpc7xt59hIhMePi1E\n70BuKSKCThnn78vImFq8xfBIGbIkYnyvICPeYfSl7Zj7nn3bsX9HzzD2dw5ixpRaJJIGZrXUwTS9\nfd4jo8UR7yN+eryFuC4ljyw7Xz788IrVaOUGJLOd+MMLo+hK+eg5R+GrFx+Dc0+Yrfy9lSEwpKXy\nn5ywtNUVF75wZkrQl82bnD6nWAbus+QcQQw6lejVZSHeQZgzTe435MPcgkzmiYRDiEZCiCeStvCa\nUM+gExNvqb4XLW9+NqNz/0y4osXslpQY7+sctAXRME20HRqUWtUJIUTPz/KWuSh4oQy6wLNIUMub\nvzaW+AYRahl+kTCWi6TaI4X0wy/swfd/8wJe2dGJRNK0nwkvxFQAheKInx4PAP/5tXdKswV+8jyG\nA51DuFCyMEI+iDdbTIjlBy+cYrmromEcf1SL5++t6c38eyv6ZM89YTZmt9RjyZym9DkFyzubWUYK\n4gqXRjaWdxBqFC9jPzcgOKoYuOUJhVKNXXwsmZkFaJjKiUD7OwfT64WGpD5ve4IIJ6i3/HWrND80\nkBZlYbmuhtqUYXGwa8ghOpu2d0iNArGh8OtVDkrith3fS8Q9CEFdDjLLm7ega9IuwCDILGA+3jw+\nlsQfn9iRWTfVg47uYSSSBqLhkK+hUiy3CVneSL0AVRLxbqyL4WPnHlVwMRHfFyt2vLkh2AQg/oXz\nWuFGhfUbMZ82TzgUwvIFkz1zpHhx2bsZAODYRerwSllkAuDdC1Ixd5o8hMvreLzlHQ9geWtaaqJU\nnEtcZRimshFq6xjEnY+m3FuyToYlHLzlrhJuANigu33xfBTLSDxpL5+3dt0ubBBmIYbT6YB5/KxG\nv6RKRbe8DRN16SyQ8XhqAs96bt3Qj2SRHE60vPsG4zhwOBMJ9+aebjz84p5AZRsYSdiTiGThlF7n\nLRQk3hOAaMXWVkfwi6+9E9dcebLiF+rfe60SpMLKOMhbBNOa1QsZy5g3vQGnHzsD37z0OOn356ye\ni5u+u0Y6ycNCjKaw4C3GU5ZPwznvULuALD57wTJpAwyou8H8TESVAPOENA2xaCgVbcJFinhNwX/6\nldTMO5nbJKnweauQ+ZeHBavOy8VXVxN1CZPf4tF+qVJzTXRoraPqv59hu9FGxpL4zz87My9mMx4l\nWsC/useZYZK/j/U1UU9jyvL7R8IhnyUCNQyTz7tykD1uk2pjvgs9WPDdtFgOk58sS4bv8s6b3oBP\nv3ep6ieSMmj49HuPxjELpyg9JJFwyPPlEru73/3Y8fj5V9/pEJTPXbg8vaydN5FISJp1EVBb3vyq\n6UGsLQ0pS5UXb8DbsrKTVUmE6sH1u7F1V5fnCkJ+8HmjAXVPTEPqOnT3j+LWh7ZiyIrj9xHQICWr\n81mtSoaYVVJFImnaa8bKBhyz8TiK92lXuzN0kC9PTVUYJ7BWqLBcbpFwyPO9rY5FyPKuJLyMhZ9c\neTKu+by3Be6wvBWC5YVlyQwIeSlOXjYdNVURnC9Z9gtIpQi47ounZnUuMZohFg3hncemQgRFt0lj\nXQyNdbGc0u5GwyFlQyaKtyXyb7e7J054DVZpmpYWbwNJTvREAZUh08hD3cO47vevFHT6tGoAMhYL\nI5ru0Tzz6gHc8eg2ALnnCG9pysz2ra+NZdXwAymXUdBcLdWxMDRN7l/PxvIWxV8cGOeNiepYBCce\nrRZv2/KOeFve1bFwoIWmc4EGLEuM6ZP93RdeA5ZBsCxZsbsfjYRwwzfPUP5u1tQ65XcqRC0JhzTU\npsVUdJtY7hJXVz7A+xmNqi1vMc9LS2MNegZGpVELNVUR5QCTpqV6OknDdLgTeL+pCi+hKmS+Z9Xk\nqapo2DEhp+3QoF2uaCQUOJ/1ysVT0VQfQ0tTDf701E4Aqeu7ZHZTVuW8++m3cMzCYOkmopEQqqJh\nvH3A3dhm0/aI97W+JupwRQ05xDvs6YKylmWLhjXP3u97TpxLlnclke+CKLw1m8ski0KH4nkh+vct\nv7EMS7RF8Q5Sw5pYRNmQiYOrsWgITYrskF6NYShteQOZyIegeK1EL4bv5YPqedA051iCNe3bNLMb\n9D7vpLn45HlLUc+5smLRUHpJvRRrVs6U/dTFVbe8FGi/SDiERsX9yqbnMBJPYOvubmzalhrIrRV6\nZPwScLVVEXvGqoy+tNskHFYbDQBwzqrZuOCU+YHLmA0k3hNAvstZ8WF62ebZBoCGcRRv0W2iae61\nNS0s4YmKkSwBlrKKRkKoUoiQuCRcNBLGJMmLqSEjcDL3iaZlojNk/lcg9bL+8IrVru1e97yglrdC\nvE3DdHw3NJrAaDw1zd9v0NJx/PT14QeHo+GQQ0jmtKojf3IhHNLwtYuPkX6XzbqewyMJXHfXJlyf\nHqi03FWnLJ8OwGl5tzTVKJ9TIBOdFA0Q510sSLy9KE4a3rwt73wnfI5XWlsZ4ZCmFFmryy++NH7V\ntcT5Y+cuQWtTDZbMds4uFQc8Y5GQtEscDofshvHoec2YLywGrGmabaWqXCsnsBZp2KKXcS36vE87\nZjpOWKr2t3rBu00+dR6zUxAYptul8sWfPY1D3cNZpX+w5iTwAh2NOhf1yMWV50V9TRQzptRh6Vy3\na8arUVwwowE/+8pp+FY6Imrrrkzyt0TSwEg8iYbaKFYeNRWAM3Q06DtSHQsXvL5BIfGeALy60EHI\nd7r+/OkN+NAZC/Gvnzohr+MEQaxrKKSOi7WsXtHNsXSuOtwQyFjJ86Y34N+/cAoWzJzk+N4l3tGw\ntEscCWuOae+iFctb3irCitWJTJhKq1iMdFk0qzHQ2If8/Br+8bLjsWpJC05ZPt0ur+lhYWfzPFlC\nP31yrX3do+GQowFQhWzmijXjVLaKlZfhHQ6l3GPLF0yGpjmn9d+/bhcOHB6CaQIzpqSuNR99tGJh\namax+Jz8x5dOxWLOOKipihS8vkEh8Z4A8vd55/d7TdPwvlPnOxL2FwtZVVU+VktcRDFZPLsRF5wy\nz7Ht09yCAmI0iWhJNtQ4X8BYJIRGSQ7uSDjjuzVMt9iGPFw+FqrJS9bAoAzR8o6GQ64ootOPlSfx\nEomENbC5zfjyh45BLBrmFvNVu1RCIeCM44L5qa1jaJpmzw1IJE1Hb1A29yCftV8tK1i6LquHeoe4\nsoqDivf/fReA1CSjmVPqEIuG7Eb0ojMWYnZLqmfxkytPxtcuPtZRFl6sa6oiynQGxYbEewLQchhk\ndPy+CImyioWsoeK7mbxQZ4TB/Zupjc5u7OnHzbT3F6e/iyIlikl1VQQtXG4Z+3ec5W2YpqsROHPl\nTF/L27LiRAyPgUHR5x2NuGOHmcRlIENM8arZPQlTKaCapuGK85e63E3S43PHsHo0/UNxx32UW6K5\nP7PW4LKs58C7TcRGnH8OvO5bKKQ5cg1Na858rqmKuCbr8PWrrYqgpdH9LI0HJN7jyHc/djzOO3Eu\nZipe8KCUunbzYilzEfFiy4/Ue61UL8s7bVlK1aLl7XOBFkxvwKJZ7l5HhHN5iG6TL3xgOdjcZqUA\n19dE8ZvvneUaHAVS8cN9g3Gl1S4uJRYJh1wCGHQylqu3EOLq4+M2CeL75o+fEe8xp+UtEe/WtCA2\n5ZDkzXJdyMTb5Czv733seMd95e+VV0QIAHz+/cvtzzILn4fvFdVURTC1aWLGkCjOexw5el6z53Tx\noIRDIXz5ohVS63EisARv8exGrFrSgpWLp9rfidptmqZjHcBYNOwa/Auy6ASPl8UlY+m8ZmmoYMrn\nnUk4xYuFVSY/C07G13/5LADgcJ98xR0x3jwaCTnqdOyiKcrQRhHRbaNxPYmI0m2SjXhn6m8lxeof\njgsDlu5rdNbxs6BpwKKZjfjRrcFCBP/1Uyeg/fAQZqbnF0QjzvJNqo3ixGXT8PsndqTKFtbw1Q8d\niwfW78KegwP42LuW2PuqBPnj707tw19v0bduCs4/p9skLG2wxwOyvMuUVawVcxWpTieS95w4F9O4\nwTYxGsCE0wUis2Rbm2rw7tVzHHlTvBJhiWF9vAj9yydWpf7/5Cp84t1L8M+fWGU3etd+4RR8+aJM\nCNrIWNLhZuAbAUu0/HzeMoKsuHLuqkz+lkg4hEncIOtXPnSMr+VoIfZQTluR8pVf/q4lymso5myf\nMqkKl54lT/jEX5MVC1KDeqtZq6/bJBYN4ex3zA6U6sBi1tQ6nLJiOnduZ91+8bXTHY1aJBzCpLoY\nLjt3Cf7p8nc4XCGy+/bu1XNwdjpvjkO8BaEXDZCY4PMGgK9+SB7KWEzI8pbQVB9Dz0AcTQGz/BEe\niLplpl6k1IIQSemLrmkaPnrOUY5tXq4QrwFLa1GKRTMbsWim06c7takGUxqroaWLOTg8Bi09CCe6\nTSx/sTJ3SB6urGmTax29kWgk5MhkGQ5pgSMaxMZydms9fvOPZyGkadgtWfsRcLur6mtiWL5gMvCk\ne1++AVixcAp+eMVqzJhS64iYkQ1YWnHxkydV4/2nzXcsKBEJa1izchYe29jm+I3YE/Bzh6l6FoC8\nN8CLNF9m0UqfO60eKxZOtuPBRZ83ABy/xDsNczEgy1vCD69Yja986JhxicaodESft/WXFe0hC//K\nFtGlEM5CSTVNw3vTkSyJpMmFCprSmazZTsgIklKgvjridNGEQ5hUm/ENa5rmsPb+6fJ3OGYxXnbu\nUbZPmc+UaGGJHt8YWUt58d9nIlPUg5uiS2re9IZUVAu3Xeaf52OoP3j6Qsd3KxZMwWWci8MulyjG\nPrfVa11VWaPL15FvGCKCeyYcCuFbl67MiDcn9DWSpFynHTPdta0YkHhLaKyvwjsmoCWtRFSRXJZb\nQDVTUYT3qX7hA8sd34nRANmuOVrLvYC16Qx2VbGIw8q0/Joq8Vad8QPvXGD7bNXnjzrOFYmEUFMl\nDFhyluOSOU048ehp9t/nnjDHjgs/2KXOs8I3EJ94D7OPaXkj+AUmVD5i1cApf8llwj9/htrFp5po\n41p6T3kE9XktZG4Tldj7zTgVQwVFLjx1vufvCwW5TYiiIg72WA7E+rRlqcrpLcK/x7xwAe4Ihmzz\nvdRwA06XnXsUImENl6xZ5OjaT09HCCmjPhTW/tHzm7HyqKm48rqnlOevq444w9rCmisc1M9tcsma\nRdi5rxeXneu2YC1aham3pJ0AAArdSURBVAHuaDiVIVF0mximKRW2H3/2JGXDyJeX/3zR6Qtw7KKp\nmCfOVkWmFxZ4MWKf2+olun6WN49ftAnfkMpcOXU1UVx85kLML3LPncSbKC7Ce2m9p5ZPV7WajojX\ne9tcn5/lzQvj5EnV+MIHVgAAFs6YhOfSK9tY+2TrNomEQoiEQ7jywmVobqjC/7tzk2ufWsFtYn0+\n7ZjpdtpesUES5W52Sz2u/4Y6I2TqeDPw1Cv77BmrsWgYgyMJV6hgUjGhyCtRk8ofHYuGXcINpMI7\nrRSsQfOT+C2952l5y3zeitlufpa3X3k1aEVLRsVD4k0UFbFLbC2bZYl34FzWHu+tmHEua/FW5PC2\npkhbs+2A7N0mlv/05OVqP+gq1upYT9NafOAzFyzLHF/T8OWLVtj+/VySm0UjIVz16RMzf9szWpH+\nP+M2kfVeaj0WXfBakEPG3NZ66Ht77PMFwW8oQzYXwEJmTavGW/zO0zfkvfRbvjOgg0I+b6JwSN7B\n49Ix32KX3StXsgzZrNKrPr0aX7/kWJegZus2UYn31MYa/PCK1fjOR1dKv+cnhKhi7v0iJK44fymO\nntesjHzgWcVa7eiZfPPjABnxysR5p7aLmQbfc+IcrFw81bMuqlm/qsbuSxetsD8nC1AXwLvRlrm7\nxOgUa5q7X9z2GcfOwLTmGnzjw8dKvx+vGdBkeRN5Y+ckkbw8s1vqcdN312B7Wy+uuyvjMlizciZ2\nHejDmStn4d9/97LvOWSvw9xpDdJY92wTd1nCK8tRIXb5+Z4Cb82Jg6gWfi9ySzpEMNsGpzUd0rho\nZu5+VT5PCQBhgpKG1UtTjcW7V8/J+RwqV0ZDbSoccfPbXYEtb9V9nTe9AbslqyLxyBoIsVfw48+c\nhJ6BUc8eBpDq6V3z+VM8yun584JB4k3kzeXvOgq3P2Lg4+kV40Ui4ZAjXwSQGv3/3IXL7ckw/KzM\nvMny5WltqsE/XnY8ZkzxD+uz9jn+qKm49OzF+OMTO3DZuUsCpxD9yZUnY2B4DD+5fSOATMSD1yQk\nVZn/7XMn5ZXe1xJvS2xOP24mnn3tAC46YyE0TcMXP7jC49fB8PIf89EtIvWynPOKS/SvnzrBtwFo\nP5xaOWj65Fq0pyNyxGteUxVRrneaDWR5E2VDa3Mtvv3R4z33mTypGldeuMwlkJqm4cbvrvG1lov9\nQjCftLMWzQ1VuONH52FkcBShkIavXizvOn/snKPs1VZ4xFSvliUfZBamSJDGxgtLvCzdWzyrEfdd\neyG6ugbzOi6PZzoBzSnec1rrsffQAC5/1xJHHLoF/wRYszut44R8Gj9r9ahjFk6xxTubRSiyId+U\nzUEh8SbGDdWgXZCHvZSScTXWVyE+7BZmnncFdDVYa0cOBwyZLCRWtEWScwV5TXTJBa+wOzu6Je3S\n+Polx2LdG+04c+VMuf+a26RqNFV84t0MxyxuwanLWvHohr0Aiife4/Ws0oAlURYsnZtK6lWIrnwp\nwOakUrxaMyMti/y4RcEW5S0E1qzN0YCLD+eCl0Bag9it9phDNS48db7yN6cfm5pV+vn3L886ZHNS\nXQwXn32UEJJZHJUltwlBcEQjIXz3Y96umXLim5ceh87eEdv1sWDGJFz9Dydi2uTxyxRpzeIcKZDV\nf8X5S12Jp7zSH7z/nfNRVxPB6QEXgpg+uRb/809n51VGnmJZ3uMFiTdBTACxaNg1bX52gRfu9cMa\nnCuUy0a2Go+XS6w6FhmXySwqsh0kLjXyanoYYzWMsZ2MsSsKVB6CIMYJS7yHRoPll8mFUhqrEFHN\nsMyVJXOasg75zId8Le/vA+jy3YsgiJLDEu/As1wrjEJktOT5x8uOz3t92mzIWbwZY0sBLAPwYOGK\nQxDEeCGu/VkMxitsLhcKPWCpadq49jTysbx/CuArAD7lt2Nzcy0iOaxAwtPSUnqrxhSKSq3bRNWr\noaG36GWohHvWOjXjY+frU4i6ffXSlXhiw16sXDa9pAYG+bq1tjSgpTm/9WQnkpzEmzH2SQDrdV1/\nmzH5rDqe7m51juEgtLQ0oKPDe/pruVKpdZvIeiVGM4mDilGGSrlniXhmoNKqT6HqdvzCyTh+4WR0\nF3DCT76IdevrGYKWKJ6/vxB4NaS5Wt4XAFjIGHsfgNkARhljbbquP5bj8QiiYByzaAouPHU+Vi9t\nneiilDTL5jdjamO1vZLQkUahJySNNzmJt67rH7E+M8auArCLhJsoFUKahovOWOi/4xFOdSyCa794\n6kQXY8IoYXd8ICjOmyCII4orL1wGfW+PvXhwuZJ36XVdv6oA5SAIghgXTl4+3XNxjHKhvJ0+BEEQ\nRygk3gRBEGUIiTdBEEQZQuJNEARRhpB4EwRBlCEk3gRBEGUIiTdBEEQZQuJNEARRhmjmeCagJQiC\nIAoCWd4EQRBlCIk3QRBEGULiTRAEUYaQeBMEQZQhJN4EQRBlCIk3QRBEGULiTRAEUYaU9FISjLGf\nAzgZgAng67quvzTBRcoJxtgKAH8B8HNd13/FGJsD4HYAYQAHAHxC1/VRxtjlAL4BwABwk67rv52w\nQgeAMXYtgNOReo6uAfASKqNetQBuBTANQDWAHwN4FRVQNwvGWA2AN5Cq2+OogLoxxtYA+BOAzelN\nrwO4FhVQNxkla3kzxs4EcJSu66cA+AyAX05wkXKCMVYH4HqkXhCLqwHcoOv66QB2APiH9H4/AHAu\ngDUAvskYmzzOxQ0MY+wsACvS9+c8AL9ABdQrzYUANui6fiaASwH8DJVTN4vvA+hKf66kuj2t6/qa\n9L+vorLq5qBkxRvAOQDuAwBd17cCaGaMTZrYIuXEKID3AtjPbVsDYG368/1IPUQnAXhJ1/VeXdeH\nAawDcNo4ljNbngHw4fTnHgB1qIx6Qdf1P+i6fm36zzkA2lAhdQMAxthSAMsAPJjetAYVUjcJa1Ch\ndStlt8l0ABu5vzvS2/ompji5oet6AkCCMcZvrtN1fTT9+RCAGUjVrYPbx9pekui6ngQwmP7zMwD+\nCuA95V4vHsbY3wHMBvA+AI9VUN1+CuArAD6V/rvsn0eOZYyxtQAmA/gRKqtuDkrZ8hbRJroARUJV\nr7KoL2PsA0iJ91eEr8q6XgCg6/qpAN4P4A44y122dWOMfRLAel3X31bsUrZ1A7AdKcH+AFIN02/h\nNFDLuW4uSlm89yPVQlrMRGrAoRIYSA8YAcAspOoq1tfaXrIwxt4D4F8AnK/rei8qp16r0oPK0HX9\nFaQEoL8S6gbgAgAfYIw9D+CzAP4VFXLfdF3fl3Z5mbqu7wTQjpS7tezrJqOUxftvAC4BAMbYOwDs\n13W9f2KLVDAeA3Bx+vPFAB4G8AKA1YyxJsZYPVI+uGcnqHy+MMYaAVwH4H26rlsDX2VfrzRnAPg2\nADDGpgGoR4XUTdf1j+i6vlrX9ZMB/AapaJOKqBtj7HLG2HfSn6cjFS10CyqgbjJKOiUsY+zfkXqR\nDABf1nX91QkuUtYwxlYh5WOcD2AMwD4AlyMVilYNYDeAT+u6PsYYuwTAd5EKjbxe1/XfTUSZg8AY\nuxLAVQC2cZs/hZQglG29ADuM7rdIDVbWINUV3wDgf1HmdeNhjF0FYBeAR1ABdWOMNQC4E0ATgBhS\n920TKqBuMkpavAmCIAg5pew2IQiCIBSQeBMEQZQhJN4EQRBlCIk3QRBEGULiTRAEUYaQeBMEQZQh\nJN4EQRBlyP8HYwVEhO92dvAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JDuU3dBdBw2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d1786eb-a15e-4da9-fc83-ccd9cd1aa63a"
      },
      "cell_type": "code",
      "source": [
        "f"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "crR_p_3UCCDU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}